{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"are.ipynb","provenance":[{"file_id":"1GqZu6zmCy2vMNMZqO78DDuHvwv_9vJcp","timestamp":1610896059391},{"file_id":"1IzPbxNFQmmSYk9s14L4YjBfUgACn9mW2","timestamp":1593978024316},{"file_id":"1G7NKeneJNyRtcRxLVbbF9jYtRyuTOa-R","timestamp":1592749700622},{"file_id":"https://github.com/satyajitghana/TSAI-DeepVision-EVA4.0/blob/master/Utils/Colab_25GBRAM_GPU.ipynb","timestamp":1592043804148}],"collapsed_sections":["1xq60D_tqajl","4cbgwZWWfWpp","P5vicRPTfcT4","hcRcLeSFH2Ap","ZYPg9A5nHrhW","mO-omVf397wc","G6ei1SQuALhP","aHl37tiKeYau","pxFT2535ui8y","Kz7i2VSsAgnL","i01G721Agfi0","0l797mzuBa8t","58B1Cz0wBkjo","mZjvskHx-lYk","TVs3XVBXA22T","2rx07OS-faK9","9kJS4vOK99ja","UCZoIA0e-D4z","-5nYRiX1f-ya","gGf32xK4qIXQ","3JRIlWyFjSXl","It5emxOFNrBB","MWq73EDK8JMj","j37RCU8qvIHE","Jy03DcJbX6LD","qL_yyo2lnJIw","zO1zFSwon2r8","s_gLbgljX-nS","tUNo5vl-oQIc","n1Yn6stOGiKR","RlB5k2FKMDdM","eK0jmMLvC1ui","HKWvynDgZWeL","XXrgNCixLhlF","vMi5QJ8UV9zN","pjverEx3cxM9","-uEar-G9dEPB","3bqOcsCvA0Vl"],"machine_shape":"hm","toc_visible":true},"kernelspec":{"display_name":"Python 3","name":"python3"}},"cells":[{"cell_type":"markdown","metadata":{"id":"1xq60D_tqajl"},"source":["# INSTRUCTIONS"]},{"cell_type":"markdown","metadata":{"id":"V4S71cwpj_bZ"},"source":["**Use 'ctrl+]' to collapse all** if running in Google Colab"]},{"cell_type":"markdown","metadata":{"id":"Y-M1gNtBqwtz"},"source":["Steps to be followed\n","\n","1. Mount your drive containing data.\n","2. Set 'prjct_dir' and 'data_dir' arguments in argparser. \n","3. Run all cells before \"Final results\" section.\n","4. Run the exp(experiment) required in \"Final results\" section. \n","\n","Note: \n","* Arguments except 'prjct_dir' and 'data_dir' will be set automatically according to selected exp in \"Final results\"\n","\n","* *seq_len(no. of sequence of samples used for rnn/lstm)*\n","**  Offline: data is coming from both side. Therefore, No. of sequence of samples used for rnn/lstm = seq_len taken\n","** Online: data is coming from one side.\n","No. of sequence of samples used for rnn/lstm = (seq_len+1)/2, i.e. if seq_len = 7 then  no. of samples used for rnn/lstm = 4\n","* SNR(signal to noise ratio)"]},{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"-ZngTMWunzO4","executionInfo":{"status":"ok","timestamp":1645620820415,"user_tz":-330,"elapsed":4310,"user":{"displayName":"2K19/ME/275 YASH KUMAR","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"06719952441128256168"}},"outputId":"b704010a-253c-4be1-a14f-bcbcffb31901"},"execution_count":2,"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"]}]},{"cell_type":"markdown","metadata":{"id":"4cbgwZWWfWpp"},"source":["\n","# ARGPARSE"]},{"cell_type":"code","metadata":{"id":"BkaijrX98151","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1645620824511,"user_tz":-330,"elapsed":4107,"user":{"displayName":"2K19/ME/275 YASH KUMAR","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"06719952441128256168"}},"outputId":"8bfd5d7a-8a8c-4b81-ba9f-f91281c7e786"},"source":["! pip install cmocean\n","import pdb\n","import pandas as pd\n","import yaml\n","import h5py\n","import numpy as np\n","from numpy import sum, isrealobj, sqrt\n","from numpy.random import standard_normal\n","import scipy.linalg as la\n","\n","import os\n","import os.path as osp\n","from os.path import dirname\n","\n","import torch\n","import torch as T\n","from torch import nn\n","from torch.utils.data import DataLoader\n","from torch.optim.lr_scheduler import ExponentialLR\n","from torch.optim.lr_scheduler import MultiplicativeLR\n","from torch.optim.lr_scheduler import LambdaLR\n","import torch.nn.functional as F\n","from torch.utils.data.dataset import Dataset\n","from torch.autograd import Variable\n","from torch.nn.parameter import Parameter, UninitializedParameter\n","\n","import matplotlib.pyplot as plt\n","from matplotlib import rc\n","from matplotlib import cm\n","import matplotlib as mpl\n","import cmocean\n","import time\n","import random\n","import argparse\n","from itertools import count\n","from itertools import chain\n","from skimage.transform import resize\n","from secrets import token_hex"],"execution_count":3,"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: cmocean in /usr/local/lib/python3.7/dist-packages (2.0)\n"]}]},{"cell_type":"code","metadata":{"id":"wFvFfmNPR04T","executionInfo":{"status":"ok","timestamp":1645620824513,"user_tz":-330,"elapsed":8,"user":{"displayName":"2K19/ME/275 YASH KUMAR","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"06719952441128256168"}}},"source":["\"\"\"\n","Change directory path \n","prjct_dir: saving weights and results \n","data_dir: data \n","\"\"\"\n","parser = argparse.ArgumentParser('ARE')\n","\n","prjct_dir = '../content/drive/My Drive/Colab Notebooks/are'\n","data_dir = '../content/drive/My Drive/Data/are_data'\n","\n","parser.add_argument('--prjct_dir', type=str, default=prjct_dir)\n","parser.add_argument('--data_dir', type=str, default=data_dir)\n","\n","parser.add_argument('--exp', type=str, choices=['mthd__snsr', 'snsr__seq_len', 'mthd__snr', \n","                                                'mthd__tr_samp', 'mthd__bn',\n","                                                'rndm_trials', 'mthd__snsr_with_noise'], default='mthd__snsr')\n","parser.add_argument('--operation_mode', type=str, choices=['offline', 'online'], default='online')  \n","parser.add_argument('--RNN', type=str, choices=['lstm', 'rnn'], default='lstm')\n","parser.add_argument('--gpu', type=int, default=0)\n","args = parser.parse_args(\"\")\n","\n","args.device = device = torch.device('cuda:' + str(args.gpu) if torch.cuda.is_available() else 'cpu')"],"execution_count":4,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"P5vicRPTfcT4"},"source":["# Utils"]},{"cell_type":"code","metadata":{"id":"e5JnLKnIVuii","executionInfo":{"status":"ok","timestamp":1645620833381,"user_tz":-330,"elapsed":11,"user":{"displayName":"2K19/ME/275 YASH KUMAR","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"06719952441128256168"}}},"source":["def get_path(method, string, **kwargs):\n","    \"\"\" \n","    determine path to directory based upon string\n","    e.g. W=1 in kwargs return path of dir. to save weights\n","\n","    Returns: path to directory\n","    \"\"\"\n","\n","    dir1 = args.data_type\n","    dir2 = args.exp\n","    dir3 = osp.join(method, args.operation_mode, args.RNN) if method == 'are' else method \n","    if args.exp == 'mthd__tr_samp':\n","        if hasattr(args, 'tr_samp'): add = 'tr_samp{}'.format(args.tr_samp)  \n","        else: raise Exception('number of training samples are required')\n","    if args.exp == 'mthd__snsr_loc':\n","        seed = kwargs.get('seed', 0)\n","        dir3 = osp.join(dir3, f'seed{seed}')\n","    if args.exp == 'SDnets':\n","        SDnet = kwargs.get('SDnet', '')\n","        dir3 = osp.join(dir3, f'net{str(SDnet)}')\n","\n","    if string == 'W':\n","        path = osp.join(args.prjct_dir, 'weights_results', dir1, dir2, dir3)\n","\n","    elif string == 'W_AE':\n","        path = osp.join(args.prjct_dir, 'weights_results', dir1, dir2, method, 'autoencoder')\n","\n","    elif string == 'I':\n","        path = osp.join(args.prjct_dir, 'weights_results', dir1, dir2, dir3)\n","\n","    elif string == 'P':\n","        path = osp.join(args.prjct_dir, 'weights_results', dir1, dir2)\n","\n","    else:\n","        raise ValueError('type of path not recongnized')\n","\n","    path = osp.join(path, add) if args.exp == 'mthd__tr_samp' else path\n","    path = osp.join(path, kwargs.get('add_dir')) if kwargs.get('add_dir') else path\n","    if not os.path.exists(path):\n","        os.makedirs(path)\n","\n","    return path\n","\n","def get_name(method, string, **kwargs):\n","    \"\"\" \n","    determine name of file \n","    e.g. W=1 in kwargs return name of file of autoencoder weights\n","\n","    Returns: path to directory\n","    \"\"\"\n","    \n","    if string == 'W':\n","\n","        n_sensor = kwargs.get('n_sensor')\n","        seq_len = kwargs.get('seq_len')\n","        bottle_neck = kwargs.get('bottle_neck')\n","\n","        if method == 'are' or method == 'AFE':\n","            return 'weights_s{}_sq{}_bn{}'.format(n_sensor, seq_len, bottle_neck) \n","\n","        elif method == 'pds':\n","            return 'weights_s{}_bn{}'.format(n_sensor, bottle_neck)\n","\n","        elif method == 'sd':\n","            if kwargs.get('drop'):\n","                return 'weights_s{}_drop'.format(n_sensor)\n","            else:\n","                return 'weights_s{}'.format(n_sensor)\n","\n","        elif method == 'FCNN':\n","            return 'weights_s{}'.format(n_sensor)\n","\n","    elif string == 'W_AE':\n","\n","        bn = kwargs.get('bottle_neck')\n","        dropout = kwargs.get('drop')\n","        \n","        if dropout:\n","            return 'weights_bn{}_drop{}'.format(bn, dropout)\n","        else:\n","            return 'weights_bn{}'.format(bn)\n","\n"],"execution_count":5,"outputs":[]},{"cell_type":"code","metadata":{"id":"msy3wuAAHVLn","executionInfo":{"status":"ok","timestamp":1645620834076,"user_tz":-330,"elapsed":703,"user":{"displayName":"2K19/ME/275 YASH KUMAR","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"06719952441128256168"}}},"source":["def get_data():\n","  \"\"\" \n","  load data from data_dir\n","  data is strored in args.raw_data\n","  \"\"\"\n","  path_data = args.data_dir\n","\n","  if hasattr(args, 'raw_data') and hasattr(args, 'last_data_type') and args.data_type == args.last_data_type:\n","        print('using previously loaded data')\n","\n","  else:\n","\n","        start_time = time.time()\n","        if args.data_type == 'transient':\n","            print('loading transient flow past cylinder data...')\n","            td180 = np.load(osp.join(path_data, 'data180.npy'))\n","            td190 = np.load(osp.join(path_data, 'data190.npy'))\n","            td200 = np.load(osp.join(path_data, 'data200.npy'))\n","            td185 = np.load(osp.join(path_data, 'data185.npy'))\n","            td195 = np.load(osp.join(path_data, 'data195.npy'))\n","            args.raw_data = (td180, td190, td200, td185, td195)\n","            print('data loaded')\n","\n","        elif args.data_type == 'periodic':\n","            args.raw_data = np.load(osp.join(path_data, 'data190+.npy'))\n","            print('data loaded')\n","\n","        elif args.data_type == 'turbulence':\n","            args.raw_data = np.load(osp.join(path_data, 'tb_128_5000.npy'))\n","            print('data loaded')\n","        \n","        elif args.data_type == 'sea_temp':\n","            land_sea = np.load(osp.join(path_data, 'SSTdata1990.npz'))['arr_0']\n","\n","            t = land_sea[0]\n","            n_pixel = len(t)\n","            len_data = len(land_sea[:, 0])\n","            land_pos = np.where(t > 1e+30)[0] \n","\n","            full_idx = np.arange(n_pixel)\n","            args.sea_idx = np.delete(full_idx, land_pos)\n","            args.raw_data = np.delete(land_sea, [land_pos], 1)\n","            print('data loaded')\n","\n","        elif args.data_type == 'burgers1D':\n","            args.raw_data = np.load(osp.join(path_data, 'run49_step51_tend0.5_nu0.01.npy'))\n","            print('data loaded')\n","\n","        else:\n","            print('data_type is not recognised')\n","\n","        print(\"--- %s seconds --\" % (time.time() - start_time))  \n","        args.last_data_type = args.data_type\n","\n","\n","def img_dims(flat=0):\n","    \"\"\" returns data image dimentions \"\"\"\n","\n","    if args.data_type == 'periodic':\n","        nx, ny = 251, 168\n","        vec_dim = nx*ny\n","    if args.data_type == 'transient':\n","        nx, ny = 502, 252\n","        vec_dim = nx*ny\n","    if args.data_type == 'sea_temp':\n","        nx, ny = 360, 180\n","        vec_dim = 44219\n","    if args.data_type == 'burgers1D':\n","        nx, ny = 128, 1\n","        vec_dim = nx*ny\n","    \n","    if flat: return vec_dim\n","    return nx, ny\n","\n","if 0:\n","    args.data_type = 'sea_temp'\n","    get_data()\n","    print(f'data shape: {args.raw_data}')\n","\n","if 0:\n","    args.data_type = 'transient'\n","    args.exp = 'None'\n","    get_data()\n","\n","    savePath = dirname(get_path(0, 'P'))\n","    print(savePath, args.raw_data[0].shape)\n","    plot_data = args.raw_data[0], list(range(55, 380, 55))\n","    Plots().transientFlow_dataVisualize(1, savePath, plot_data, figsize=(22, 7))"],"execution_count":6,"outputs":[]},{"cell_type":"code","metadata":{"id":"_Do3IO-R2Dlq","executionInfo":{"status":"ok","timestamp":1645620834078,"user_tz":-330,"elapsed":15,"user":{"displayName":"2K19/ME/275 YASH KUMAR","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"06719952441128256168"}}},"source":["def process_data(**kwargs):\n","    \"\"\"\n","    Call :func:`get_data` to load data.\n","    Add noise to data of certain level.\n","    Split data for training and testing.\n","    Calculate mean/std of trainAE.\n","\n","    Vars: W: number of pixels.\n","\n","    Returns:\n","        trainAE (ndarray): (numSampTrain, W)\n","        validAE (ndarray): (numSampValid, W)\n","        testAE (ndarray): (numSampTest, W)\n","        stats (ndarray): (2, W) [mean, std]\n","    \"\"\"\n","  \n","    noisy_data = SNR = kwargs.get('SNR')\n","    plot, save = kwargs.get('plot'), kwargs.get('save')\n","\n","    if args.data_type == 'periodic': \n","\n","        get_data()\n","        td = args.raw_data  \n","        m, n = img_dims()\n","        tr_samp = args.tr_samp if hasattr(args, 'tr_samp') else 180\n","\n","        if noisy_data:\n","            print('in process_data-- noise data', 'SNR: ', SNR)\n","            \n","            td_ = np.zeros_like(td)\n","            for i in range(len(td)): td_[i,:] = awgn(td[i],kwargs.get('SNR',10))\n","\n","            image, name = td_[tr_samp+60:tr_samp+120,:][39,:], 'im_true_39_SNR{}'.format(SNR)\n","            if SNR in kwargs.get('plot_SNR', [0]): show_save_image('', image, plot, save, path=get_path('', 'P'), name=name, contour=0) \n","\n","        else :\n","            print('in process_data-- no noise data')\n","            td_ = td\n","\n","        trainAE = td_[0:tr_samp,:]\n","        validAE = td_[tr_samp:tr_samp+60,:]\n","        testAE = td_[tr_samp+60:tr_samp+120,:]  \n"," \n","\n","    elif args.data_type == 'transient':\n","\n","        get_data()\n","        td180, td190, td200, td185, td195 = args.raw_data\n","        tr_samp = len(td180)*3\n","        args.n_TrainDatasets = 3\n","        args.n_TestDatasets = 1\n","\n","        if noisy_data:\n","            print('in process_data-- noise data', 'SNR =',SNR)\n","            \n","            td1_ = td2_ = td3_ = td4_ = td5_ = np.zeros_like(td180)\n","            for i in range(len(td1_)):\n","                td1_[i,:] = awgn(td180[i],SNR)\n","                td2_[i,:] = awgn(td190[i],SNR)\n","                td3_[i,:] = awgn(td200[i],SNR)\n","                td4_[i,:] = awgn(td185[i],SNR)\n","                td5_[i,:] = awgn(td195[i],SNR)\n","            image, name = td5_[335,:], 'im_true_335_SNR{}'.format(SNR)\n","            if SNR in kwargs.get('plot_SNR', [0]): show_save_image('', image, plot, save, path=get_path('', 'P'), name=name, contour=0) \n","\n","        else :\n","            print('in process_data-- no noise data')\n","            td1_, td2_, td3_, td4_, td5_ = td180, td190, td200, td185, td195\n","        \n","        trainAE = np.concatenate((td1_, td2_, td3_), axis=0)\n","        validAE = td4_\n","        testAE = td5_ \n","\n","\n","    elif args.data_type == 'sea_temp':\n","\n","        get_data()\n","        td = args.raw_data\n","        tr_samp = args.tr_samp if hasattr(args, 'tr_samp') else 400\n","\n","        if noisy_data:\n","            print('in process_data-- noise data', 'SNR: ', SNR)\n","            \n","            td_ = np.zeros_like(td)\n","            for i in range(len(td)):\n","                    td_[i,:] = awgn(td[i], SNR)\n","                \n","            image, name = td_[tr_samp+100:tr_samp+200,:][0], 'im_true_0_SNR{}'.format(SNR)\n","            if SNR in kwargs.get('plot_SNR', [0]): show_save_image('', image, plot, save, path=get_path('', 'P'), name=name)\n","\n","        else :\n","            print('in process_data-- no noise data')\n","            td_ = td\n","        \n","        trainAE = td_[0:tr_samp+0,:]\n","        validAE = td_[tr_samp+0:tr_samp+100,:]\n","        testAE = td_[tr_samp+100:tr_samp+300,:]\n","\n","    if args.data_type == 'burgers1D': \n","\n","        get_data()\n","        td = args.raw_data  \n","        m, n = img_dims()\n","\n","        tr_samp = args.tr_samp if hasattr(args, 'tr_samp') else 51*32\n","        args.n_TrainDatasets = 32\n","        args.n_TestDatasets = 4\n","\n","        if noisy_data:\n","            print('in process_data-- noise data', 'SNR: ', SNR)\n","            \n","            td_ = np.zeros_like(td)\n","            for i in range(len(td)): td_[i,:] = awgn(td[i],kwargs.get('SNR'))\n","\n","        else :\n","            print('in process_data-- no noise data')\n","            td_ = td\n","\n","        trainAE = td_[0:tr_samp,:]\n","        validAE = td_[tr_samp:tr_samp+15,:]\n","        testAE1 = td_[tr_samp+51*2:tr_samp+51*3,:]\n","        testAE2 = td_[tr_samp+51*7:tr_samp+51*8,:]\n","        testAE3 = td_[tr_samp+51*9:tr_samp+51*10,:]\n","        testAE4 = td_[tr_samp+51*15:tr_samp+51*16,:]\n","        testAE = np.concatenate((testAE1, testAE2, testAE3, testAE4), axis=0)\n","\n","  \n","    AE_mean = np.mean(trainAE, axis = 0)  \n","    AE_std = np.std(trainAE, axis = 0)  \n","\n","    if args.data_type == 'sea_temp': \n","        AE_std = np.ones_like(AE_std)\n","\n","    stats = np.array([AE_mean, AE_std])\n","    stats[stats==0] = 0.0000001\n","    print('number of training samples:', tr_samp)\n","\n","    return trainAE, validAE, testAE, stats"],"execution_count":7,"outputs":[]},{"cell_type":"code","metadata":{"id":"PShg7PEiLssv","executionInfo":{"status":"ok","timestamp":1645620834079,"user_tz":-330,"elapsed":14,"user":{"displayName":"2K19/ME/275 YASH KUMAR","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"06719952441128256168"}}},"source":["def sensor_data(trainAE_stan, validAE_stan, testAE_stan, sensor_num, seed=0):\n","\n","    n_train, n_valid, n_test = trainAE_stan.shape[0], validAE_stan.shape[0], testAE_stan.shape[0]\n","\n","    sqrt_s = int(sensor_num**0.5)\n","    strain_in = (resize(trainAE_stan, (n_train, sqrt_s, sqrt_s))).reshape(n_train, sensor_num)\n","    svalid_in = (resize(validAE_stan, (n_valid, sqrt_s, sqrt_s))).reshape(n_valid, sensor_num)\n","    stest_in = (resize(testAE_stan, (n_test, sqrt_s, sqrt_s))).reshape(n_test, sensor_num)\n","\n","    return strain_in, svalid_in, stest_in\n","\n","\n","def standardize(a, b, c, stats):\n","\n","  a_stan = (a - stats[0, :])/stats[1, :]\n","  b_stan = (b - stats[0, :])/stats[1, :]\n","  c_stan = (c - stats[0, :])/stats[1, :]\n","  \n","  return a_stan, b_stan, c_stan \n","  \n","\n","class Dict2Class(object):\n","\t\n","\tdef __init__(self, my_dict):\n","\t\tfor key in my_dict:\n","\t\t\tsetattr(self, key, my_dict[key])"],"execution_count":8,"outputs":[]},{"cell_type":"code","metadata":{"id":"J4F_GOvGvXbO","executionInfo":{"status":"ok","timestamp":1645620834081,"user_tz":-330,"elapsed":15,"user":{"displayName":"2K19/ME/275 YASH KUMAR","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"06719952441128256168"}}},"source":["def sensor_cord_data(sensor_num, seed=0):\n","    \"\"\" \n","    Sensor pixel co-ordinates for all data_types.\n","\n","    Returns:\n","        s_idx_in_flatten (ndarray, int): (numSensor,) \n","        cords (ndarray, int): (numSensor, 2)\n","    \"\"\"\n","    nx, ny = m, n = img_dims()\n","    print(f'sensor Location seed: {seed}')\n","\n","    if args.data_type == 'periodic':\n","\n","        theata = np.linspace(0, 2*np.pi, 300)\n","        x_cord = np.round(25 * np.cos(theata)) + 0\n","        y_cord = np.round(25 * np.sin(theata)) + n/2\n","        cords_ = np.vstack((x_cord,y_cord)).T\n","        cords_ = np.unique(cords_, axis=0)\n","        idx = cords_[:,0] > 0\n","        cords_ = cords_[idx,:]\n","        # Changing the seed will change the position of \n","        # sensors and networks will have to be retrained.\n","        np.random.seed(3265 + seed)  \n","        idx = np.random.choice(range(cords_.shape[0]), sensor_num, False)\n","        cords = np.int64(cords_[idx,:])\n","\n","        mask = np.zeros((m,n))\n","        mask[cords[:,0], cords[:,1]] = 1\n","        s_idx_in_flatten = np.where(mask.reshape(-1) == 1)\n","        s_idx_in_flatten = np.asarray(s_idx_in_flatten).ravel()\n","\n","    elif args.data_type == 'transient':\n","\n","        theata = np.linspace(0, 2*np.pi, 300)\n","        x_cord = np.round(24 * np.cos(theata)) + 0\n","        y_cord = np.round(24 * np.sin(theata)) + n/2\n","        cords_ = np.vstack((x_cord,y_cord)).T\n","        cords_ = np.unique(cords_, axis=0)\n","        idx = cords_[:,0] > 0\n","        cords_ = cords_[idx,:]\n","        # Changing the seed will change the position of \n","        # sensors and networks will have to be retrained.\n","        np.random.seed(323 + seed)\n","        idx = np.random.choice(range(cords_.shape[0]), sensor_num, False)\n","        cords = np.int64(cords_[idx,:])\n","\n","        mask = np.zeros((m,n))\n","        mask[cords[:,0], cords[:,1]] = 1\n","        s_idx_in_flatten = np.where(mask.reshape(-1) == 1)\n","        s_idx_in_flatten = np.asarray(s_idx_in_flatten).ravel()\n","\n","    elif args.data_type == 'sea_temp': \n","\n","        idices = np.arange(0, 44219)\n","        # Changing the seed will change the position of \n","        # sensors and networks will have to be retrained.\n","        np.random.seed(73 + seed)\n","        s_idx_in_flatten = np.random.choice(idices, sensor_num, False)\n","\n","        flat_sea_mask = np.zeros((44219))\n","        flat_sea_mask[s_idx_in_flatten] = 1\n","        flat_mask = np.zeros((m*n))\n","        for i, j in zip(args.sea_idx, count(0, 1)):\n","            flat_mask[i] = flat_sea_mask[j]\n","\n","        mask = flat_mask.reshape(n, m)\n","        cords = np.where(mask == 1)\n","        cords = np.asarray(cords)\n","\n","        # plt.figure()\n","        # plt.scatter(cords[1], cords[0])\n","        # plt.show()\n","\n","    elif args.data_type == 'burgers1D':\n","\n","        idices = np.arange(0, nx*ny)\n","        np.random.seed(seed)\n","        # s_idx_in_flatten = np.random.choice(idices, sensor_num, False)\n","        s_idx_in_flatten = np.arange(0, nx*ny, round(nx*ny/(sensor_num+1)-0.6))[1:sensor_num+1]\n","        sensor_mask = np.zeros((nx*ny))\n","        sensor_mask[s_idx_in_flatten] = 1\n","        cords = np.where(sensor_mask.reshape(nx, ny) == 1)\n","        cords = np.asarray(cords)\n","    pdb.set_trace()\n","    return s_idx_in_flatten, cords\n","\n","if 0:\n","    \"\"\" plot sensor locations \"\"\"\n","    args.data_type = 'transient'\n","\n","    for seed in [10, 46, 17, 4, 50]:\n","        _, sensorCoords = sensor_cord_data(2, seed=seed)\n","\n","        nx, ny = img_dims()\n","        imData = np.zeros((nx*ny))\n","\n","        fig, ax = plt.subplots()\n","        Plots().cylinderFlow_imshow(imData, ax, cmocean.cm.balance, -1, 1, coords=sensorCoords)\n","\n","if 0:\n","    \"\"\" plot sensor locations \"\"\"\n","    args.data_type = 'burgers1D'\n","\n","    for seed in [10]:\n","        _, sensorCoords = sensor_cord_data(10, seed=seed)\n","        print(sensorCoords)\n","\n","        nx, ny = img_dims()\n","        imData = np.zeros((nx*ny))\n","\n","        fig, ax = plt.subplots()\n","        Plots().burgers1D_imshow(imData, ax, 0, -1, 1, coords=sensorCoords)"],"execution_count":9,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"hcRcLeSFH2Ap"},"source":["## Plot"]},{"cell_type":"code","metadata":{"id":"VMSXDWeQks7M","executionInfo":{"status":"ok","timestamp":1645620837436,"user_tz":-330,"elapsed":3368,"user":{"displayName":"2K19/ME/275 YASH KUMAR","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"06719952441128256168"}}},"source":["class Plots:\n","    \"\"\" Plots data samples by creating seperate object for every data type\n","    \"\"\"\n","    marker = ['-o', '-*', '-,', '-x', '-+', '-P', '-s', '-D', '-p', '-v']\n","\n","    def save_show(self, plot, save_Path, fig):\n","        if save_Path:\n","            fig.savefig(save_Path+'.pdf', format='pdf')\n","            print(f'saved plot: {save_Path}.pdf')\n","        fig.show() if plot else 0\n","\n","\n","    def plot_graph(self, rerror_train, rerror_test, savePath=None):\n","        fig = plt.figure()\n","        plt.plot(rerror_train, lw=2, label='Trainings error', color='#377eb8',)  \n","        plt.plot(rerror_test, lw=2, label='Valid error', color='#e41a1c',)            \n","        plt.tick_params(axis='x', labelsize=14) \n","        plt.tick_params(axis='y', labelsize=14) \n","        plt.locator_params(axis='y', nbins=10)\n","        plt.locator_params(axis='x', nbins=10)\n","        \n","        plt.ylabel('Error', fontsize=14)\n","        plt.xlabel('Epochs', fontsize=14)\n","        plt.grid(False)\n","        plt.yscale(\"log\")\n","        plt.legend(fontsize=14)\n","        fig.tight_layout()\n","\n","        self.save_show(1, savePath, fig)\n","\n","\n","    def cylinderFlow_imshow(self, imData, axes, cmap, v_min, v_max, **kwargs):\n","\n","        nx, ny = img_dims()\n","        imD = imData.reshape((nx, ny))\n","\n","        im = axes.imshow(imD.T, cmap=cmap, interpolation='none', vmin=v_min, vmax=v_max)\n","        \n","        if kwargs.get('contour', 1): \n","            mX, mY = np.meshgrid(np.arange(0, nx, 1), np.arange(0, ny, 1))\n","            try: axes.contourf(mX, mY, imD.T, 80, cmap=cmap, alpha=1, vmin=v_min+3, vmax=v_max-3) \n","            except: axes.contourf(mX, mY, imD.T, 80, cmap=cmap, alpha=1, vmin=v_min, vmax=v_max) \n","        if 'coords' in kwargs.keys():\n","            coords = kwargs.get('coords')\n","            axes.scatter(coords[:,0], coords[:,1], marker='.', color='#ff7f00', s=100, zorder=5)\n","        axes.axis('off')\n","\n","\n","    def seaTemp_imshow(self, imData, axes, cmap, v_min, v_max, **kwargs):\n","\n","        nx, ny = img_dims()\n","        recon = np.ones((nx*ny))*35\n","        recon[args.sea_idx] = imData\n","        recon_im = recon.reshape(ny, nx)\n","        # recon_im = np.flip(recon_im, axis=0)\n","\n","        axes.imshow(recon_im, cmap=cmap, vmin=v_min, vmax=v_max)\n","        axes.invert_yaxis()\n","        if 'coords' in kwargs.keys():\n","            coords = kwargs.get('coords')\n","            axes.scatter(coords[1], coords[0], marker='.', color='#ff7f00', s=40, zorder=5)\n","        axes.axis('off')\n","\n","\n","    def burgers1D_imshow(self, imData, axes, cmap, v_min, v_max, linestyle='-', label='', **kwargs):\n","\n","        nx, ny = img_dims()\n","        imD = imData\n","\n","        # im = axes.plot(imD)\n","        axes.plot(imD, label=label, linestyle=linestyle)\n","        \n","        if 'coords' in kwargs.keys():\n","            coords = kwargs.get('coords')\n","            axes.scatter(coords[0], coords[1], marker='.', color='#ff7f00', s=100, zorder=5)\n","        # axes.axis('off')\n","\n","    \n","    def sst_fig(self, show: bool, savePath: str, plot_data: tuple):\n","\n","        ARE_dataPred, PDS_dataPred, SD_dataPred, plotParams = plot_data\n","        idx = plotParams\n","\n","        mpl.rcParams['font.family'] = ['serif'] # default is sans-serif\n","        rc('text', usetex=False)\n","        rc('font', size=8)\n","\n","\n","        fig, ax = plt.subplots(2, 2, figsize=(10, 6))\n","        fig.subplots_adjust(wspace=0.1)\n","\n","        Ar = np.array(ARE_dataPred['pred'])[idx]\n","        Pd = np.array(PDS_dataPred['pred'])[idx]\n","        Sd = np.array(SD_dataPred['pred'])[idx]\n","        Tr = np.array(ARE_dataPred['true'])[idx]\n","\n","        coords = np.array(ARE_dataPred['coords'])  # sensor co-ordinates\n","        # assert Tr == PDS_dataPred['true'][idx]\n","        \n","        cmap = 'terrain'\n","        v_max = np.max(np.array([Ar, Pd, Sd, Tr]))\n","        v_min = np.min(np.array([Ar, Pd, Sd, Tr]))\n","        # print(f'v_min: {v_min} \\n v_max: {v_max}')\n","        # print(f'{np.max(np.array([Ar]))}, {np.max(np.array([Pd]))}, {np.max(np.array([Sd]))}, {np.max(np.array([Tr]))}')\n","\n","        self.seaTemp_imshow(Pd, ax[0, 0], cmap, v_min, v_max, coords=coords)\n","        self.seaTemp_imshow(Ar, ax[0, 1], cmap, v_min, v_max, coords=coords)\n","        self.seaTemp_imshow(Sd, ax[1, 0], cmap, v_min, v_max, coords=coords)\n","        self.seaTemp_imshow(Tr, ax[1, 1], cmap, v_min, v_max, coords=coords)\n","\n","        # ax.set_title('Title with special font: {}'.format(fname), fontproperties = prop, fontsize = 14)\n","        ax[0, 0].set_title('PDS')\n","        ax[0, 1].set_title('ARE')\n","        ax[1, 0].set_title('SD')\n","        ax[1, 1].set_title('Ground Truth')\n","\n","        # ---------------- color bar\n","        p0 = ax[0, 1].get_position().get_points().flatten()\n","        p1 = ax[1, 1].get_position().get_points().flatten()\n","        ax_cbar = fig.add_axes([p1[2]+0.0075, p1[1], 0.01, p0[3]-p1[1]])\n","        ticks = np.linspace(0, 1, 5)\n","        tickLabels = np.linspace(v_min, v_max, 5)\n","        tickLabels = [\"{:02.2f}\".format(t0) for t0 in tickLabels]\n","        cbar = mpl.colorbar.ColorbarBase(ax_cbar, cmap=plt.get_cmap(cmap), orientation='vertical', ticks=ticks)\n","        cbar.set_ticklabels(tickLabels)\n","\n","        self.save_show(show, savePath, fig)\n","\n","\n","    def periodicFlow_fig(self, show: bool, savePath: str, plot_data: tuple, figsize=(9, 6)):\n","\n","        ARE_dataPred, PDS_dataPred, SD_dataPred, plotParams = plot_data\n","        idx = plotParams\n","\n","        mpl.rcParams['font.family'] = ['serif'] # default is sans-serif\n","        rc('text', usetex=False)\n","        rc('font', size=8)\n","\n","\n","        fig, ax = plt.subplots(2, 2, figsize=figsize)\n","        fig.subplots_adjust(wspace=0.01)\n","\n","        Ar = np.array(ARE_dataPred['pred'])[idx]\n","        Pd = np.array(PDS_dataPred['pred'])[idx]\n","        Sd = np.array(SD_dataPred['pred'])[idx]\n","        Tr = np.array(ARE_dataPred['true'])[idx]\n","\n","        coords = np.array(ARE_dataPred['coords'])  # sensor co-ordinates\n","        # assert Tr == PDS_dataPred['true'][idx]\n","        \n","        cmap = cmocean.cm.balance\n","        v_max = np.max(np.abs(np.array([Ar, Pd, Sd, Tr]))) * 0.65\n","        v_min = -v_max\n","        print(f'v_min: {v_min} \\n v_max: {v_max}')\n","\n","        self.cylinderFlow_imshow(Pd, ax[0, 0], cmap, v_min, v_max, coords=coords)\n","        self.cylinderFlow_imshow(Ar, ax[0, 1], cmap, v_min, v_max, coords=coords)\n","        # self.cylinderFlow_imshow(Ar, ax[0, 1], cmap, np.min(Ar), np.max(Ar), coords=coords)\n","        self.cylinderFlow_imshow(Sd, ax[1, 0], cmap, v_min, v_max, coords=coords)\n","        self.cylinderFlow_imshow(Tr, ax[1, 1], cmap, v_min, v_max, coords=coords)\n","\n","        # ax.set_title('Title with special font: {}'.format(fname), fontproperties = prop, fontsize = 14)\n","        ax[0, 0].set_title('PDS')\n","        ax[0, 1].set_title('ARE')\n","        ax[1, 0].set_title('SD')\n","        ax[1, 1].set_title('Ground Truth')\n","\n","        # ---------------- color bar\n","        p0 = ax[0, 1].get_position().get_points().flatten()\n","        p1 = ax[1, 1].get_position().get_points().flatten()\n","        ax_cbar = fig.add_axes([p1[2]+0.0075, p1[1], 0.01, p0[3]-p1[1]])\n","        ticks = np.linspace(0, 1, 5)\n","        tickLabels = np.linspace(v_min, v_max, 5)\n","        tickLabels = [\"{:02.2f}\".format(t0) for t0 in tickLabels]\n","        cbar = mpl.colorbar.ColorbarBase(ax_cbar, cmap=plt.get_cmap(cmap), orientation='vertical', ticks=ticks)\n","        cbar.set_ticklabels(tickLabels)\n","\n","        self.save_show(show, savePath, fig)\n","\n","\n","    def transientFlow_graphPlot_senLoc(self, show, savePath, plot_data, plotParams):\n","        \n","        keys = list(plot_data.keys())\n","        data = lambda i: plot_data[keys[i]]\n","\n","        fig, ax = plt.subplots()\n","\n","        for i in range(len(plot_data)):\n","            xx = range(len(data(i)['x_ticks_lables']))\n","            ax.plot(xx, data(i)['data'], self.marker[i], label=data(i)['legend'], color=data(i)['color'])\n","        ax.legend()\n","        ax.set_xticks(range(len(data(i)['x_ticks_lables'])))\n","        ax.set_xticklabels(data(0)['x_ticks_lables'])\n","\n","        ax.set_xlabel(plotParams['xlabel'], fontsize=14)\n","        ax.set_ylabel(plotParams['ylabel'], fontsize=14)\n","\n","        self.save_show(show, savePath, fig)\n","\n","\n","    def transientFlow_imgPlot_senLoc(self, show, savePath, plot_data, figsize=(18, 9)):\n","\n","        ARE_dataPred, plotParams = plot_data\n","        idx = plotParams\n","\n","        mpl.rcParams['font.family'] = ['serif'] # default is sans-serif\n","        rc('text', usetex=False)\n","        rc('font', size=8)\n","\n","        fig, ax = plt.subplots(3, 2, figsize=figsize)\n","        fig.subplots_adjust(wspace=0.001)\n","\n","        Ar = []\n","        for i in range(5):\n","            Ar.append(np.array(ARE_dataPred[i]['pred'])[idx])\n","        Tr = np.array(ARE_dataPred[0]['true'])[idx]\n","\n","        sensorCoords = lambda i: np.array(ARE_dataPred[i]['coords'])  # sensor co-ordinates\n","        # assert Tr == PDS_dataPred['true'][idx]\n","        \n","        cmap = cmocean.cm.balance\n","        v_max = np.max(np.abs(np.array([Ar[0], Ar[1], Ar[2], Ar[3], Ar[4], Tr]))) * 0.65\n","        v_min = -v_max\n","        print(f'v_min: {v_min} \\n v_max: {v_max}')\n","\n","        self.cylinderFlow_imshow(Tr, ax[0, 0], cmap, v_min, v_max)\n","        self.cylinderFlow_imshow(Ar[0], ax[0, 1], cmap, v_min, v_max, coords=sensorCoords(0))\n","        self.cylinderFlow_imshow(Ar[1], ax[1, 0], cmap, v_min, v_max, coords=sensorCoords(1))\n","        self.cylinderFlow_imshow(Ar[2], ax[1, 1], cmap, v_min, v_max, coords=sensorCoords(2))\n","        self.cylinderFlow_imshow(Ar[3], ax[2, 0], cmap, v_min, v_max, coords=sensorCoords(3))\n","        self.cylinderFlow_imshow(Ar[4], ax[2, 1], cmap, v_min, v_max, coords=sensorCoords(4))\n","\n","        # ax.set_title('Title with special font: {}'.format(fname), fontproperties = prop, fontsize = 14)\n","        ax[0, 0].set_title('Ground Truth')\n","        ax[0, 1].set_title('ARE1')\n","        ax[1, 0].set_title('ARE2')\n","        ax[1, 1].set_title('ARE3')\n","        ax[2, 0].set_title('ARE4')\n","        ax[2, 1].set_title('ARE5')\n","\n","        # ---------------- color bar\n","        p0 = ax[0, 1].get_position().get_points().flatten()\n","        p1 = ax[2, 1].get_position().get_points().flatten()\n","        ax_cbar = fig.add_axes([p1[2]+0.009, p1[1], 0.01, p0[3]-p1[1]])\n","        ticks = np.linspace(0, 1, 5)\n","        tickLabels = np.linspace(v_min, v_max, 5)\n","        tickLabels = [\"{:02.2f}\".format(t0) for t0 in tickLabels]\n","        cbar = mpl.colorbar.ColorbarBase(ax_cbar, cmap=plt.get_cmap(cmap), orientation='vertical', ticks=ticks)\n","        cbar.set_ticklabels(tickLabels)\n","\n","        self.save_show(show, savePath, fig)\n","\n","    \n","    def transientFlow_imgPlot_mthd2(self, show, savePath, plot_data, figsize=(9, 9)):\n","\n","        AFE_dataPred, FCNN_dataPred, plotParams = plot_data\n","        idx = plotParams\n","\n","        mpl.rcParams['font.family'] = ['serif'] # default is sans-serif\n","        rc('text', usetex=False)\n","        rc('font', size=8)\n","\n","\n","        fig, ax = plt.subplots(2, 2, figsize=figsize)\n","        fig.subplots_adjust(wspace=0.01)\n","\n","        Af = np.array(AFE_dataPred['pred'])[idx]\n","        Fc = np.array(FCNN_dataPred['pred'])[idx]\n","        Tr = np.array(AFE_dataPred['true'])[idx]\n","\n","        coords = np.array(AFE_dataPred['coords'])  # sensor co-ordinates\n","        # assert Tr == PDS_dataPred['true'][idx]\n","        \n","        cmap = cmocean.cm.balance\n","        v_max = np.max(np.abs(np.array([Af, Fc, Tr]))) * 0.65\n","        v_min = -v_max\n","        print(f'v_min: {v_min} \\n v_max: {v_max}')\n","\n","        self.cylinderFlow_imshow(Tr, ax[0, 0], cmap, v_min, v_max, coords=coords)\n","        self.cylinderFlow_imshow(Af, ax[0, 1], cmap, v_min, v_max, coords=coords)\n","        self.cylinderFlow_imshow(Fc, ax[1, 0], cmap, v_min, v_max, coords=coords)\n","\n","        # ax.set_title('Title with special font: {}'.format(fname), fontproperties = prop, fontsize = 14)\n","        ax[0, 0].set_title('Ground Truth')\n","        ax[0, 1].set_title('AFE')\n","        ax[1, 0].set_title('FCNN')\n","\n","        # ---------------- color bar\n","        p0 = ax[0, 1].get_position().get_points().flatten()\n","        p1 = ax[1, 1].get_position().get_points().flatten()\n","        ax_cbar = fig.add_axes([p1[2]+0.0075, p1[1], 0.01, p0[3]-p1[1]])\n","        ticks = np.linspace(0, 1, 5)\n","        tickLabels = np.linspace(v_min, v_max, 5)\n","        tickLabels = [\"{:02.2f}\".format(t0) for t0 in tickLabels]\n","        cbar = mpl.colorbar.ColorbarBase(ax_cbar, cmap=plt.get_cmap(cmap), orientation='vertical', ticks=ticks)\n","        cbar.set_ticklabels(tickLabels)\n","\n","        self.save_show(show, savePath, fig)\n","\n","\n","    def transientFlow_dataVisualize(self, show, savePath, plot_data, figsize=(18, 9)):\n","\n","        flowData, idx_ls = plot_data\n","        flowData = flowData[idx_ls]\n","\n","        mpl.rcParams['font.family'] = ['serif'] # default is sans-serif\n","        rc('text', usetex=False)\n","        rc('font', size=14)\n","\n","        fig, ax = plt.subplots(2, 3, figsize=figsize)\n","        # fig.subplots_adjust()\n","        fig.subplots_adjust(wspace=0.01, hspace=0.01)\n","        \n","        cmap = cmocean.cm.balance\n","        v_max = np.max(np.abs(np.array([flowData]))) * 0.65\n","        v_min = -v_max\n","        print(f'v_min: {v_min} \\n v_max: {v_max}')\n","\n","        for i in range(3):\n","            self.cylinderFlow_imshow(flowData[i], ax[0, i], cmap, v_min, v_max)\n","            self.cylinderFlow_imshow(flowData[i+3], ax[1, i], cmap, v_min, v_max)\n","\n","        # ---------------- color bar\n","        p0 = ax[0, 2].get_position().get_points().flatten()\n","        p1 = ax[1, 2].get_position().get_points().flatten()\n","        ax_cbar = fig.add_axes([p1[2]+0.009, p1[1], 0.01, p0[3]-p1[1]])\n","        ticks = np.linspace(0, 1, 5)\n","        tickLabels = np.linspace(v_min, v_max, 5)\n","        tickLabels = [\"{:02.2f}\".format(t0) for t0 in tickLabels]\n","        cbar = mpl.colorbar.ColorbarBase(ax_cbar, cmap=plt.get_cmap(cmap), orientation='vertical', ticks=ticks)\n","        cbar.set_ticklabels(tickLabels)\n","\n","        self.save_show(show, savePath, fig)\n","\n","\n","    def burgers1D_imgPlot1(self, show, savePath, plot_data, figsize=(18, 9)):\n","\n","        ARE_predData, PDS_predData, SD_predData, plotParams = plot_data\n","        idx_ls = plotParams\n","\n","        mpl.rcParams['font.family'] = ['serif'] # default is sans-serif\n","        rc('text', usetex=False)\n","        rc('font', size=9)\n","\n","        fig, ax = plt.subplots(2, 2, figsize=figsize)\n","        fig.subplots_adjust(hspace=0.001, wspace=0.1)\n","\n","        Ar = []; Pd = []; Sd = []; Tr = []\n","        for idx in idx_ls:\n","            Ar.append(np.array(ARE_predData['pred'])[idx])\n","            Pd.append(np.array(PDS_predData['pred'])[idx])\n","            Sd.append(np.array(SD_predData['pred'])[idx])\n","            Tr.append(np.array(PDS_predData['true'])[idx])\n","\n","        sensorCoords = lambda i: np.array(ARE_predData['coords'][i])  # sensor co-ordinates\n","        # assert Tr == PDS_dataPred['true'][idx]\n","        \n","        cmap = 0\n","        v_max = 0\n","        v_min = 0\n","        print(f'v_min: {v_min} \\n v_max: {v_max}')\n","\n","        ax_ls = [ax[0, 0], ax[0, 1], ax[1, 0], ax[1, 1]]\n","        for i, idx in enumerate(idx_ls):\n","            # pdb.set_trace()\n","            self.burgers1D_imshow(Tr[i], ax_ls[i], cmap, v_min, v_max, linestyle='-', label='True')\n","            self.burgers1D_imshow(Ar[i], ax_ls[i], cmap, v_min, v_max, linestyle='--', label='ARE')\n","            self.burgers1D_imshow(Pd[i], ax_ls[i], cmap, v_min, v_max, linestyle='-.', label='PDS')\n","            self.burgers1D_imshow(Sd[i], ax_ls[i], cmap, v_min, v_max, linestyle=':', label='SD')\n","            ax_ls[i].legend(fontsize=10)\n","\n","        self.save_show(show, savePath, fig)\n","\n","\n","    def burgers1D_imgPlot(self, show, savePath, plot_data, figsize=(18, 9)):\n","\n","        ARE_predData, PDS_predData, SD_predData, plotParams = plot_data\n","        idx = plotParams\n","\n","        mpl.rcParams['font.family'] = ['serif'] # default is sans-serif\n","        rc('text', usetex=False)\n","        rc('font', size=9)\n","\n","        fig, ax = plt.subplots(2, 2, figsize=figsize)\n","        fig.subplots_adjust(hspace=0.001, wspace=0.1)\n","\n","        Ar = []; Pd = []; Sd = []; Tr = []\n","        Ar.append(np.array(ARE_predData['pred'])[idx])\n","        Pd.append(np.array(PDS_predData['pred'])[idx])\n","        Sd.append(np.array(SD_predData['pred'])[idx])\n","        Tr.append(np.array(PDS_predData['true'])[idx])\n","\n","        sensorCoords = lambda i: np.array(ARE_predData['coords'])  # sensor co-ordinates\n","        # assert Tr == PDS_dataPred['true'][idx]\n","        \n","        cmap = 0\n","        v_max = 0\n","        v_min = 0\n","        print(f'v_min: {v_min} \\n v_max: {v_max}')\n","\n","        ax_ls = [ax[0, 0], ax[0, 1], ax[1, 0], ax[1, 1]]\n","        # self.burgers1D_imshow(Tr[0], ax_ls[0], cmap, v_min, v_max, label='True', coords=sensorCoords(idx))\n","        # self.burgers1D_imshow(Ar[0], ax_ls[1], cmap, v_min, v_max, label='ARE', coords=sensorCoords(idx))\n","        # self.burgers1D_imshow(Pd[0], ax_ls[2], cmap, v_min, v_max, label='PDS', coords=sensorCoords(idx))\n","        # self.burgers1D_imshow(Sd[0], ax_ls[3], cmap, v_min, v_max, label='SD', coords=sensorCoords(idx))\n","        self.burgers1D_imshow(Tr[0], ax_ls[0], cmap, v_min, v_max, label='True')\n","        self.burgers1D_imshow(Ar[0], ax_ls[1], cmap, v_min, v_max, label='ARE')\n","        self.burgers1D_imshow(Pd[0], ax_ls[2], cmap, v_min, v_max, label='PDS')\n","        self.burgers1D_imshow(Sd[0], ax_ls[3], cmap, v_min, v_max, label='SD')\n","        \n","        ymin = np.min(Tr[0])*(1+0.2)\n","        ymax = np.max(Tr[0])*(1+0.2)\n","        for i in range(4):\n","            ax_ls[i].legend(fontsize=12)\n","            ax_ls[i].set_ylim([ymin, ymax])\n","\n","        self.save_show(show, savePath, fig)\n","\n","\n"],"execution_count":10,"outputs":[]},{"cell_type":"code","metadata":{"id":"uhHXW5YBfvnn","executionInfo":{"status":"ok","timestamp":1645620837438,"user_tz":-330,"elapsed":14,"user":{"displayName":"2K19/ME/275 YASH KUMAR","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"06719952441128256168"}}},"source":["\n","def show_save_image(method, images, plot, save, **kwargs):\n","\n","  istupl = True if isinstance(images, tuple) else False\n","\n","  len_tuple = len(images) if istupl else 1\n","  get_value = lambda index, tuple_ : tuple_[index] if istupl else tuple_\n","  i = kwargs.get('i', 0)\n","  stats = kwargs.get('stats', np.array([0]))\n","\n","  if istupl:\n","    assert len_tuple == len(save) == len(plot)\n","    if any(save):\n","      assert len(kwargs.get('name')) == len_tuple, 'mention all names of images for saving'\n","  else:\n","    if save: assert kwargs.get('name') \n","\n","  for idx in range(len_tuple):\n","      \n","        save_ = get_value(idx, save)\n","        plot_ = get_value(idx, plot)\n","        name = get_value(idx, kwargs.get('name', ['im']*len_tuple))\n","\n","        sensorCoords = cords = kwargs.get('cords', np.array([0]))\n","\n","        t = get_value(idx, images)\n","        t = t.cpu().data.numpy() if torch.is_tensor(t) else t\n","        if t.ndim == 1:\n","            t = t\n","        elif t.ndim == 2:\n","            t = t[-1]\n","        elif t.ndim == 3:\n","            t = t[-1, -1]\n","        t = t * stats[1, :] + stats[0, :] if stats.any() else t\n","\n","        ny, nx = m, n = img_dims()\n","\n","        if args.data_type == 'periodic' or args.data_type == 'transient':\n","            minmax = np.max(np.abs(t)) * 0.65\n","            imData = t\n","\n","            fig, ax = plt.subplots(figsize=(7.9,4.7))\n","            if cords.any(): Plots().cylinderFlow_imshow(imData, ax, cmocean.cm.balance, -minmax, minmax, coords=sensorCoords)\n","            else: Plots().cylinderFlow_imshow(imData, ax, cmocean.cm.balance, -minmax, minmax)\n","\n","        if args.data_type == 'turbulence':\n","\n","            t_ = t.reshape((ny, nx))\n","            plt.figure(facecolor=\"white\",  edgecolor='k')\n","            im = plt.imshow(t_)\n","      \n","        if args.data_type == 'sea_temp':\n","\n","            v_min = np.min(t); v_max = np.max(t); imData = t\n","\n","            fig, ax = plt.subplots()\n","            if cords.any(): Plots().seaTemp_imshow(imData, ax, 'terrain', v_min, v_max, coords=sensorCoords)\n","            else: Plots().seaTemp_imshow(imData, ax, 'terrain', v_min, v_max)\n","\n","        if args.data_type == 'burgers1D':\n","\n","            v_min = np.min(t); v_max = np.max(t); imData = t\n","\n","            fig, ax = plt.subplots()\n","            if cords.any(): Plots().burgers1D_imshow(imData, ax, 'terrain', v_min, v_max, coords=sensorCoords)\n","            else: Plots().burgers1D_imshow(imData, ax, 'terrain', v_min, v_max)\n","\n","        if save_:\n","            path = kwargs.get('path') if kwargs.get('path') else get_path(method, 'I', seed = kwargs.get('seed', 0)) \n","            fig.savefig(osp.join(path, '{}.pdf'.format(name)), format='pdf')\n","            print('SAVED ==', osp.join(path, '{}.pdf'.format(name)))\n","        \n","        fig.show() if plot_ else 0\n","        "],"execution_count":11,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"ZYPg9A5nHrhW"},"source":["## Add noise"]},{"cell_type":"code","metadata":{"id":"xPUL6ZhqGIN0","executionInfo":{"status":"ok","timestamp":1645620837439,"user_tz":-330,"elapsed":13,"user":{"displayName":"2K19/ME/275 YASH KUMAR","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"06719952441128256168"}}},"source":["\n","def awgn(s,SNRdB,L=1):\n","    \"\"\"\n","    AWGN channel\n","    Add AWGN noise to input signal. The function adds AWGN noise vector to signal\n","    's' to generate a resulting signal vector 'r' of specified SNR in dB. It also\n","    returns the noise vector 'n' that is added to the signal 's' and the power \n","    spectral density N0 of noise added\n","    Parameters:\n","        s : input/transmitted signal vector\n","        SNRdB : desired signal to noise ratio (expressed in dB) for the received signal\n","        L : oversampling factor (applicable for waveform simulation) default L = 1.\n","    Returns:\n","        r : received signal vector (r=s+n)\n","\"\"\"\n","    gamma = 10**(SNRdB/10) #SNR to linear scale\n","    if s.ndim==1:# if s is single dimensional vector\n","        P=L*sum(abs(s)**2)/len(s) #Actual power in the vector\n","    else: # multi-dimensional signals like MFSK\n","        P=L*sum(sum(abs(s)**2))/len(s) # if s is a matrix [MxN]\n","    N0=P/gamma # Find the noise spectral density\n","    if isrealobj(s):# check if input is real/complex object type\n","        n = sqrt(N0/2)*standard_normal(s.shape) # computed noise\n","    else:\n","        n = sqrt(N0/2)*(standard_normal(s.shape)+1j*standard_normal(s.shape))\n","    r = s + n # received signal\n","    return r\n","\n","# td_n2 = np.zeros((300,42168))\n","# # td_n2 = awgn(td,10)\n","# for i in range(len(td)):\n","#   td_n2[i,:] = awgn(td[i],1)"],"execution_count":12,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"mO-omVf397wc"},"source":["\n","# AUTO RECURRENT ESTIMATION (ARE)"]},{"cell_type":"markdown","metadata":{"id":"G6ei1SQuALhP"},"source":["## Autoencoder data"]},{"cell_type":"code","metadata":{"id":"yGgtqqwkAUTi","executionInfo":{"status":"ok","timestamp":1645620837440,"user_tz":-330,"elapsed":13,"user":{"displayName":"2K19/ME/275 YASH KUMAR","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"06719952441128256168"}}},"source":["class sensorgcdatasetAE(Dataset):\n","\n","    def __init__(self, in_file, stats, transform=None):\n","        self.sensor_frame = in_file\n","        self.stats = stats\n","        self.transform = transform\n","\n","    def __len__(self):\n","        return len(self.sensor_frame)\n","\n","    def __getitem__(self, idx):\n","        sensor = self.sensor_frame[idx, :]\n","        sensor = (sensor - self.stats[0, :]) / self.stats[1, :]\n","        sensor = torch.from_numpy(sensor).float()\n","        return sensor"],"execution_count":13,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"aHl37tiKeYau"},"source":["## Autoencoder Net"]},{"cell_type":"code","metadata":{"id":"Hsb3jPPkfnEq","executionInfo":{"status":"ok","timestamp":1645620838374,"user_tz":-330,"elapsed":946,"user":{"displayName":"2K19/ME/275 YASH KUMAR","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"06719952441128256168"}}},"source":["class autoencoder(nn.Module):\n","    def __init__(self, bottle_neck, **kwargs):\n","        super(autoencoder, self).__init__()\n","        self.bottle_neck = bottle_neck\n","        m, n = img_dims()\n","        k = m*n\n","        drop = kwargs.get('drop')\n","\n","        if args.data_type == 'transient':\n","\n","            if drop:\n","                print('autoencoder oprating with dropout:', drop)\n","                self.encoder = nn.Sequential(\n","                    # nn.Dropout(p =0.5),\n","                    nn.Linear(k, 2000),\n","                    nn.BatchNorm1d(num_features = 2000),\n","                    nn.ReLU(True),\n","                    # nn.Dropout(p =0.5),\n","                    nn.Linear(2000, 300),\n","                    nn.BatchNorm1d(num_features =300),\n","                    nn.ReLU(True),\n","                    nn.Dropout(p=drop),\n","                    nn.Linear(300, bottle_neck))\n","                self.decoder = nn.Sequential(\n","                    nn.Linear(bottle_neck, 300),\n","                    nn.ReLU(True),\n","                    nn.Linear(300, 2000),       \n","                    nn.ReLU(True),\n","                    nn.Linear(2000, k)) \n","\n","            else:\n","                print('autoencoder oprating without dropout layer')\n","                self.encoder = nn.Sequential(\n","                    nn.Linear(k, 2000),\n","                    nn.ReLU(True),\n","                    nn.Linear(2000, 300), \n","                    nn.ReLU(True),\n","                    nn.Linear(300, bottle_neck)) \n","                self.decoder = nn.Sequential(\n","                    nn.Linear(bottle_neck, 300),\n","                    nn.ReLU(True),\n","                    nn.Linear(300, 2000),\n","                    nn.ReLU(True),\n","                    nn.Linear(2000, k)) \n","\n","        elif args.data_type == 'periodic':\n","\n","            if drop:\n","                print('autoencoder oprating with dropout:', drop)\n","                self.encoder = nn.Sequential(\n","                    nn.Linear(k, 1024),\n","                    nn.BatchNorm1d(num_features = 1024),\n","                    nn.ReLU(True),\n","                    nn.Linear(1024, 256),\n","                    nn.BatchNorm1d(num_features =256), \n","                    nn.ReLU(True),\n","                    nn.Dropout(p=drop),\n","                    nn.Linear(256, bottle_neck)) \n","                self.decoder = nn.Sequential(\n","                    nn.Linear(bottle_neck, 256),\n","                    nn.ReLU(True),\n","                    nn.Linear(256, 1024),          \n","                    nn.ReLU(True),\n","                    nn.Linear(1024, k)) \n","            else:\n","                print('autoencoder oprating without dropout layer')\n","                self.encoder = nn.Sequential(\n","                    nn.Linear(k, 1024),\n","                    nn.ReLU(True),\n","                    nn.Linear(1024, 256),\n","                    nn.ReLU(True),\n","                    nn.Linear(256, bottle_neck)) \n","                self.decoder = nn.Sequential(\n","                    nn.Linear(bottle_neck, 256),\n","                    nn.ReLU(True),\n","                    nn.Linear(256, 1024),\n","                    nn.ReLU(True),\n","                    nn.Linear(1024, k)) \n","              \n","        elif args.data_type == 'sea_temp':\n","            k = 44219\n","\n","            if drop:\n","                print('autoencoder oprating with dropout:', drop)\n","                self.encoder = nn.Sequential(\n","                    nn.Linear(k, 512),\n","                    nn.BatchNorm1d(num_features=512),\n","                    nn.ReLU(True),\n","                    nn.Linear(512, 256),\n","                    nn.BatchNorm1d(num_features=256), \n","                    nn.ReLU(True),\n","                    nn.Dropout(p=drop),\n","                    nn.Linear(256, bottle_neck)) \n","                self.decoder = nn.Sequential(\n","                    nn.Linear(bottle_neck, 256),\n","                    nn.ReLU(True),\n","                    nn.Linear(256, 512),          \n","                    nn.ReLU(True),\n","                    nn.Linear(512, k)) \n","            else:\n","                print('autoencoder oprating without dropout layer')\n","                self.encoder = nn.Sequential(\n","                    nn.Linear(k, 512),\n","                    # nn.BatchNorm1d(num_features = 512),\n","                    nn.ReLU(True),\n","                    nn.Linear(512, 256),\n","                    # nn.BatchNorm1d(num_features =256), \n","                    nn.ReLU(True),\n","                    nn.Linear(256, bottle_neck)) \n","                self.decoder = nn.Sequential(\n","                    nn.Linear(bottle_neck, 256),\n","                    # nn.BatchNorm1d(num_features =256),\n","                    nn.ReLU(True),\n","                    nn.Linear(256, 512),\n","                    # nn.BatchNorm1d(num_features =512),\n","                    nn.ReLU(True),\n","                    nn.Linear(512, k)) \n","            \n","        if args.data_type == 'burgers1D':\n","\n","            if drop:\n","                print('autoencoder oprating with dropout:', drop)\n","                self.encoder = nn.Sequential(\n","                    nn.Linear(k, 128),\n","                    nn.BatchNorm1d(num_features = 128),\n","                    nn.ReLU(True),\n","                    nn.Linear(128, 128),\n","                    nn.BatchNorm1d(num_features =128),\n","                    nn.ReLU(True),\n","                    nn.Dropout(p=drop),\n","                    nn.Linear(128, bottle_neck))\n","                self.decoder = nn.Sequential(\n","                    nn.Linear(bottle_neck, 128),\n","                    nn.ReLU(True),\n","                    nn.Linear(128, 128),       \n","                    nn.ReLU(True),\n","                    nn.Linear(128, k)) \n","\n","            else:\n","                print('autoencoder oprating without dropout layer')\n","                self.encoder = nn.Sequential(\n","                    nn.Linear(k, 128),\n","                    nn.ReLU(True),\n","                    nn.Linear(128, 128),\n","                    nn.ReLU(True),\n","                    nn.Linear(128, bottle_neck)) \n","                self.decoder = nn.Sequential(\n","                    nn.Linear(bottle_neck, 128),\n","                    nn.ReLU(True),\n","                    nn.Linear(128, 128),\n","                    nn.ReLU(True),\n","                    nn.Linear(128, k)) \n","            \n","\n","    def forward(self, x):\n","      if x.shape[-1] == self.bottle_neck:\n","        w = self.decoder(x)\n","        return w\n","      else:\n","        y = self.encoder(x)\n","        z = self.decoder(y)\n","        return z,y\n","      # pdb.set_trace()\n","      # try :\n","      #   w = self.decoder(x)\n","      #   pdb.set_trace()\n","      #   return w\n","      # except :\n","      #   y = self.encoder(x)\n","      #   z = self.decoder(y)\n","      #   pdb.set_trace()\n","      #   return z,y\n","        "],"execution_count":14,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"pxFT2535ui8y"},"source":["## AUTOENCODER Train Loop"]},{"cell_type":"code","metadata":{"id":"XNOOTjJ7uwZI","executionInfo":{"status":"ok","timestamp":1645620838376,"user_tz":-330,"elapsed":8,"user":{"displayName":"2K19/ME/275 YASH KUMAR","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"06719952441128256168"}}},"source":["def AutoEcoder(bn, lr, num_epochs, bs, bsv, early_stop, **kwargs):\n","\n","    bottle_neck = bn\n","    \n","    trainAE, validAE, _, stats = process_data(drop=0, tr_samp=kwargs.get('tr_samp'))\n","    train = sensorgcdatasetAE(trainAE, stats)\n","    valid = sensorgcdatasetAE(validAE, stats)\n","\n","    train_data_gen = DataLoader(train, shuffle=True, batch_size=bs)\n","    valid_data_gen = DataLoader(valid, batch_size=bsv)\n","\n","    dataloaders = {'train': train_data_gen, 'valid':valid_data_gen}\n","    dataset_sizes = {'train': len(train_data_gen.dataset), 'valid': len(valid_data_gen.dataset)}\n","\n","    model = autoencoder(bottle_neck, drop=kwargs.get('drop',None)).to(device)\n","    criterion = nn.MSELoss().to(device)\n","    optimizer = torch.optim.Adam(model.parameters(), lr=lr,  weight_decay=1e-5)\n","\n","    prev_valid_loss = prev_train_loss = 100  # anylarge number\n","    stop=0; ii = 0; k=0\n","    rerror_train = []\n","    rerror_valid = []\n","    \n","    for epoch in range(num_epochs):\n","        for phase in ['train']:\n","            running_loss=0\n","            if phase == 'train': model.train()\n","            else:model.eval()\n","\n","            for data in enumerate(dataloaders[phase]):\n","                img = data[1]\n","                img = Variable(img).to(device)\n","                output, waste = model(img)\n","                loss = criterion(output, img)\n","                optimizer.zero_grad()\n","\n","                if phase=='train':\n","                    loss.backward()\n","                    optimizer.step()\n","\n","                running_loss += output.shape[0]*loss.data\n","\n","            if phase == 'train':\n","                train_epoch_loss = running_loss/dataset_sizes[phase]\n","                train_out_last = output \n","                train_in_last = img \n","            elif phase == 'valid':\n","                valid_epoch_loss = running_loss/dataset_sizes[phase]\n","        stop = stop+1\n","\n","        progress = f'   ({epoch}) Training loss: {train_epoch_loss:.8f}'\n","        if epoch%50 == 0: print(progress)\n","        \n","        if (train_epoch_loss < prev_train_loss):\n","\n","            model_wts = model.state_dict()\n","            prev_train_loss = train_epoch_loss\n","            stop = 0\n","            best_train_out, best_train_in = train_out_last, train_in_last\n","            best_valid_out, best_valid_in = output, img\n","\n","        if stop == early_stop:\n","            print('Early stopping criteria fulfilled')\n","            break\n","\n","        rerror_train.append(train_epoch_loss)\n","\n","    images = (best_train_out, best_train_in, best_train_out, best_train_in)\n","    show, save, path = (1, 1, 1, 1), (1, 1, 1, 1), get_path('are', 'W_AE') \n","    name = ('best_train_out', 'best_train_in', 'best_valid_out', 'best_valid_in')\n","    show_save_image('are', images, show, save, stats=stats, path=path, name=name)\n","\n","    path_to_weights = get_path('are', 'W_AE')\n","    weights_name = get_name('', 'W_AE', drop=kwargs.get('drop'), bottle_neck=bottle_neck)\n","    torch.save(model_wts, osp.join(path_to_weights, weights_name))\n","    print('==========weights saved==========')\n","\n","    Plots().plot_graph(rerror_train, rerror_train, savePath=osp.join(path_to_weights, weights_name))"],"execution_count":15,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Kz7i2VSsAgnL"},"source":["## ARE_train_data"]},{"cell_type":"code","metadata":{"id":"MUh4DNH_AnSS","executionInfo":{"status":"ok","timestamp":1645620838813,"user_tz":-330,"elapsed":443,"user":{"displayName":"2K19/ME/275 YASH KUMAR","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"06719952441128256168"}}},"source":["class sensorgcdatasetRNN(Dataset):\n","\n","    def __init__(self, in_file, out_file, transform=None):\n","        self.sensor_frame = in_file\n","        self.gc_frame = out_file\n","        self.transform = transform\n","\n","    def __len__(self):\n","        return len(self.sensor_frame)\n","\n","    def __getitem__(self, idx):\n","        sensor = self.sensor_frame[idx, :]\n","        gc = self.gc_frame[idx, :]\n","        sensor = torch.from_numpy(sensor).float()\n","        gc = torch.from_numpy(gc).float()\n","        return sensor, gc\n","\n","class sensorgcdatasetAA(Dataset):\n","    def __init__(self, in_file, out_file, stats, transform=None):\n","        self.sensor_frame = in_file\n","        self.gc_frame = out_file\n","        self.stats = stats\n","        self.transform = transform\n","\n","    def __len__(self):\n","        return len(self.sensor_frame)\n","\n","    def __getitem__(self, idx):\n","        sensor = self.sensor_frame[idx, :]\n","        gc = self.gc_frame[idx, :]\n","        gc = (gc - self.stats[0, :])/self.stats[1, :]\n","        sensor = torch.from_numpy(sensor).float()\n","        gc = torch.from_numpy(gc).float()\n","        return sensor, gc\n","\n","def ARE_train_data(sensor_num, bottle_neck, seq_len, **kwargs):\n","    seq_impct = int((seq_len-1)/2)\n","\n","    trainAE, validAE, testAE, stats = process_data(SNR=kwargs.get('SNR'), plot=kwargs.get('plot'), save=kwargs.get('save'),\n","                                                            tr_samp=kwargs.get('tr_samp'), plot_SNR=kwargs.get('plot_SNR'))\n","    \n","    len_train_and_valid = len(trainAE[:,0])+ len(validAE[:,0])\n","\n","    trainAE_stan, validAE_stan, testAE_stan = standardize(trainAE, validAE, testAE, stats)\n","    strain_in, svalid_in, stest_in = sensor_data(trainAE_stan, validAE_stan, testAE_stan, sensor_num, seed=kwargs.get('seed', 0))\n","    n_train, n_valid, n_test = trainAE_stan.shape[0], validAE_stan.shape[0], testAE_stan.shape[0]\n","    assert n_train == len(trainAE[:,0])\n","    \n","    # ----------------------------------------------------------------------------\n","    total = np.concatenate((trainAE, validAE, testAE), axis=0)\n","    train = sensorgcdatasetAE(total, stats)\n","\n","    path_to_weights = get_path('are', 'W_AE')\n","    weights_name = get_name('are', 'W_AE', drop=kwargs.get('drop'), bottle_neck=bottle_neck)\n","    pretrained_weightsAE = osp.join(path_to_weights, weights_name)\n","    # pretrained_weightsAE = '/content/drive/MyDrive/Colab Notebooks/are/weights_results/transient/mthd__snsr/are/autoencoder/weights_bn25'\n","\n","    train_data_gen = DataLoader(train, shuffle=False, batch_size=len(total[:,0]))\n","    model = autoencoder(bottle_neck, drop=kwargs.get('drop')).to(device)\n","    model.load_state_dict(torch.load(pretrained_weightsAE, map_location=torch.device(device)))\n","    model.eval()\n","\n","    for data in enumerate(train_data_gen):\n","        # img = data[1]\n","        # img = Variable(img).to(device)\n","        waste, gg = model(data[1].to(device))\n","        gtotal_out = gg.cpu().data.numpy()\n","\n","    gstats = np.array([np.mean(gtotal_out[0:n_train,:],axis = 0), np.std(gtotal_out[0:n_train,:], axis = 0)])\n","    del(trainAE,validAE,testAE)\n","    \n","    gtotal = (gtotal_out - gstats[0]) / gstats[1] \n","\n","    if args.operation_mode == 'online':\n","            gtrain_out = gtotal[seq_impct:n_train, :] \n","            gvalid_out = gtotal[n_train+seq_impct: len_train_and_valid,:]\n","            gtest = gtotal[len_train_and_valid+seq_impct : len(total[:,0]),:]\n","\n","    elif args.operation_mode == 'offline':\n","            gtrain_out = gtotal[seq_impct:n_train - seq_impct, :] \n","            gvalid_out = gtotal[n_train+seq_impct: len_train_and_valid-seq_impct,:]\n","            gtest = gtotal[len_train_and_valid+seq_impct : len(total[:,0])-seq_impct,:]\n","\n","    # ----------------------------------------------------------------------------\n","    del_TrainRows_ls = delStates_BW_datasets(n_train, seq_impct, args.n_TrainDatasets)\n","    gtrain_out = np.delete(gtrain_out, del_TrainRows_ls, 0)\n","\n","    del_TestRows_ls = delStates_BW_datasets(n_test, seq_impct, args.n_TestDatasets)\n","    gtest = np.delete(gtest, del_TestRows_ls, 0)\n","\n","    del(total,train,gtotal)\n","\n","    # ----------------------------------------------------------------------------\n","    if args.operation_mode == 'online':\n","        \n","        strain_in_stacked = np.zeros((n_train-seq_impct,(seq_impct+1)*sensor_num))\n","        svalid_in_stacked = np.zeros((n_valid-seq_impct,(seq_impct+1)*sensor_num))\n","        stest_in_stacked = np.zeros((n_test-seq_impct,(seq_impct+1)*sensor_num))\n","        \n","        for i in range(0,n_train-seq_impct):\n","            strain_in_stacked[i] = np.concatenate([(strain_in[i+sl,:]) for sl in range(seq_impct+1)], axis=None)\n","        for i in range(0,n_valid-seq_impct):\n","            svalid_in_stacked[i] = np.concatenate([(svalid_in[i+sl,:]) for sl in range(seq_impct+1)], axis=None)\n","        for i in range(0,n_test-seq_impct):\n","            stest_in_stacked[i] = np.concatenate([(stest_in[i+sl,:]) for sl in range(seq_impct+1)], axis=None)\n","        \n","        if args.data_type == 'transient' or args.data_type == 'burgers1D':\n","            strain_in_stacked = np.delete(strain_in_stacked, del_TrainRows_ls-seq_impct, 0)\n","            stest_in_stacked = np.delete(stest_in_stacked, del_TestRows_ls-seq_impct, 0)\n","\n","\n","    elif args.operation_mode == 'offline':\n","\n","        strain_in_stacked = np.zeros((n_train-2*seq_impct,seq_len*sensor_num))\n","        svalid_in_stacked = np.zeros((n_valid-2*seq_impct,seq_len*sensor_num))\n","        stest_in_stacked = np.zeros((n_test-2*seq_impct,seq_len*sensor_num))\n","\n","        for i in range(0,n_train-2*seq_impct) :\n","            strain_in_stacked[i] = np.concatenate([(strain_in[i+sl,:]) for sl in range(seq_len)], axis=None)\n","        for i in range(0,n_valid-2*seq_impct) :\n","            svalid_in_stacked[i] = np.concatenate([(svalid_in[i+sl,:]) for sl in range(seq_len)], axis=None)\n","        for i in range(0,n_test-2*seq_impct) :\n","            stest_in_stacked[i] = np.concatenate([(stest_in[i+sl,:]) for sl in range(seq_len)], axis=None)\n","        \n","        if args.data_type == 'transient' or args.data_type == 'burgers1D':\n","            strain_in_stacked = np.delete(strain_in_stacked, del_TrainRows_ls-seq_impct, 0) \n","            stest_in_stacked = np.delete(stest_in_stacked, del_TestRows_ls-seq_impct, 0)\n","    \n","    return gtrain_out, gvalid_out, strain_in_stacked, svalid_in_stacked, gtest, stest_in_stacked, gstats, stats\n","\n","\n","def delStates_BW_datasets(n_samp, seq_impct, n_datasets):\n","    \"\"\" Deleting rows in concatenated data at junction of datasets.\n","        e.g. 3 datasets for 'transient' Reynolds's number 180, 190 and 200 \n","        Because, data is not sequential at junction. Only in training data. \"\"\"\n","    del_rows_ls = []\n","\n","    if args.data_type == 'transient' or args.data_type == 'burgers1D':\n","\n","        for i in range(1, n_datasets):\n","            idx = n_samp * i/n_datasets\n","            start = int(idx)\n","            end = int(idx) + seq_impct\n","\n","            if args.operation_mode == 'offline': start = start - seq_impct            \n","            del_rows_ls = del_rows_ls + list(range(start, end))\n","        \n","    print(f'del_rows_ls: {del_rows_ls}')\n","    return np.array(del_rows_ls, dtype=np.int64)\n","\n"],"execution_count":16,"outputs":[]},{"cell_type":"code","metadata":{"id":"D-oNtwTtWnb7","executionInfo":{"status":"ok","timestamp":1645620838815,"user_tz":-330,"elapsed":16,"user":{"displayName":"2K19/ME/275 YASH KUMAR","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"06719952441128256168"}}},"source":["\"\"\"test delStates_BW_datasets \"\"\"\n","\n","if 0:\n","    n_train = 399*3\n","    seq_impct = 2\n","    n_datasets = 3\n","\n","    args.data_type = 'transient'\n","    delStates_BW_datasets(0, n_train, seq_impct, n_datasets)\n","    # del_rows_ls: [399 400 798 799]\n","\n","if 0:\n","    n_train = 51*20\n","    seq_impct = 2\n","    n_datasets = 20\n","\n","    args.data_type = 'burgers1D'\n","    delStates_BW_datasets(0, n_train, seq_impct, n_datasets)\n","    # del_rows_ls: [51, 52, 102, 103, 153,..., 919, 969, 970]"],"execution_count":17,"outputs":[]},{"cell_type":"code","metadata":{"id":"Pe3n_7wJV2Xm","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1645620838817,"user_tz":-330,"elapsed":16,"user":{"displayName":"2K19/ME/275 YASH KUMAR","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"06719952441128256168"}},"outputId":"223924d2-843c-4419-d6ff-bd048024e3cd"},"source":["n_train = 399*3\n","seq_impct = 2\n","del_row = np.array([])\n","for e in range(int(n_train/3), int(n_train/3)+seq_impct): # if seq_impct=2 (397. 398. 399. 400.)\n","  del_row = np.append(del_row, [e], axis=0)    #n_train/3 = 399\n","for e in range(int(n_train*2/3), int(n_train*2/3)+seq_impct):  #if seq_impct=2 (796. 797. 798. 799.)\n","  del_row = np.append(del_row, [e], axis=0)\n","del_row = del_row.astype(int)\n","print('del_row =',del_row)\n","del_row-seq_impct"],"execution_count":18,"outputs":[{"output_type":"stream","name":"stdout","text":["del_row = [399 400 798 799]\n"]},{"output_type":"execute_result","data":{"text/plain":["array([397, 398, 796, 797])"]},"metadata":{},"execution_count":18}]},{"cell_type":"markdown","metadata":{"id":"i01G721Agfi0"},"source":["## ARE LSTM Net "]},{"cell_type":"code","metadata":{"id":"8ub64Yw2hPA8","executionInfo":{"status":"ok","timestamp":1645620838819,"user_tz":-330,"elapsed":14,"user":{"displayName":"2K19/ME/275 YASH KUMAR","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"06719952441128256168"}}},"source":[" \n","class are_lstm_net(nn.Module):\n","\n","    def __init__(self, hidden_size, n_sensor, bottle_neck, layers, seq_len):\n","        super(are_lstm_net, self).__init__()\n","        self.hidden_size = hidden_size\n","        print('data_type:', args.data_type, '\\n', 'using lstm cell')\n","\n","        if args.data_type == 'transient': \n","\n","          self.rl = nn.LSTM(n_sensor , self.hidden_size, num_layers=layers)\n","          H = 200\n","\n","          if args.operation_mode == 'online':      \n","            self.l1 = nn.Linear(hidden_size, H)\n","            self.l2 = nn.Linear(H, H)\n","            self.l3 = nn.Linear(H, bottle_neck)\n","\n","          if args.operation_mode == 'offline':\n","            self.rlr = nn.LSTM(n_sensor, self.hidden_size, num_layers=layers)\n","            self.l1 = nn.Linear(hidden_size*2, H)\n","            self.l2 = nn.Linear(H, H)\n","            self.l3 = nn.Linear(H, bottle_neck)\n","\n","        if args.data_type == 'periodic': \n","\n","          self.rl = nn.LSTM(n_sensor, self.hidden_size, num_layers=layers)\n","          H = 50\n","          \n","          if args.operation_mode == 'online':      \n","            self.l1 = nn.Linear(hidden_size, H)            \n","            self.l2 = nn.Linear(H, H)\n","            self.l3 = nn.Linear(H, bottle_neck)\n","\n","          if args.operation_mode == 'offline':\n","            self.rlr = nn.LSTM(n_sensor, self.hidden_size, num_layers=layers)\n","            self.l1 = nn.Linear(hidden_size*2, H)\n","            self.l2 = nn.Linear(H, H)\n","            self.l3 = nn.Linear(H, bottle_neck)\n","\n","        if args.data_type == 'sea_temp': \n","\n","          self.rl = nn.LSTM(n_sensor , self.hidden_size, num_layers=layers)\n","          H = 100\n","\n","          if args.operation_mode == 'online':      \n","            self.l1 = nn.Linear(hidden_size, H)\n","            self.l2 = nn.Linear(H, H)\n","            self.l3 = nn.Linear(H, bottle_neck)\n","\n","          if args.operation_mode == 'offline':\n","            self.rlr = nn.LSTM(n_sensor, self.hidden_size, num_layers=layers)\n","            self.l1 = nn.Linear(hidden_size*2, H)\n","            self.l2 = nn.Linear(H, H)\n","            self.l3 = nn.Linear(H, bottle_neck)\n","\n","        if args.data_type == 'burgers1D': \n","\n","          self.rl = nn.LSTM(n_sensor ,self.hidden_size, num_layers=layers)\n","          H = 128\n","\n","          if args.operation_mode == 'online':      \n","            self.l1 = nn.Linear(hidden_size, H)\n","            self.l2 = nn.Linear(H, H)\n","            self.l3 = nn.Linear(H, bottle_neck)\n","\n","\n","    def reset_hidden_states(self, for_batch=None):\n","        if for_batch is not None:\n","            batch_size = for_batch.shape[1]\n","\n","        # Initialize recurrent hidden states\n","        if args.operation_mode == 'online':      \n","          self.rl_h = self.init_hidden(batch_size=batch_size)\n","          self.rl_c = self.init_hidden(batch_size=batch_size)\n","\n","        if args.operation_mode == 'offline':\n","          self.rl_h = self.init_hidden(batch_size=batch_size)\n","          self.rlr_h = self.init_hidden(batch_size=batch_size)\n","          self.rl_c = self.init_hidden(batch_size=batch_size)\n","          self.rlr_c = self.init_hidden(batch_size=batch_size)\n","\n","        if for_batch is not None:\n","            device = for_batch.device\n","\n","            if args.operation_mode == 'online':      \n","              self.rl_h = self.rl_h.to(device)\n","              self.rl_c = self.rl_c.to(device)\n","\n","            if args.operation_mode == 'offline':\n","                self.rl_h = self.rl_h.to(device)\n","                self.rlr_h = self.rlr_h.to(device)\n","                self.rl_c = self.rl_c.to(device)\n","                self.rlr_c = self.rlr_c.to(device)\n","\n","\n","    def init_hidden(self, batch_size):\n","        layers =1\n","        return Variable(torch.zeros(layers, batch_size, self.hidden_size))\n","\n","    def forward(self, leading_dna):\n","        self.reset_hidden_states(for_batch=leading_dna)\n","        \n","        batch = len(leading_dna[0,:,0])\n","        n_sensor = len(leading_dna[0,0,:])\n","        seq_len = len(leading_dna[:,0,0])\n","        seq_impct = int((seq_len-1)/2)\n","\n","        if args.operation_mode == 'online':      \n","          split_data1 = torch.zeros(seq_impct+1,batch,n_sensor)\n","          split_data1 = leading_dna[0:seq_impct+1,:,:] \n","\n","          rl_out, self.rl_h= self.rl(split_data1, (self.rl_h, self.rl_c))\n","          r_out = rl_out[seq_impct]\n","\n","        if args.operation_mode == 'offline':\n","          split_data1 = torch.zeros(seq_impct+1, batch, n_sensor)\n","          split_data2 = torch.zeros(seq_impct+1, batch, n_sensor)\n","          split_data1 = leading_dna[0:seq_impct+1,:,:] #(seq_impct+1,batch,n_sensor)\n","          split_data2 = leading_dna[seq_impct:seq_len,:,:]\n","\n","          rl_out, self.rl_h= self.rl(split_data1, (self.rl_h, self.rl_c))\n","          rlr_out, self.rlr_h = self.rlr(self.flip_input(split_data2), (self.rlr_h, self.rlr_c))\n","          r_out = torch.cat((rl_out[seq_impct], rlr_out[seq_impct]), 1)\n","\n","        l1_out = F.relu(self.l1(r_out))\n","        l2_out = F.relu(self.l2(l1_out))\n","        out = l3_out = self.l3(l2_out)\n","\n","        return out\n","\n","\n","    def flip_input(self, input):\n","        device = input.device\n","        flipped_array = np.flip(input.data.cpu().numpy(), 0).copy()\n","        return Variable(torch.from_numpy(flipped_array).to(device))\n"],"execution_count":19,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"0l797mzuBa8t"},"source":["## ARE TrainLoop"]},{"cell_type":"code","metadata":{"id":"vk5J8fv797wh","executionInfo":{"status":"ok","timestamp":1645620839386,"user_tz":-330,"elapsed":580,"user":{"displayName":"2K19/ME/275 YASH KUMAR","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"06719952441128256168"}}},"source":["def ARE_train_LSTM(n_sensor, seq_len, bottle_neck, hidden_size, num_epochs, lr, **kwargs):    \n","\n","  print('n_sensor =',n_sensor,'seq_len = ',seq_len)\n","  layers = 1\n","  seq_impct = int((seq_len-1)/2)\n","  seed = kwargs.get('seed', 0)\n","\n","  gtrain_out, gvalid_out, strain_in, svalid_in, _, _, gstats, stats  = ARE_train_data(n_sensor, \n","                                                                                    bottle_neck,\n","                                                                                    seq_len, \n","                                                                                    drop=kwargs.get('drop'),\n","                                                                                    seed=seed)\n","\n","  train = sensorgcdatasetRNN(strain_in, gtrain_out)\n","  valid = sensorgcdatasetRNN(svalid_in, gvalid_out)\n","  print(f'strain_in.shape :{strain_in.shape} \\ngtrain_out.shape :{gtrain_out.shape}')\n","\n","  bs = len(train)  # batch size  \n","  bsv = len(valid)\n","\n","  train_data_gen = DataLoader(train, shuffle=False, batch_size=bs)\n","  valid_data_gen = DataLoader(valid, shuffle=False, batch_size=bsv)\n","\n","  dataloaders = {'train': train_data_gen, 'valid':valid_data_gen}\n","  dataset_sizes = {'train': len(train_data_gen.dataset), 'valid': len(valid_data_gen.dataset)}\n","  batch ={'train':bs, 'valid':bsv}\n","\n","  model = are_lstm_net(hidden_size, n_sensor, bottle_neck, layers, seq_len).to(device)\n","\n","  criterion = nn.MSELoss().to(device)\n","  optimizer = torch.optim.Adam(model.parameters(), lr=lr,  weight_decay=1e-5)\n","\n","  prev_valid_loss = prev_train_loss = 100  # any large number\n","  stop = 0\n","  rerror_train = []\n","  rerror_valid = []\n","  for epoch in range(num_epochs):\n","\n","    for phase in ['train']:\n","        running_loss=0\n","        if phase == 'train': model.train()\n","        else: model.eval()\n","\n","        for i, (sensor, gc) in enumerate(dataloaders[phase]):\n","            # print(f'sensor_data: {sensor.shape} \\n {sensor.flatten()[:7]} \\n')\n","            # print(f'embed_data: {gc.shape} \\n {gc.flatten()[:7]} \\n')\n","\n","            if args.operation_mode == 'online': \n","                y = torch.zeros((seq_impct+1), batch[phase], n_sensor)\n","                for i in range(0, batch[phase]):\n","                    for j in range(0, (seq_impct+1)):\n","                        y[j, i,:] = sensor[i, n_sensor*j: n_sensor*j + n_sensor]\n","                    \n","            elif args.operation_mode == 'offline':\n","                y = torch.zeros(seq_len, batch[phase], n_sensor)\n","                for i in range(0, batch[phase]):\n","                    for j in range(0, seq_len):\n","                        y[j, i,:] = sensor[i, n_sensor*j: n_sensor*j + n_sensor]\n","                      \n","            sensor = Variable(y).to(device)\n","            gc = Variable(gc).to(device)\n","\n","            optimizer.zero_grad()\n","            gc_out = model(sensor)\n","            loss = criterion(gc_out, gc)\n","            if phase=='train':\n","                loss.backward()\n","                optimizer.step()\n","\n","            running_loss = gc_out.shape[0] * loss.data + running_loss\n","\n","        if phase == 'train': train_epoch_loss = running_loss/dataset_sizes[phase]\n","        elif phase == 'valid': valid_epoch_loss = running_loss/dataset_sizes[phase]\n","    stop = stop + 1\n","\n","    progress = f'   ({epoch}) Training loss: {train_epoch_loss:.8f}'\n","    if epoch%50 == 0: print(progress)\n","\n","    if (train_epoch_loss < prev_train_loss):\n","        model_wts = model.state_dict()\n","        prev_train_loss = train_epoch_loss\n","        stop = 0\n","        output_last = gc_out\n","        gc_last = gc\n","    \n","    if stop == 230:\n","        print('Early stopping criteria fulfilled')\n","        break\n","\n","    rerror_train.append(train_epoch_loss)\n","\n","  path_to_weights = get_path('are', 'W', seed=seed)\n","  weights_name = get_name('are', 'W', drop=kwargs.get('drop'), n_sensor=n_sensor, seq_len=seq_len, bottle_neck=bottle_neck)\n","  torch.save(model_wts, osp.join(path_to_weights, weights_name))\n","  print('=========saved weights=========')\n","\n","  Plots().plot_graph(rerror_train, rerror_train, savePath=osp.join(path_to_weights, weights_name))"],"execution_count":20,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"58B1Cz0wBkjo"},"source":["## ARE TestLoop"]},{"cell_type":"code","metadata":{"id":"VA9j2Dn_AQ1g","executionInfo":{"status":"ok","timestamp":1645620839387,"user_tz":-330,"elapsed":9,"user":{"displayName":"2K19/ME/275 YASH KUMAR","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"06719952441128256168"}}},"source":["def ARE_test(n_sensor, seq_len, bottle_neck, plot, save, hidden_size = 50, **kwargs):\n","    \"\"\"\n","    ARGS:\n","        plot (tuple): 0 or 1 ()\n","        save (tuple): 0 or 1 ()\n","\n","    VARS:\n","        states_pred (torch.tensor): [n_test_samp, n_pixel] \n","        stats (torch.tensor): [2, n_pixel]\n","\n","    RETURNS: average L2 error accros all test samples\n","    \"\"\"\n","  \n","    layers = 1\n","    seq_impct = int((seq_len-1)/2)\n","    SNR = kwargs.get('SNR', None)\n","    seed = kwargs.get('seed', 0)\n","\n","    if args.data_type == 'periodic': start, end = 0, 60\n","    if args.data_type == 'transient': start, end = 200, 399\n","    if args.data_type == 'turbulence': start, end = 0, 400\n","    if args.data_type == 'sea_temp': start, end = 0, 200 \n","    if args.data_type == 'burgers1D': start, end = 0, 51\n","\n","    # ------------------------------ Load data -----------------------------------\n","    _, _, _, _, embed_data, sensor_data, embed_stats, stats = ARE_train_data(n_sensor, bottle_neck, seq_len, \n","                                                                    drop=kwargs.get('drop',None), \n","                                                                    SNR=SNR, plot_SNR=kwargs.get('plot_SNR'),\n","                                                                    plot=plot[2], save=save[2],\n","                                                                    seed=seed)\n","    \"\"\" \n","    sensor_data (np.array): [n_test_samp, (seq_impct+1)*n_sensor]\n","    embed_stats (np.array): [n_test_samp, n_embed]\n","    \"\"\"\n","    datasetClass = sensorgcdatasetRNN(sensor_data, embed_data)\n","    testLoader = DataLoader(datasetClass, shuffle=False, batch_size=1)\n","\n","    # -------------------------- load model weights ------------------------------\n","    model = are_lstm_net(hidden_size, n_sensor, bottle_neck, layers, seq_len).to(device)\n","\n","    weightsDir = get_path('are', 'W', seed=seed)\n","    weightsName = get_name('are', 'W', drop=kwargs.get('drop'), n_sensor=n_sensor, \n","                            seq_len=seq_len, bottle_neck=bottle_neck)\n","    # weightsName = get_name('are', 'W', n_sensor=n_sensor, \n","    #                         seq_len=7, bottle_neck=bottle_neck)\n","    pretrained_weightsRNN = osp.join(weightsDir, weightsName)\n","    print(f'LSTM weights path: {pretrained_weightsRNN}')\n","\n","    model.load_state_dict(torch.load(pretrained_weightsRNN, map_location=torch.device(device)))\n","    model.eval()\n","\n","    # -------------------------- predict embeddings ------------------------------\n","    gc_pred = np.zeros([len(datasetClass), bottle_neck])\n","    for i, (sensor, gc) in enumerate(testLoader):\n","        \"\"\" sensor: [1, ]\"\"\"\n","        \"\"\" \n","        sensor (torch.tensor): [1, (seq_impct+1)*n_sensor]\n","        gc (torch.tensor): [1, n_embed]\n","        \"\"\"\n","\n","        if args.operation_mode == 'online': \n","                y = torch.zeros((seq_impct+1), 1, n_sensor)\n","                for j in range(0, (seq_impct+1)):\n","                    y[j, 0, :] = sensor[0, n_sensor*j: (n_sensor*j + n_sensor)]\n","                    \n","        elif args.operation_mode == 'offline':\n","                y = torch.zeros(seq_len, 1, n_sensor)\n","                for j in range(0, seq_len):\n","                    y[j, 0, :] = sensor[0, n_sensor*j: (n_sensor*j + n_sensor)]\n","                \n","        gc_pred[i, :] = model(y.to(device)).cpu().data.numpy()\n","        \n","    gc_pred = gc_pred * embed_stats[1, :] + embed_stats[0, :]\n","\n","    # ----------------------------- Load embeddings ------------------------------\n","    _, _, testAE, statsAE = process_data()\n","    n_test = testAE.shape[0]\n","    end = n_test\n","    del_TestRows_ls = delStates_BW_datasets(n_test, seq_impct, args.n_TestDatasets)\n","    target = np.delete(testAE, del_TestRows_ls, 0)\n","\n","    if args.operation_mode == 'online': \n","        AEdatasetClass = sensorgcdatasetAA(gc_pred, target[seq_impct:n_test, :], statsAE)\n","    elif args.operation_mode == 'offline':\n","        AEdatasetClass = sensorgcdatasetAA(gc_pred, target[seq_impct:n_test-seq_impct, :], statsAE)\n","    \n","    AEtestLoader = DataLoader(AEdatasetClass, shuffle=False, batch_size=1)\n","    _, cords = sensor_cord_data(n_sensor, seed=kwargs.get('seed', 0))\n","\n","    # ------------------------- Load autoencoder weights -------------------------\n","    path_to_weights = get_path('are', 'W_AE')\n","    weights_name = get_name('are', 'W_AE', drop=kwargs.get('drop'), bottle_neck=bottle_neck)\n","    pretrained_weightsAE = osp.join(path_to_weights, weights_name)\n","    print(f'Autoencoder weights path: {pretrained_weightsAE}')\n","\n","    model = autoencoder(bottle_neck, drop=kwargs.get('drop')).to(device)\n","\n","    model.load_state_dict(torch.load(pretrained_weightsAE, map_location=torch.device(device)))\n","    model.eval()\n","\n","    # ---------------------------- predict states --------------------------------\n","    t_pred = np.zeros([len(AEtestLoader), len(testAE[0,:]) ])\n","    t_true = np.zeros([len(AEtestLoader), len(testAE[0,:]) ])\n","    for i, (gc_pre, t) in enumerate(AEtestLoader):\n","        t_out_tensor = model(gc_pre.to(device))\n","        t_out = t_out_tensor.cpu().data.numpy()\n","        t_pred[i, :] = t_out\n","        t_true[i, :] = t\n","\n","    if kwargs.get('use_stats', 1):\n","            # t_pred = t_pred * statsAE[1, :] + statsAE[0, :]\n","            t_pred = t_pred * stats[1, :] + stats[0, :]\n","            t_true = t_true * statsAE[1, :] + statsAE[0, :]\n","\n","    # -------------------------- Calc average error ------------------------------\n","    Ferr = []\n","    if args.operation_mode == 'online': end = end-seq_impct*args.n_TestDatasets\n","    elif args.operation_mode == 'offline': end = end-2*seq_impct*args.n_TestDatasets\n","\n","    # pdb.set_trace()\n","    for i in range(start,end):\n","        Ferr.append(np.linalg.norm(t_true[i,:] - t_pred[i,:]) / np.linalg.norm(t_true[i,:]))\n","    \n","    errorAvg = np.mean(Ferr)\n","    print(f'Error: {errorAvg}')\n","\n","  \n","    # ------------------------- Save pred for plotting ---------------------------\n","    state_pred = np.zeros([len(testAE), len(testAE[0,:]) ])\n","    statesPerDataset = len(testAE)//args.n_TestDatasets\n","    statesPredPerDataset = len(AEtestLoader)//args.n_TestDatasets\n","    # pdb.set_trace()\n","    for i in range(args.n_TestDatasets):\n","        state_pred[i*statesPerDataset+seq_impct: (i+1)*statesPerDataset] = \\\n","        t_pred[i*statesPredPerDataset: (i+1)*statesPredPerDataset]\n","    \n","    predData_Path = osp.join(weightsDir, f'predData_s{n_sensor}_seqlen{seq_len}_SNR{SNR}.hdf5')\n","    with h5py.File(predData_Path, 'w') as f:\n","        f.create_dataset('pred', data=state_pred)\n","        f.create_dataset('true', data=t_true)\n","        f.create_dataset('coords', data=cords)\n","        f.create_dataset('method', data=np.array(list('ARE'.encode('utf8'))))\n","\n","    print(f'pred data saved at {predData_Path}')\n","\n","    return float(errorAvg)"],"execution_count":21,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"mZjvskHx-lYk"},"source":["# POD DEEP STATE (PDS)"]},{"cell_type":"markdown","metadata":{"id":"TVs3XVBXA22T"},"source":["## PDS_train_data"]},{"cell_type":"code","metadata":{"id":"pksOAQGhBxPG","executionInfo":{"status":"ok","timestamp":1645620839388,"user_tz":-330,"elapsed":9,"user":{"displayName":"2K19/ME/275 YASH KUMAR","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"06719952441128256168"}}},"source":["class sensorgcdatasetDS(Dataset):\n","\n","    def __init__(self, in_file, out_file, transform=None):\n","        self.in_frame = in_file\n","        self.out_frame = out_file\n","        self.transform = transform\n","\n","    def __len__(self):\n","        return len(self.in_frame)\n","\n","    def __getitem__(self, idx):\n","        inn = self.in_frame[idx, :]\n","        out = self.out_frame[idx, :]\n","        inn = torch.from_numpy(inn).float()\n","        out = torch.from_numpy(out).float()\n","        return inn, out\n","\n","def PDS_train_data(sensor_num, bottle_neck, **kwargs):\n","  \n","  trainAE, validAE, testAE, stats = process_data(SNR=kwargs.get('SNR'), plot=kwargs.get('plot'), save=kwargs.get('save'),\n","                                                 tr_samp=kwargs.get('tr_samp'), plot_SNR=kwargs.get('plot_SNR'),\n","                                                 seed=kwargs.get('seed', 0))\n","  mean, std = stats[0], stats[1]\n","\n","  # ----------------------------------------------------------------------------\n","  X = (trainAE - mean).transpose()\n","  phi, s, V = np.linalg.svd(X, full_matrices=False)\n","  del(s,V)\n","  phi = phi[:,0:bottle_neck]\n","\n","  gtrain_out = (np.dot(phi.transpose(), X)).transpose()\n","  del(X)\n","\n","  XX = (validAE - mean).transpose()\n","  gvalid_out = (np.dot(phi.transpose(), XX)).transpose()\n","\n","  XXX = (testAE - mean).transpose()\n","  gtest_out = (np.dot(phi.transpose(), XXX)).transpose()\n","  del(XX, XXX)\n","  # ----------------------------------------------------------------------------\n","\n","  trainAE_stan, validAE_stan, testAE_stan = standardize(trainAE, validAE, testAE, stats)\n","  del(trainAE, validAE, testAE)\n","\n","  strain_in, svalid_in, stest_in = sensor_data(trainAE_stan, validAE_stan, testAE_stan, sensor_num)\n","\n","  gstats = np.array([np.mean(gtrain_out, axis = 0), np.std(gtrain_out, axis = 0)])\n","  gtrain_out, gvalid_out, gtest_out = standardize(gtrain_out, gvalid_out, gtest_out, gstats)\n","\n","  del(trainAE_stan,validAE_stan,testAE_stan)\n","\n","  return gtrain_out, gvalid_out, strain_in, svalid_in, gtest_out, stest_in, gstats, phi, stats   "],"execution_count":22,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"2rx07OS-faK9"},"source":["## PDS net"]},{"cell_type":"code","metadata":{"id":"vzvoU2sZf3aL","executionInfo":{"status":"ok","timestamp":1645620839389,"user_tz":-330,"elapsed":9,"user":{"displayName":"2K19/ME/275 YASH KUMAR","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"06719952441128256168"}}},"source":["class PDSnetwork(nn.Module):\n","\n","    def __init__(self, n_sensor, bottle_neck):\n","        super(PDSnetwork, self).__init__()\n","\n","        if args.data_type == 'transient':\n","            H = 200\n","        if args.data_type == 'periodic':\n","            H = 50\n","        if args.data_type == 'sea_temp':\n","            H = 100\n","        if args.data_type == 'burgers1D': \n","            H = 128\n","\n","        self.l1 = nn.Linear(n_sensor, H)\n","        self.l2 = nn.Linear(H, H)\n","        self.output = nn.Linear(H, bottle_neck)\n","\n","    def forward(self, leading_dna):\n","        l1_out = F.relu(self.l1(leading_dna))\n","        l2_out = F.relu(self.l2(l1_out))\n","        output = self.output(l2_out)        \n","        return output"],"execution_count":23,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"9kJS4vOK99ja"},"source":["## PDS TrainLoop"]},{"cell_type":"code","metadata":{"id":"PiShL93hrBi7","executionInfo":{"status":"ok","timestamp":1645620840029,"user_tz":-330,"elapsed":649,"user":{"displayName":"2K19/ME/275 YASH KUMAR","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"06719952441128256168"}}},"source":["def PDS_train(n_sensor, bottle_neck, num_epochs, lr, **kwargs):\n","  \n","  gtrain_out, gvalid_out, strain_in, svalid_in, gtest_stan, stest_DS, gstats, phi, stats = PDS_train_data(n_sensor, bottle_neck,\n","                                                                                                          seed=kwargs.get('seed', 0))\n","\n","\n","  del(gtest_stan, stest_DS,phi, stats)\n","\n","  train = sensorgcdatasetDS(strain_in, gtrain_out)\n","  valid = sensorgcdatasetDS(svalid_in, gvalid_out)\n","\n","  bs = len(train)  # batch size\n","  bsv = len(valid)\n","  train_data_gen = DataLoader(train, shuffle=True, batch_size=bs)\n","  valid_data_gen = DataLoader(valid, shuffle=True, batch_size=bsv)\n","\n","  dataloaders = {'train': train_data_gen, 'valid':valid_data_gen}\n","  dataset_sizes = {'train': len(train_data_gen.dataset), 'valid': len(valid_data_gen.dataset)}\n","  batch ={'train':bs, 'valid':bsv}\n","\n","  model = PDSnetwork(n_sensor, bottle_neck).to(device)\n","\n","  # Loss and Optimizer\n","  criterion = nn.MSELoss()\n","  optimizer = torch.optim.Adam(model.parameters(), lr=lr, weight_decay=1e-5)\n","  #exp_lr_scheduler = torch.optim.lr_scheduler.MultiStepLR(optimizer, milestones=[500], gamma=0.1)\n","\n","  prev_valid_loss = prev_train_loss = 100  # any large number\n","  stop = 0\n","  rerror_train = []\n","  rerror_valid = []\n","  for epoch in range(num_epochs):\n","    for phase in ['train','valid']:\n","      running_loss=0\n","      if phase == 'train':\n","        model.train()\n","      else:\n","        model.eval()\n","\n","      for i, (sensor, gc) in enumerate(dataloaders[phase]):\n","          sensor = Variable(sensor).to(device)\n","          gc = Variable(gc).to(device)\n","          optimizer.zero_grad()\n","          gc_out = model(sensor)\n","\n","          # calculate loss\n","          loss = criterion(gc_out, gc)\n","\n","          if phase=='train':\n","            loss.backward()\n","            optimizer.step()\n","            #exp_lr_scheduler.step()\n","\n","          running_loss = gc_out.shape[0] * loss.data + running_loss\n","\n","      if phase == 'train':\n","        train_epoch_loss = running_loss/dataset_sizes[phase]\n","      elif phase == 'valid':\n","        valid_epoch_loss = running_loss/dataset_sizes[phase]\n","    stop = stop + 1\n","\n","    progress = f'   ({epoch}) Training loss: {train_epoch_loss:.8f}'\n","    if epoch%50 == 0: print(progress)\n","    \n","    if (train_epoch_loss < prev_train_loss):\n","      model_wts = model.state_dict()\n","      prev_valid_loss = valid_epoch_loss\n","      prev_train_loss = train_epoch_loss\n","      stop = 0\n","\n","    if stop == 230:\n","        print('Early stopping criteria fulfilled')\n","        break\n","    rerror_train.append(train_epoch_loss)\n","    rerror_valid.append(valid_epoch_loss)\n","\n","  path_to_weights = get_path('pds', 'W')\n","  weights_name = get_name('pds', 'W', drop=kwargs.get('drop'), n_sensor=n_sensor, bottle_neck=bottle_neck)\n","  torch.save(model_wts, osp.join(path_to_weights, weights_name))\n","  print('=========saved weights=========')\n","\n","  Plots().plot_graph(rerror_train, rerror_valid, savePath=osp.join(path_to_weights, weights_name))"],"execution_count":24,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"UCZoIA0e-D4z"},"source":["## PDS TestLoop"]},{"cell_type":"code","metadata":{"id":"XDhfdO0b-lY3","executionInfo":{"status":"ok","timestamp":1645620840032,"user_tz":-330,"elapsed":26,"user":{"displayName":"2K19/ME/275 YASH KUMAR","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"06719952441128256168"}}},"source":["\n","def PDS_test(n_sensor, bottle_neck, plot, save, **kwargs): \n","    SNR=kwargs.get('SNR',None)\n","\n","    path_to_weights = get_path('pds', 'W')\n","    weights_name = get_name('pds', 'W', n_sensor=n_sensor, bottle_neck=bottle_neck)\n","    pretrained_weightsRNN = osp.join(path_to_weights, weights_name)\n","    \n","    gtrain_out, gvalid_out, strain_in, svalid_in, gtest, stest_in, gstats, phi, stats = PDS_train_data(n_sensor,bottle_neck,\n","                                                                                                        SNR=SNR,\n","                                                                                                        plot_SNR=kwargs.get('plot_SNR'),\n","                                                                                                        plot=plot[2], save=save[2],\n","                                                                                                        seed=kwargs.get('seed', 0))\n","\n","    del(gtrain_out, gvalid_out, strain_in, svalid_in)\n","\n","    modell = PDSnetwork(n_sensor, bottle_neck).to(device)\n","    modell.load_state_dict(torch.load(pretrained_weightsRNN, map_location=torch.device(device)))\n","    modell.eval()\n","\n","    test = sensorgcdatasetDS(stest_in, gtest)\n","\n","    gc_pred = np.zeros([len(test), bottle_neck])\n","    gc_pod = np.zeros([len(test), bottle_neck])\n","    for i, (sensor, gc) in enumerate(test):\n","        y = modell(sensor.to(device))\n","        gc_pred[i, :] = y.cpu().data.numpy()\n","        gc_pod[i, :] = gc.cpu().data.numpy()\n","\n","    gc_pred = gc_pred * gstats[1, :] + gstats[0, :]\n","    gc_pod = gc_pod * gstats[1, :] + gstats[0, :]\n","\n","    t_pred = gc_pred.dot(phi.T) \n","    t_pod = gc_pod.dot(phi.T) \n","    \n","    _, _, testAE, statsAE = process_data()\n","    n_test = testAE.shape[0]\n","    end = n_test\n","    t_true = testAE \n","\n","    if kwargs.get('use_stats', 1):\n","        # t_pred = t_pred + statsAE[0]\n","        t_pred = t_pred + stats[0]\n","    else:\n","        t_true = t_true - statsAE[0]\n","\n","    _, cords = sensor_cord_data(n_sensor, seed=kwargs.get('seed', 0))\n","\n","    # ----------------------------------------------------------------------------\n","    \n","    Ferr = []\n","    if args.data_type == 'periodic': start, end = 0, 60\n","    if args.data_type == 'transient': start, end = 200, 399\n","    if args.data_type == 'turbulence': start, end = 0, 400\n","    if args.data_type == 'sea_temp': start, end = 0, 200 \n","    if args.data_type == 'burgers1D': start, end = 0, n_test\n","\n","    # if args.operation_mode == 'online': end = end-seq_impct\n","    # elif args.operation_mode == 'offline': end = end-2*seq_impct\n","\n","    for i in range(start,end):\n","        Ferr.append(np.linalg.norm(t_true[i,:] - t_pred[i,:]) / np.linalg.norm(t_true[i,:]))\n","\n","    Ferr_avg = np.mean(Ferr)\n","    print(\"Error: \", Ferr_avg, '------------------------------------------------')\n","\n","    predData_Path = osp.join(path_to_weights, f'predData_s{n_sensor}_SNR{SNR}.hdf5')\n","    with h5py.File(predData_Path, 'w') as f:\n","        f.create_dataset('pred', data=t_pred)\n","        f.create_dataset('true', data=t_true)\n","        f.create_dataset('coords', data=cords)\n","        f.create_dataset('method', data=np.array(list('PDS'.encode('utf8'))))\n","    \n","    return float(Ferr_avg)"],"execution_count":25,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"-5nYRiX1f-ya"},"source":["# SHALLOW DECODER (SD)"]},{"cell_type":"markdown","metadata":{"id":"gGf32xK4qIXQ"},"source":["## SD_train_data"]},{"cell_type":"code","metadata":{"id":"fO6gSkfPqOn0","executionInfo":{"status":"ok","timestamp":1645620840035,"user_tz":-330,"elapsed":26,"user":{"displayName":"2K19/ME/275 YASH KUMAR","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"06719952441128256168"}}},"source":["def SD_train_data(sensor_num, **kwargs):\n","\n","  trainAE, validAE, testAE, stats = process_data(SNR=kwargs.get('SNR'), plot=kwargs.get('plot'), save=kwargs.get('save'),\n","                                                tr_samp=kwargs.get('tr_samp'), plot_SNR=kwargs.get('plot_SNR'),\n","                                                seed=kwargs.get('seed', 0))\n","  stats[1] = np.ones_like(stats[1])\n","\n","  trainAE_stan, validAE_stan, testAE_stan = standardize(trainAE, validAE, testAE, stats)\n","\n","  strain_in, svalid_in, stest_in = sensor_data(trainAE_stan, validAE_stan, testAE_stan, sensor_num)\n","\n","  return trainAE_stan, validAE_stan, strain_in, svalid_in, testAE_stan, stest_in, stats"],"execution_count":26,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"3JRIlWyFjSXl"},"source":["## Shallow Decoder Net"]},{"cell_type":"code","metadata":{"id":"CCmAB_Xws9Mz","executionInfo":{"status":"ok","timestamp":1645620840037,"user_tz":-330,"elapsed":27,"user":{"displayName":"2K19/ME/275 YASH KUMAR","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"06719952441128256168"}}},"source":["class shallow_decoder_net(nn.Module):\n","    def __init__(self, n_sensors, **kwargs):\n","        super(shallow_decoder_net, self).__init__()\n","        \n","        m, n = img_dims()\n","        self.n_sensors = n_sensors\n","        self.outputlayer_size = m*n\n","        self.drop = kwargs.get('drop')\n","\n","        if self.drop: print('shallow_decoder oprating with drop')\n","        else: print('shallow_decoder oprating without drop')\n","\n","        if args.data_type == 'periodic':\n","            N1, N2 = 35, 40\n","        if args.data_type == 'transient':\n","            N1, N2 = 350, 400\n","        if args.data_type == 'turbulence':\n","            N1, N2 = 350, 400\n","        if args.data_type == 'sea_temp':\n","            N1, N2 = 350, 400\n","            self.outputlayer_size = 44219\n","        if args.data_type == 'burgers1D': \n","            N1, N2 = 128, 256\n","        \n","        if args.exp == 'SDnets':\n","            SDnet = kwargs.get('SDnet', '')\n","            N1, N2 = SDnet[0], SDnet[1] \n","            print(f'N1 {N1}, N2 {N2}')\n","        \n","        self.learn_features = nn.Sequential(         \n","            nn.Linear(n_sensors, N1),\n","            nn.ReLU(True), \n","            nn.BatchNorm1d(1),  \n","            )        \n","        \n","        self.learn_coef = nn.Sequential(            \n","            nn.Linear(N1, N2),\n","            nn.ReLU(True),  \n","            nn.BatchNorm1d(1),  \n","            )\n","\n","        self.learn_dictionary = nn.Sequential(\n","            nn.Linear(N2, self.outputlayer_size),\n","            )\n","        \n","\n","        for m in self.modules():\n","            # torch.manual_seed(args.seed)\n","            if isinstance(m, nn.Conv2d):\n","                nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')\n","            \n","            elif isinstance(m, nn.BatchNorm2d):\n","                nn.init.constant_(m.weight, 1)\n","                nn.init.constant_(m.bias, 0)        \n","\n","            elif isinstance(m, nn.Linear):\n","                nn.init.xavier_normal(m.weight)\n","                if m.bias is not None:\n","                    nn.init.constant(m.bias, 0.0)    \n","\n","\n","    def forward(self, x):\n","        \"\"\"\n","        ARGS:\n","            x (torch.tensor): [n_samp, 1, n_sensors]\n","        \"\"\"\n","        # print(f'x: {x.shape}')\n","        x = self.learn_features(x)\n","        if self.drop:\n","            # torch.manual_seed(args.seed)\n","            x = nn.functional.dropout(x, p=0.1, training=self.training)   \n","        x = self.learn_coef(x)\n","        x = self.learn_dictionary(x) \n","        return x"],"execution_count":27,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"It5emxOFNrBB"},"source":["## SD Train Loop"]},{"cell_type":"code","metadata":{"id":"sKVNtfLAN5re","executionInfo":{"status":"ok","timestamp":1645620840040,"user_tz":-330,"elapsed":28,"user":{"displayName":"2K19/ME/275 YASH KUMAR","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"06719952441128256168"}}},"source":["def SD_train(n_sensor, num_epochs, lr, wd, learning_rate_change=0.9, weight_decay_change=0.8, epoch_update=100, **kwargs):    \n","\n","  print('n_sensor =',n_sensor)\n","  train_out, valid_out, strain_in, svalid_in, _, _, stats = SD_train_data(n_sensor, seed=kwargs.get('seed', 0))\n","\n","  strain_in, svalid_in  = torch.tensor(np.expand_dims(strain_in, axis=1)).float().to(device), torch.tensor(np.expand_dims(svalid_in, axis=1)).float().to(device)\n","  train_out, valid_out = torch.tensor(np.expand_dims(train_out, axis=1)).float().to(device), torch.tensor(np.expand_dims(valid_out, axis=1)).float().to(device)\n","\n","  train_data = torch.utils.data.TensorDataset(strain_in, train_out)\n","  valid_data = torch.utils.data.TensorDataset(svalid_in, valid_out)\n","\n","  bs = round(len(strain_in)/3)  # batch size\n","  bsv = len(svalid_in)\n","\n","  train_loader = DataLoader(dataset = train_data, batch_size = bs, shuffle = True)\n","  valid_loader = DataLoader(dataset = valid_data, batch_size = bsv, shuffle = True)\n","\n","  dataloaders = {'train': train_loader, 'valid':valid_loader}\n","  dataset_sizes = {'train': len(train_loader.dataset), 'valid': len(valid_loader.dataset)}\n","  batch ={'train':bs, 'valid':bsv}\n","\n","  model = shallow_decoder_net(n_sensor, drop=kwargs.get('drop',None), SDnet=kwargs.get('SDnet')).to(device)\n","\n","  criterion = nn.MSELoss().to(device)\n","  optimizer = torch.optim.Adam(model.parameters(), lr=lr, weight_decay=wd)\n","\n","  def exp_lr_scheduler(optimizer, epoch, lr_decay_rate=0.8, weight_decay_rate=0.8, lr_decay_epoch=100):\n","        \"\"\"Decay learning rate by a factor of lr_decay_rate every lr_decay_epoch epochs\"\"\"\n","        if epoch % lr_decay_epoch:\n","            return \n","        \n","        # if args.optimizer == 'sgd':\n","        for param_group in optimizer.param_groups:\n","            param_group['lr'] *= lr_decay_rate\n","            param_group['weight_decay'] *= weight_decay_rate\n","        return \n","\n","  prev_valid_loss = prev_train_loss = 100  # any large number\n","  stop = 0\n","  rerror_train = []\n","  rerror_valid = []\n","\n","  for epochs in range(num_epochs):\n","    for phase in ['train','valid']:\n","      running_loss=0\n","      if phase == 'train':\n","        model.train()\n","      else:\n","        model.eval()\n","\n","      for i, (sensor, true) in enumerate(dataloaders[phase]):\n","\n","          optimizer.zero_grad()\n","          pred = model(sensor).to(device)\n","          loss = criterion(pred, true)\n","\n","          if phase=='train':\n","            loss.backward()\n","            optimizer.step()\n","            # ===================adjusted lr========================\n","            exp_lr_scheduler(optimizer, epochs, lr_decay_rate = learning_rate_change, \n","                             weight_decay_rate = weight_decay_change, \n","                             lr_decay_epoch = epoch_update)\n","\n","          running_loss = pred.shape[0] * loss.data + running_loss\n","\n","      if phase == 'train':\n","        train_epoch_loss = running_loss/dataset_sizes[phase]\n","        train_out_last = pred \n","        train_in_last = true \n","      elif phase == 'valid':\n","        valid_epoch_loss = running_loss/dataset_sizes[phase]\n","    stop = stop + 1\n","\n","    progress = f'   ({epochs}) Training loss: {train_epoch_loss:.8f}'\n","    if epochs%50 == 0: print(progress)\n","    \n","    # if (valid_epoch_loss < prev_valid_loss):\n","    if (train_epoch_loss < prev_train_loss):\n","      model_wts = model.state_dict()\n","      prev_valid_loss = valid_epoch_loss\n","      prev_train_loss = train_epoch_loss\n","\n","      stop = 0\n","      best_train_out, best_train_in = train_out_last, train_in_last\n","      best_valid_out, best_valid_in = pred, true\n","\n","    \n","    if stop == 230:\n","        print('Early stopping criteria fulfilled')\n","        break\n","\n","    rerror_train.append(train_epoch_loss)\n","    rerror_valid.append(valid_epoch_loss)\n","\n","  print('({}) BEST_Training Loss: {:.8f} BEST_Valid Loss: {:.8f} '.format(epochs, prev_train_loss, prev_valid_loss))\n","\n","  path_to_weights = get_path('sd', 'W', **kwargs)\n","  weights_name = get_name('sd', 'W', drop=kwargs.get('drop'), n_sensor=n_sensor)\n","  torch.save(model_wts, osp.join(path_to_weights, weights_name))\n","  print('=========saved weights=========')\n","\n","  Plots().plot_graph(rerror_train,rerror_valid, osp.join(path_to_weights, weights_name))\n","  print('================================ over ================================')"],"execution_count":28,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"MWq73EDK8JMj"},"source":["## SD TestLoop"]},{"cell_type":"code","metadata":{"id":"YTFX_FQf8auV","executionInfo":{"status":"ok","timestamp":1645620840691,"user_tz":-330,"elapsed":678,"user":{"displayName":"2K19/ME/275 YASH KUMAR","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"06719952441128256168"}}},"source":["def SD_test(n_sensor, plot, save, **kwargs):\n","\n","    SNR=kwargs.get('SNR',None)\n","\n","    path_to_weights = get_path('sd', 'W', **kwargs)\n","    weights_name = get_name('sd', 'W', drop=kwargs.get('drop'), n_sensor=n_sensor)\n","    pretrained_weights = osp.join(path_to_weights, weights_name)\n","\n","    train_out, valid_out, strain_in, svalid_in, test_out, stest_in, stats = SD_train_data(n_sensor, SNR=SNR,\n","                                                                                            plot_SNR=kwargs.get('plot_SNR'),\n","                                                                                            plot=plot[2], save=save[2],\n","                                                                                            seed=kwargs.get('seed', 0))\n","    del(train_out, valid_out, strain_in, svalid_in)\n","    test_out, stest_in  = torch.tensor(np.expand_dims(test_out, axis=1)).float().to(device), torch.tensor(np.expand_dims(stest_in, axis=1)).float().to(device)\n","    \n","    model = shallow_decoder_net(n_sensor, drop=kwargs.get('drop'), SDnet=kwargs.get('SDnet')).to(device)\n","    print(model)\n","\n","    # Load saved neural network weights\n","    model.load_state_dict(torch.load(pretrained_weights, map_location=torch.device(device)))\n","    model.eval()\n","\n","    test = torch.utils.data.TensorDataset(stest_in, test_out)\n","    test_data_genn = DataLoader(test, shuffle=False, batch_size=1)\n","\n","    _, cords = sensor_cord_data(n_sensor, seed=kwargs.get('seed', 0))      \n","\n","    states_pred = np.zeros([len(test), len(test_out[0, 0, :]) ])\n","    states_true = np.zeros([len(test), len(test_out[0, 0, :]) ])\n","\n","    for i, (sensor, state) in enumerate(test):\n","        pred = (model(sensor[:, None].to(device)))[:, 0]\n","        states_pred[i, :] = pred.cpu().data.numpy()\n","        states_true[i, :] = state.cpu().data.numpy()\n","\n","    assert (states_true == test_out[:, 0].cpu().data.numpy()).any()\n","\n","    trainAE, validAE, testAE, statsAE = process_data()\n","    del(trainAE,validAE)\n","    n_test = testAE.shape[0]\n","    statsAE[1] = np.ones_like(statsAE[1])\n","\n","    states_true = testAE\n","\n","    if kwargs.get('use_stats', 1):\n","        # states_pred = states_pred + statsAE[0, :]\n","        states_pred = states_pred + stats[0, :]\n","    else:\n","        states_true = (testAE - statsAE[0, :])  # / statsAE[1, :]\n","\n","    Ferr = []\n","    if args.data_type == 'periodic': start, end = 0, 60\n","    if args.data_type == 'transient': start, end = 200, 399\n","    if args.data_type == 'turbulence': start, end = 0, 400\n","    if args.data_type == 'sea_temp': start, end = 0, 200 \n","    if args.data_type == 'burgers1D': start, end = 0, n_test\n","\n","    # if args.operation_mode == 'online': end = end-seq_impct\n","    # elif args.operation_mode == 'offline': end = end-2*seq_impct\n","    \n","    for i in range(start, end):\n","        Ferr.append(np.linalg.norm(states_true[i,:] - states_pred[i,:]) / np.linalg.norm(states_true[i,:]))\n","\n","    Ferr_avg = np.mean(Ferr)\n","    print(\"Error: \", Ferr_avg, '------------------------------------------------')\n","\n","    predData_Path = osp.join(path_to_weights, f'predData_s{n_sensor}_SNR{SNR}.hdf5')\n","    with h5py.File(predData_Path, 'w') as f:\n","        f.create_dataset('pred', data=states_pred)\n","        f.create_dataset('true', data=states_true)\n","        f.create_dataset('coords', data=cords)\n","        f.create_dataset('method', data=np.array(list('SD'.encode('utf8'))))\n","    \n","    return float(Ferr_avg)"],"execution_count":29,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"j37RCU8qvIHE"},"source":["# Final results"]},{"cell_type":"code","metadata":{"id":"E-NTCgyfGNBa","executionInfo":{"status":"ok","timestamp":1645620840693,"user_tz":-330,"elapsed":8,"user":{"displayName":"2K19/ME/275 YASH KUMAR","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"06719952441128256168"}}},"source":["def append_graph(graphData):\n","    error_path = osp.join(get_path('', 'P'), 'error.yaml')\n","    if not os.path.exists(error_path): \n","        errorPlots = graphData\n","        yaml.dump(errorPlots, open(error_path, 'w'))\n","    else: \n","        errorPlots_ = yaml.load(open(error_path, 'r'))\n","        errorPlots = {**errorPlots_, **graphData}\n","        yaml.dump(errorPlots, open(error_path, 'w'))"],"execution_count":30,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Jy03DcJbX6LD"},"source":["## Periodic"]},{"cell_type":"markdown","metadata":{"id":"qL_yyo2lnJIw"},"source":["### method and sensors"]},{"cell_type":"code","metadata":{"id":"k6WeKByMm2ox"},"source":["def periodic_mthd__snsr():\n","\n","  num_epochs, lr, bs, bsv, bn, early_stop  = 1200, 0.0007, 399, 133, 25, 200\n","  AutoEcoder(bn, lr, num_epochs, bs, bsv, early_stop, drop=0)\n","\n","\n","  for s in [1, 2, 5, 10]:\n","\n","      num_epochs, lr, hidden_size, seq_len, bn  = 2700, 0.0006, 50, 7, 25\n","      ARE_train_LSTM(s, seq_len, bn, hidden_size, num_epochs, lr, drop=0)\n","\n","      num_epochs, lr, bn  = 1200, 0.0009, 25\n","      PDS_train(s, bn, num_epochs, lr)\n","\n","      num_epochs, lr, wd  = 1500, 0.001, 1e-4\n","      SD_train(s, num_epochs, lr, wd)\n","\n","  error = np.zeros((3, 4))\n","  plot_s_list = np.array([1, 2])\n","  plot_seq_len_list = np.array([7])\n","  plot_image_idx = np.array([39])  \n","\n","  for s, i in zip([1], count(0, 1)):  #[1, 2, 5, 10]\n","    seq_len, bn, plot, save = 7, 25, (1, 1, 0), (1, 1, 0)\n","\n","    er1 = ARE_test(s, seq_len, bn,\n","                  plot, save,\n","                  plot_s_list=plot_s_list, \n","                  plot_seq_len_list=plot_seq_len_list,\n","                  plot_image_idx=plot_image_idx)\n","    \n","    er2 = PDS_test(s, bn, plot, save,\n","                   plot_s_list=plot_s_list,\n","                   plot_image_idx=plot_image_idx)\n","\n","    er3 = SD_test(s, plot, save,\n","                  plot_s_list=plot_s_list,\n","                  plot_image_idx=plot_image_idx)\n","\n","    error[:, i] = [er1, er2, er3]\n","\n","  error_path = get_path('', 'P')\n","  np.savetxt(osp.join(error_path, \"error_{}.csv\".format(args.RNN)), error, delimiter=\",\")\n","\n","\n","def imgPlot_fig9():\n","    n_sensor = 1\n","    seq_len = 7\n","    SNR = None\n","    try:\n","        predData_Path = osp.join(get_path('are', 'W'), f'predData_s{n_sensor}_seqlen{seq_len}_SNR{SNR}.hdf5')\n","        ARE_predData = h5py.File(predData_Path, 'r')\n","        print(f'loaded ARE pred data')\n","\n","        predData_Path = osp.join(get_path('pds', 'W'), f'predData_s{n_sensor}_SNR{SNR}.hdf5')\n","        PDS_predData = h5py.File(predData_Path, 'r')\n","        print(f'loaded PDS pred data')\n","\n","        predData_Path = osp.join(get_path('sd', 'W'), f'predData_s{n_sensor}_SNR{SNR}.hdf5')\n","        SD_predData = h5py.File(predData_Path, 'r')\n","        print(f'loaded SD pred data')\n","    except:\n","        raise Exception(FileNotFoundError)\n","\n","    plotParams = 39  # index of image to be plotted as data contains many predictions\n","    saveplot_Path = osp.join(get_path('are', 'P'), f'imgPlot_fig9')\n","    Plots().periodicFlow_fig(1, saveplot_Path, (ARE_predData, PDS_predData, SD_predData, plotParams))\n","\n","\n","\n","args.data_type = 'periodic' \n","args.exp = 'mthd__snsr'\n","\n","periodic_mthd__snsr()\n","imgPlot_fig9()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"zO1zFSwon2r8"},"source":["### method and SNR"]},{"cell_type":"code","metadata":{"id":"32jIfLvWnkvv"},"source":["from posixpath import join\n","def periodic_mthd__snr():\n","  \n","  num_epochs, lr, bs, bsv, bn, early_stop = 1200, 0.0007, 399, 133, 25, 200\n","  AutoEcoder(bn, lr, num_epochs, bs, bsv, early_stop, drop=0.35)\n","\n","  for s in [1, 2]:\n","\n","      num_epochs, lr, hidden_size, seq_len, bn  = 2700, 0.0006, 50, 7, 25\n","      ARE_train_LSTM(s, seq_len, bn, hidden_size, num_epochs, lr, drop=0.35)\n","\n","      num_epochs, lr, bn  = 1200, 0.0009, 25\n","      PDS_train(s, bn, num_epochs, lr)\n","\n","      num_epochs, lr, wd  = 1500, 0.001, 1e-4\n","      SD_train(s, num_epochs, lr, wd, drop=0.1)\n","\n","  SNR_range = [20]  # range(16, 84, 4)\n","  error = np.zeros((3, len(SNR_range)))\n","  plot_s_list = np.array([1, 2])\n","  plot_seq_len_list = np.array([7])\n","  plot_image_idx = np.array([39])\n","  plot_SNR = np.array([20, 72])\n","  \n","  for s in [1]: # [1, 2]\n","    for SNR, i in zip(SNR_range, count(0, 1)):\n","      seq_len, bn, plot, save, use_stats = 7, 25, (1, 1, 1), (1, 1, 1), 1\n","      print('\\n no. of sensors:',s,' SNR:',SNR)\n","\n","      er1 = ARE_test(s, seq_len, bn,\n","                    plot, save,\n","                    drop=0.35, SNR=SNR,\n","                    plot_s_list=plot_s_list, \n","                    plot_seq_len_list=plot_seq_len_list,\n","                    plot_image_idx=plot_image_idx,\n","                    plot_SNR=plot_SNR,\n","                    use_stats=use_stats)\n","      \n","      er2 = PDS_test(s, bn, plot, save,\n","                    drop=1, SNR=SNR,\n","                    plot_s_list=plot_s_list,\n","                    plot_image_idx=plot_image_idx,\n","                    plot_SNR=plot_SNR,\n","                    use_stats=use_stats)\n","\n","      er3 = SD_test(s, plot, save,\n","                    drop=0.1, SNR=SNR,\n","                    plot_s_list=plot_s_list,\n","                    plot_image_idx=plot_image_idx,\n","                    plot_SNR=plot_SNR,\n","                    use_stats=use_stats)\n","\n","      error[:, i] = [er1, er2, er3]\n","\n","    error_path = get_path('', 'P')\n","    np.savetxt(osp.join(error_path, \"error_{}_{}.csv\".format(args.RNN, s)), error, delimiter=\",\")\n","\n","\n","def imgPlot_fig10():\n","\n","    n_sensor = 1\n","    seq_len = 7\n","    SNR = 20\n","    try:\n","        predData_Path = osp.join(get_path('are', 'W'), f'predData_s{n_sensor}_seqlen{seq_len}_SNR{SNR}.hdf5')\n","        ARE_predData = h5py.File(predData_Path, 'r')\n","        print(f'loaded ARE pred data')\n","\n","        predData_Path = osp.join(get_path('pds', 'W'), f'predData_s{n_sensor}_SNR{SNR}.hdf5')\n","        PDS_predData = h5py.File(predData_Path, 'r')\n","        print(f'loaded PDS pred data')\n","\n","        predData_Path = osp.join(get_path('sd', 'W'), f'predData_s{n_sensor}_SNR{SNR}.hdf5')\n","        SD_predData = h5py.File(predData_Path, 'r')\n","        print(f'loaded SD pred data')\n","    except:\n","        raise Exception(FileNotFoundError)\n","\n","    plotParams = 39  # index of image to be plotted as data contains many predictions\n","    saveplot_Path = osp.join(get_path('are', 'P'), f'imgPlot_fig10')\n","    Plots().periodicFlow_fig(1, saveplot_Path, (ARE_predData, PDS_predData, SD_predData, plotParams))\n","\n","\n","args.data_type = 'periodic' \n","args.exp = 'mthd__snr'\n","\n","periodic_mthd__snr()\n","imgPlot_fig10()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"s_gLbgljX-nS"},"source":["## Transient"]},{"cell_type":"markdown","metadata":{"id":"tUNo5vl-oQIc"},"source":["### method and sensors"]},{"cell_type":"code","metadata":{"id":"nXEEN2K3YNbU"},"source":["def transient_mthd__snsr():\n","\n","  num_epochs, lr, bs, bsv, bn, early_stop  = 1200, 0.001, 399, 133, 25, 200\n","  AutoEcoder(bn, lr, num_epochs, bs, bsv, early_stop, drop=0)\n","\n","\n","  for s in [1, 2, 5, 10]:\n","\n","      num_epochs, lr, hidden_size, seq_len, bn  = 2600, 0.0008, 50, 7, 25\n","      ARE_train_LSTM(s, seq_len, bn, hidden_size, num_epochs, lr, drop=0)\n","\n","      num_epochs, lr, bn  = 1500, 0.0009, 25\n","      PDS_train(s, bn, num_epochs, lr)\n","\n","      num_epochs, lr, wd  = 1500, 0.001, 1e-4\n","      SD_train(s, num_epochs, lr, wd)\n","\n","  error = np.zeros((3, 4))\n","  plot_s_list = np.array([1, 2])\n","  plot_seq_len_list = np.array([7])\n","  plot_image_idx = np.array([335])    \n","\n","  for s, i in zip([1], count(0, 1)):  # [1, 2, 5, 10]\n","    seq_len, bn, plot, save = 7, 25, (1, 1, 0), (1, 1, 0)\n","\n","    er1 = ARE_test(s, seq_len, bn,\n","                  plot, save,\n","                  plot_s_list=plot_s_list, \n","                  plot_seq_len_list=plot_seq_len_list,\n","                  plot_image_idx=plot_image_idx)\n","    \n","    er2 = PDS_test(s, bn, plot, save,\n","                   plot_s_list=plot_s_list,\n","                   plot_image_idx=plot_image_idx)\n","\n","    er3 = SD_test(s, plot, save,\n","                  plot_s_list=plot_s_list,\n","                  plot_image_idx=plot_image_idx)\n","\n","    error[:, i] = [er1, er2, er3]\n","\n","  error_path = get_path('', 'P')\n","  np.savetxt(osp.join(error_path, \"error_{}.csv\".format(args.RNN)), error, delimiter=\",\")\n","\n","\n","def imgPlot_fig12():\n","\n","    n_sensor = 1\n","    seq_len = 7\n","    SNR = None\n","    try:\n","        predData_Path = osp.join(get_path('are', 'W'), f'predData_s{n_sensor}_seqlen{seq_len}_SNR{SNR}.hdf5')\n","        ARE_predData = h5py.File(predData_Path, 'r')\n","        print(f'loaded ARE pred data')\n","\n","        predData_Path = osp.join(get_path('pds', 'W'), f'predData_s{n_sensor}_SNR{SNR}.hdf5')\n","        PDS_predData = h5py.File(predData_Path, 'r')\n","        print(f'loaded PDS pred data')\n","\n","        predData_Path = osp.join(get_path('sd', 'W'), f'predData_s{n_sensor}_SNR{SNR}.hdf5')\n","        SD_predData = h5py.File(predData_Path, 'r')\n","        print(f'loaded SD pred data')\n","    except:\n","        raise Exception(FileNotFoundError)\n","\n","    plotParams = 335  # index of image to be plotted as data contains many predictions\n","    saveplot_Path = osp.join(get_path('are', 'P'), f'imgPlot_fig12')\n","    Plots().periodicFlow_fig(1, saveplot_Path, (ARE_predData, PDS_predData, SD_predData, plotParams), figsize=(11, 6))\n","\n","\n","args.data_type = 'transient' \n","args.exp = 'mthd__snsr'\n","\n","transient_mthd__snsr()\n","imgPlot_fig12()\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"n1Yn6stOGiKR"},"source":["### method and SNR"]},{"cell_type":"code","metadata":{"id":"w7QYo1ZsGvGJ"},"source":["def transient_mthd__snr():\n","\n","  num_epochs, lr, bs, bsv, bn, early_stop  = 1500, 0.0007, 399, 133, 25, 250\n","  AutoEcoder(bn, lr, num_epochs, bs, bsv, early_stop, drop=0.35)\n","\n","\n","  for s in [4]:  #[1, 2]:\n","\n","      num_epochs, lr, hidden_size, seq_len, bn  = 2600, 0.0008, 50, 7, 25\n","      ARE_train_LSTM(s, seq_len, bn, hidden_size, num_epochs, lr, drop=0.35)\n","\n","      num_epochs, lr, bn  = 1500, 0.0009, 25\n","      PDS_train(s, bn, num_epochs, lr)\n","\n","      num_epochs, lr, wd  = 1700, 0.0009, 1e-4\n","      SD_train(s, num_epochs, lr, wd, drop=0.1)\n","\n","\n","  SNR_range = [28]  #range(4, 84, 4)\n","  error = np.zeros((3, len(SNR_range)))\n","  plot_s_list = np.array([1, 2, 4])\n","  plot_seq_len_list = np.array([7])\n","  plot_image_idx = np.array([335])\n","  plot_SNR = np.array([28, 76])\n","  \n","  for s in [2]: # [1, 2]\n","    for SNR, i in zip(SNR_range, count(0, 1)):\n","      seq_len, bn, plot, save = 7, 25, (1, 1, 1), (1, 1, 1)\n","      print('no. of sensors:', s, ' SNR:', SNR)\n","\n","      er1 = ARE_test(s, seq_len, bn,\n","                    plot, save,\n","                    drop=0.35, SNR=SNR,\n","                    plot_s_list=plot_s_list, \n","                    plot_seq_len_list=plot_seq_len_list,\n","                    plot_image_idx=plot_image_idx,\n","                    plot_SNR=plot_SNR)\n","      \n","      er2 = PDS_test(s, bn, plot, save,\n","                    SNR=SNR,\n","                    plot_s_list=plot_s_list,\n","                    plot_image_idx=plot_image_idx,\n","                    plot_SNR=plot_SNR)\n","\n","      er3 = SD_test(s, plot, save,\n","                    drop=0.1, SNR=SNR,\n","                    plot_s_list=plot_s_list,\n","                    plot_image_idx=plot_image_idx,\n","                    plot_SNR=plot_SNR)\n","      \n","      \n","\n","      error[:, i] = [er1, er2, er3]\n","\n","    error_path = get_path('', 'P')\n","    np.savetxt(osp.join(error_path, \"error_{}_{}.csv\".format(args.RNN, s)), error, delimiter=\",\")\n","\n","\n","def imgPlot_fig13():\n","\n","    n_sensor = 2\n","    seq_len = 7\n","    SNR = 28\n","    try:\n","        predData_Path = osp.join(get_path('are', 'W'), f'predData_s{n_sensor}_seqlen{seq_len}_SNR{SNR}.hdf5')\n","        ARE_predData = h5py.File(predData_Path, 'r')\n","        print(f'loaded ARE pred data')\n","\n","        predData_Path = osp.join(get_path('pds', 'W'), f'predData_s{n_sensor}_SNR{SNR}.hdf5')\n","        PDS_predData = h5py.File(predData_Path, 'r')\n","        print(f'loaded PDS pred data')\n","\n","        predData_Path = osp.join(get_path('sd', 'W'), f'predData_s{n_sensor}_SNR{SNR}.hdf5')\n","        SD_predData = h5py.File(predData_Path, 'r')\n","        print(f'loaded SD pred data')\n","    except:\n","        raise Exception(FileNotFoundError)\n","\n","    plotParams = 325  # index of image to be plotted as data contains many predictions\n","    saveplot_Path = osp.join(get_path('are', 'P'), f'imgPlot_fig13')\n","    Plots().periodicFlow_fig(1, saveplot_Path, (ARE_predData, PDS_predData, SD_predData, plotParams), figsize=(11, 6))\n","\n","args.data_type = 'transient' \n","args.exp = 'mthd__snr'\n","\n","transient_mthd__snr()\n","imgPlot_fig13()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"RlB5k2FKMDdM"},"source":["### sensor and sequence len"]},{"cell_type":"code","metadata":{"id":"bJYUq6StL339"},"source":["def transient_snsr__seq_len():\n","  args.data_type = 'transient' \n","  args.exp = 'snsr__seq_len'\n","\n","  num_epochs, lr, bs, bsv, bn, early_stop = 1300, 0.0007, 399, 133, 25, 200\n","  AutoEcoder(bn, lr, num_epochs, bs, bsv, early_stop, drop=0)\n","\n","  for s in [1, 2, 5, 10]:\n","    for seq_len in [3, 5, 7, 9]:\n","\n","        num_epochs, lr, hidden_size, s, bn  = 2700, 0.0008, 50, s, 25\n","        ARE_train_LSTM(s, seq_len, bn, hidden_size, num_epochs, lr, drop=0.35)\n","\n","  error = np.zeros((4, 4))\n","  plot_s_list = np.array([1, 2])\n","  plot_seq_len_list = np.array([7])\n","  plot_image_idx = np.array([335])  \n","  plot_SNR = np.array([28, 76])\n","\n","  for SNR in [None]:  \n","    for s, j in zip([2], count(0, 1)):  # [1, 2, 5, 10]\n","      for seq_len, i in zip([5], count(0, 1)):  # [3, 5, 7, 9]\n","        bn, plot, save = (25, (1, 1, 0), (0, 0, 0))\n","        print('\\n no. of sensors:',s,' seq_len:',seq_len)\n","        \n","        er1 = ARE_test(s, seq_len, bn,\n","                       plot, save,\n","                       plot_s_list=plot_s_list, \n","                       plot_seq_len_list=plot_seq_len_list,\n","                       plot_image_idx=plot_image_idx)\n","        \n","        # er1 = ARE_test(s, seq_len, bn,\n","        #                plot, save,\n","        #                drop=0.35, SNR=SNR,\n","        #                plot_s_list=plot_s_list, \n","        #                plot_seq_len_list=plot_seq_len_list,\n","        #                plot_image_idx=plot_image_idx,\n","        #                plot_SNR=plot_SNR)\n","\n","        error[j, i] = er1\n","\n","    # error_path = get_path('', 'P')\n","    # np.savetxt(osp.join(error_path, \"error_{}_SNR{}.csv\".format(args.RNN, SNR)), error, delimiter=\",\")\n","\n","transient_snsr__seq_len()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"eK0jmMLvC1ui"},"source":["## SST"]},{"cell_type":"markdown","metadata":{"id":"HKWvynDgZWeL"},"source":["### method and sensor at certain noise level"]},{"cell_type":"code","metadata":{"id":"yeAk4f1lZf9G"},"source":["def sst_mthd__snsr_with_noise():\n","\n","    num_epochs, lr, bs, bsv, bn, early_stop  = 1300, 0.0007, 400, 100, 25, 100\n","    AutoEcoder(bn, lr, num_epochs, bs, bsv, early_stop, drop=0.35)\n","\n","    for s in [4, 8, 16, 32]:\n","\n","        num_epochs, lr, hidden_size, seq_len, bn  = 700, 0.0006, 50, 7, 25\n","        ARE_train_LSTM(s, seq_len, bn, hidden_size, num_epochs, lr, drop=0.35)\n","\n","        num_epochs, lr, bn  = 700, 0.0007, 25\n","        PDS_train(s, bn, num_epochs, lr)\n","\n","        num_epochs, lr, wd  = 600, 0.0009, 1e-4\n","        SD_train(s, num_epochs, lr, wd, drop=0.1)\n","\n","    error = np.zeros((3, 4))\n","    plot_s_list = np.array([4, 8])\n","    plot_seq_len_list = np.array([7])\n","    plot_image_idx = np.array([20])  \n","    plot_SNR = np.array([20])\n","\n","    for SNR in [20]: #[10, 20, 30]:  \n","        for s, i in zip([4], count(0, 1)):  #zip([4, 8, 16, 32], count(0, 1)):  \n","            seq_len, bn, plot, save, use_stats = 7, 25, (1, 1, 1), (1, 1, 1), 1\n","\n","            er1 = ARE_test(s, seq_len, bn,\n","                            plot, save,\n","                            drop=0.35, SNR=SNR,\n","                            plot_s_list=plot_s_list, \n","                            plot_seq_len_list=plot_seq_len_list,\n","                            plot_image_idx=plot_image_idx,\n","                            plot_SNR=plot_SNR,\n","                            use_stats=use_stats)\n","            \n","            er2 = PDS_test(s, bn, plot, save,\n","                            SNR=SNR,\n","                            plot_s_list=plot_s_list,\n","                            plot_image_idx=plot_image_idx,\n","                            plot_SNR=plot_SNR,\n","                            use_stats=use_stats)\n","\n","            er3 = SD_test(s, plot, save,\n","                            drop=0.1, SNR=SNR,\n","                            plot_s_list=plot_s_list,\n","                            plot_image_idx=plot_image_idx,\n","                            plot_SNR=plot_SNR,\n","                            use_stats=use_stats)\n","\n","            error[:, i] = [er1, er2, er3]\n","\n","        error_path = get_path('', 'P')\n","        np.savetxt(osp.join(error_path, \"error_{}_seq_len{}_SNR{}.csv\".format(args.RNN, seq_len, SNR)), error, delimiter=\",\")\n","\n","\n","def imgPlot_fig17():\n","\n","    try:\n","        predData_Path = osp.join(get_path('are', 'W'), f'predData.hdf5')\n","        ARE_predData = h5py.File(predData_Path, 'r')\n","        print(f'loaded ARE pred data')\n","\n","        predData_Path = osp.join(get_path('pds', 'W'), f'predData.hdf5')\n","        PDS_predData = h5py.File(predData_Path, 'r')\n","        print(f'loaded PDS pred data')\n","\n","        predData_Path = osp.join(get_path('sd', 'W'), f'predData.hdf5')\n","        SD_predData = h5py.File(predData_Path, 'r')\n","        print(f'loaded SD pred data')\n","    except:\n","        raise Exception(FileNotFoundError)\n","\n","    plotParams = 20\n","    saveplot_Path = osp.join(get_path('are', 'P'), f'imgPlot_fig17')\n","    Plots().sst_fig(1, saveplot_Path, (ARE_predData, PDS_predData, SD_predData, plotParams))\n","\n","args.data_type = 'sea_temp' \n","args.exp = 'mthd__snsr_with_noise'\n","\n","sst_mthd__snsr_with_noise()\n","imgPlot_fig17()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"XXrgNCixLhlF"},"source":["### method and bottle neck"]},{"cell_type":"code","metadata":{"id":"Cw93GMiZMISF"},"source":["def sst_mthd__bn():\n","  args.data_type = 'sea_temp' \n","  args.exp = 'mthd__bn'\n","\n","  for bn in [5, 15, 25, 50]:\n","\n","    num_epochs, lr, bs, bsv, bn, early_stop  = 1300, 0.0007, 400, 100, bn, 100\n","    AutoEcoder(bn, lr, num_epochs, bs, bsv, early_stop, drop=0.35)\n","\n","    for s in [2, 4, 8]:\n","\n","      num_epochs, lr, hidden_size, seq_len, bn  = 700, 0.0006, 50, 7, bn\n","      ARE_train_LSTM(s, seq_len, bn, hidden_size, num_epochs, lr, drop=0.35)\n","\n","      num_epochs, lr, bn  = 700, 0.0009, bn\n","      PDS_train(s, bn, num_epochs, lr)\n","\n","  error = np.zeros((2, 4))\n","  plot_s_list = np.array([2])\n","  plot_seq_len_list = np.array([7])\n","  plot_image_idx = np.array([40])  # 5\n","  plot_SNR = np.array([20])\n","  \n","  for s in [2, 4, 8]:\n","    for bn, i in zip([5, 15, 25, 50], count(0, 1)): \n","\n","      seq_len, bn, plot, save, use_stats, SNR = 7, bn, (1, 1, 1), (1, 1, 1), 1, 20\n","\n","      er1 = ARE_test(s, seq_len, bn,\n","                    plot, save,\n","                    drop=0.35, SNR=SNR,\n","                    plot_s_list=plot_s_list, \n","                    plot_seq_len_list=plot_seq_len_list,\n","                    plot_image_idx=plot_image_idx,\n","                    plot_SNR=plot_SNR)\n","      \n","      er2 = PDS_test(s, bn, plot, save,\n","                    SNR=SNR,\n","                    plot_s_list=plot_s_list,\n","                    plot_image_idx=plot_image_idx,\n","                    plot_SNR=plot_SNR)\n","      \n","      error[:, i] = [er1, er2]\n","\n","    error_path = get_path('', 'P')\n","    np.savetxt(osp.join(error_path, \"error_{}_{}.csv\".format(args.RNN, s)), error, delimiter=\",\")\n","\n","\n","sst_mthd__bn()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"vMi5QJ8UV9zN"},"source":["### method and SNR"]},{"cell_type":"code","metadata":{"id":"zBL4WCWpLcKE"},"source":["def sst_mthd__snr():\n","  args.data_type = 'sea_temp' \n","  args.exp = 'mthd__snr'\n","\n","  num_epochs, lr, bs, bsv, bn, early_stop  = 1300, 0.0007, 400, 100, 25, 100\n","  AutoEcoder(bn, lr, num_epochs, bs, bsv, early_stop, drop=0.35)\n","\n","\n","  for s in [2, 4, 8]:  \n","\n","      num_epochs, lr, hidden_size, seq_len, bn  = 700, 0.0006, 50, 7, 25\n","      ARE_train_LSTM(s, seq_len, bn, hidden_size, num_epochs, lr, drop=0.35)\n","\n","      num_epochs, lr, bn  = 700, 0.0009, 25\n","      PDS_train(s, bn, num_epochs, lr)\n","\n","      num_epochs, lr, wd  = 500, 0.0009, 1e-4\n","      SD_train(s, num_epochs, lr, wd, drop=0.1)\n","\n","  SNR_range = range(5, 80, 5)\n","  error = np.zeros((3, len(SNR_range)))\n","  plot_s_list = np.array([4])\n","  plot_seq_len_list = np.array([7])\n","  plot_image_idx = np.array([20])\n","  plot_SNR = np.array([10, 70])\n","  \n","  for s in [2, 4, 8]:\n","    for SNR, i in zip(SNR_range, count(0, 1)):\n","      seq_len, bn, plot, save, use_stats = 7, 25, (0, 0, 0), (0, 0, 0), 1\n","\n","      er1 = ARE_test(s, seq_len, bn,\n","                    plot, save,\n","                    drop=0.35, SNR=SNR,\n","                    plot_s_list=plot_s_list, \n","                    plot_seq_len_list=plot_seq_len_list,\n","                    plot_image_idx=plot_image_idx,\n","                    plot_SNR=plot_SNR)\n","      \n","      er2 = PDS_test(s, bn, plot, save,\n","                    SNR=SNR,\n","                    plot_s_list=plot_s_list,\n","                    plot_image_idx=plot_image_idx,\n","                    plot_SNR=plot_SNR)\n","\n","      er3 = SD_test(s, plot, save,\n","                    drop=0.1, SNR=SNR,\n","                    plot_s_list=plot_s_list,\n","                    plot_image_idx=plot_image_idx,\n","                    plot_SNR=plot_SNR)\n","\n","      error[:, i] = [er1, er2, er3]\n","\n","    error_path = get_path('', 'P')\n","    np.savetxt(osp.join(error_path, \"error_{}_{}.csv\".format(args.RNN, s)), error, delimiter=\",\")\n","\n","sst_mthd__snr()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"pjverEx3cxM9"},"source":["## Burgers1D"]},{"cell_type":"markdown","metadata":{"id":"-uEar-G9dEPB"},"source":["### method and sensors"]},{"cell_type":"code","metadata":{"id":"nuX1KUoydEPC"},"source":["def burgers1D_mthd__snsr():\n","\n","    num_epochs, lr, bs, bsv, bn, early_stop  = 500, 0.0009, 51*3, 1, 16, 200\n","    AutoEcoder(bn, lr, num_epochs, bs, bsv, early_stop, drop=0)\n","\n","\n","    for s in [10]: # [10]\n","\n","        num_epochs, lr, hidden_size, seq_len, bn, bs  = 800, 0.001, 50, 7, 16, 1\n","        ARE_train_LSTM(s, seq_len, bn, hidden_size, num_epochs, lr, drop=0)\n","\n","        num_epochs, lr, bn  = 800, 0.001, 25\n","        PDS_train(s, bn, num_epochs, lr)\n","\n","        num_epochs, lr, wd  = 800, 0.001, 1e-4\n","        SD_train(s, num_epochs, lr, wd)\n","\n","    \n","    # --------------------------------------------------------------------------\n","    plot_s_list = np.array([3, 2, 4, 5, 8, 10])\n","    plot_seq_len_list = np.array([7, 9])\n","    plot_image_idx = np.array([15, 23, 42])  \n","    sensor_ls = [10]#[8, 10, 12]\n","\n","    err_ls = []\n","    for i, s in enumerate(sensor_ls):\n","        seq_len, bn, plot, save = 7, 16, (1, 1, 1), (1, 1, 1)\n","\n","        err = ARE_test(s, seq_len, bn,\n","                    plot, save,\n","                    hidden_size=50,\n","                    plot_s_list=plot_s_list, \n","                    plot_seq_len_list=plot_seq_len_list,\n","                    plot_image_idx=plot_image_idx)\n","        err_ls.append(err)\n","\n","    graphData = {}\n","    graphData[token_hex(2)] =  {'legend': f'ARE',\n","                                'data': err_ls,\n","                                'color': 'red',\n","                                'x_ticks_lables': sensor_ls}\n","    append_graph(graphData)\n","\n","    # --------------------------------------------------------------------------\n","    err_ls = []\n","    for i, s in enumerate(sensor_ls):\n","        bn = 25\n","        err = PDS_test(s, bn, plot, save,\n","                    plot_s_list=plot_s_list,\n","                    plot_image_idx=plot_image_idx)\n","        err_ls.append(err)\n","\n","    graphData = {}\n","    graphData[token_hex(2)] =  {'legend': f'PDS',\n","                                'data': err_ls,\n","                                'color': 'blue',\n","                                'x_ticks_lables': sensor_ls}\n","    append_graph(graphData)\n","\n","    # --------------------------------------------------------------------------\n","    err_ls = []\n","    for i, s in enumerate(sensor_ls):\n","        err = SD_test(s, plot, save,\n","                    plot_s_list=plot_s_list,\n","                    plot_image_idx=plot_image_idx)\n","        err_ls.append(err)\n","\n","    graphData = {}\n","    graphData[token_hex(2)] =  {'legend': f'SD',\n","                                'data': err_ls,\n","                                'color': 'green',\n","                                'x_ticks_lables': sensor_ls}\n","    append_graph(graphData)\n","\n","\n","def imgPlot_fig100():\n","    n_sensor = 10\n","    seq_len = 7\n","    SNR = None\n","    try:\n","        predData_Path = osp.join(get_path('are', 'W'), f'predData_s{n_sensor}_seqlen{seq_len}_SNR{SNR}.hdf5')\n","        ARE_predData = h5py.File(predData_Path, 'r')\n","        print(f'loaded ARE pred data')\n","\n","        predData_Path = osp.join(get_path('pds', 'W'), f'predData_s{n_sensor}_SNR{SNR}.hdf5')\n","        PDS_predData = h5py.File(predData_Path, 'r')\n","        print(f'loaded PDS pred data')\n","\n","        predData_Path = osp.join(get_path('sd', 'W'), f'predData_s{n_sensor}_SNR{SNR}.hdf5')\n","        SD_predData = h5py.File(predData_Path, 'r')\n","        print(f'loaded SD pred data')\n","    except:\n","        raise Exception(FileNotFoundError)\n","\n","    \n","    saveplot_Path = osp.join(get_path('are', 'P'), f'imgPlot_fig102')\n","\n","    # plotParams = 39\n","    # Plots().burgers1D_imgPlot(1, saveplot_Path, (ARE_predData, PDS_predData, SD_predData, plotParams), figsize=(14, 6))\n","    # pdb.set_trace()\n","    # plotParams = [23, 29, 39, 47] 161\n","    plotParams = [23, 38, 113, 183] \n","    ARE_predData['pred'].shape[0]\n","    Plots().burgers1D_imgPlot1(1, saveplot_Path, (ARE_predData, PDS_predData, SD_predData, plotParams), figsize=(14, 6))\n","\n","\n","def graphPlot_fig_burgers1D_mthd():\n","    error_path = osp.join(get_path('', 'P'), 'error.yaml')\n","    errorPlots = yaml.load(open(error_path, 'r'))\n","    plotParams = {'xlabel': 'No. of sensors', 'ylabel': 'Prediction error'}\n","    Plots().transientFlow_graphPlot_senLoc(1, osp.join(get_path('', 'P'), 'error'), errorPlots, plotParams)\n","    \n","\n","\n","args.data_type = 'burgers1D' \n","args.exp = 'mthd__snsr'\n","\n","burgers1D_mthd__snsr()\n","imgPlot_fig100()\n","graphPlot_fig_burgers1D_mthd()\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"3bqOcsCvA0Vl"},"source":["# Recycle Bin"]}]}