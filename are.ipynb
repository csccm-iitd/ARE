{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "5.15.2021__are.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "1xq60D_tqajl",
        "4cbgwZWWfWpp",
        "P5vicRPTfcT4",
        "rUHtHdRLA8lq",
        "2rx07OS-faK9",
        "AcHm5KadgWzG",
        "i01G721Agfi0",
        "Qcx78Uurf7Li",
        "w0gojo-TBmgh",
        "2zzdQRBFBxO4",
        "gGf32xK4qIXQ",
        "mO-omVf397wc",
        "mZjvskHx-lYk",
        "-5nYRiX1f-ya",
        "v1JLqOIUeQ5p",
        "Jy03DcJbX6LD",
        "eK0jmMLvC1ui",
        "gcdnxVd39nbx",
        "S45mQUejJA73",
        "_shlfwntJGgW"
      ],
      "toc_visible": true,
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Yash-11/are/blob/main/are.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1xq60D_tqajl"
      },
      "source": [
        "# INSTRUCTIONS"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V4S71cwpj_bZ"
      },
      "source": [
        "**Use 'ctrl+]' to collapse all** if running in Google Colab"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y-M1gNtBqwtz"
      },
      "source": [
        "Steps to be followed\n",
        "\n",
        "1. Mount your drive containing data.\n",
        "2. Set 'prjct_dir' and 'data_dir' arguments in argparser. \n",
        "3. Run all cells before \"Final results\" section.\n",
        "4. Run the exp(experiment) required in \"Final results\" section. \n",
        "\n",
        "Note: \n",
        "* Arguments except 'prjct_dir' and 'data_dir' will be set automatically according to selected exp in \"Final results\"\n",
        "\n",
        "* *seq_len(no. of sequence of samples used for rnn/lstm)*\n",
        "**  Offline: data is coming from both side. Therefore, No. of sequence of samples used for rnn/lstm = seq_len taken\n",
        "** Online: data is coming from one side.\n",
        "No. of sequence of samples used for rnn/lstm = (seq_len+1)/2, i.e. if seq_len = 7 then  no. of samples used for rnn/lstm = 4"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "G2PXhaN9xc7b",
        "outputId": "2f787d8c-d45f-4daa-d3b0-c1d7677bfb4d"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4cbgwZWWfWpp"
      },
      "source": [
        "\n",
        "# ARGPARSE"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wFvFfmNPR04T"
      },
      "source": [
        "import torch\n",
        "import argparse\n",
        "parser = argparse.ArgumentParser('ARE')\n",
        "\n",
        "\"\"\"\n",
        "Change directory path \n",
        "prjct_dir: saving weights and results \n",
        "data_dir: data \n",
        "\"\"\"\n",
        "prjct_dir = '../content/drive/My Drive/Colab Notebooks/are'\n",
        "data_dir = '../content/drive/My Drive/Data/are_data'\n",
        "\n",
        "parser.add_argument('--prjct_dir', type=str, default=prjct_dir)\n",
        "parser.add_argument('--data_dir', type=str, default=data_dir)\n",
        "\n",
        "parser.add_argument('--exp', type=str, choices=['mthd__snsr', 'snsr__seq_len', 'mthd__snr', \n",
        "                                                'mthd__tr_samp', 'mthd__bn',\n",
        "                                                'rndm_trials', 'mthd__snsr_with_noise'], default='mthd__snsr')\n",
        "parser.add_argument('--data_type', type=str, choices=['periodic', 'transient', 'sea_temp', 'turbulence'], default='turbulence')\n",
        "# non_int.parse_known_args()\n",
        "parser.add_argument('--operation_mode', type=str, choices=['offline', 'online'], default='online')  \n",
        "parser.add_argument('--RNN', type=str, choices=['lstm', 'rnn'], default='lstm')\n",
        "parser.add_argument('--gpu', type=int, default=0)\n",
        "args = parser.parse_args(\"\")\n",
        "\n",
        "device = torch.device('cuda:' + str(args.gpu) if torch.cuda.is_available() else 'cpu')"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P5vicRPTfcT4"
      },
      "source": [
        "# Utils"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tFdGqL6FfdSY",
        "outputId": "8d633763-16c5-424d-da68-9d3024db89e5"
      },
      "source": [
        "pip install cmocean"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting cmocean\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/48/02/d0f19b00b252fd972e3daec05be73aa811091528f21b90442a15d6a96d89/cmocean-2.0-py3-none-any.whl (223kB)\n",
            "\r\u001b[K     |█▌                              | 10kB 12.1MB/s eta 0:00:01\r\u001b[K     |███                             | 20kB 8.2MB/s eta 0:00:01\r\u001b[K     |████▍                           | 30kB 6.0MB/s eta 0:00:01\r\u001b[K     |█████▉                          | 40kB 3.5MB/s eta 0:00:01\r\u001b[K     |███████▍                        | 51kB 3.8MB/s eta 0:00:01\r\u001b[K     |████████▉                       | 61kB 4.3MB/s eta 0:00:01\r\u001b[K     |██████████▎                     | 71kB 4.7MB/s eta 0:00:01\r\u001b[K     |███████████▊                    | 81kB 4.9MB/s eta 0:00:01\r\u001b[K     |█████████████▏                  | 92kB 4.8MB/s eta 0:00:01\r\u001b[K     |██████████████▊                 | 102kB 4.3MB/s eta 0:00:01\r\u001b[K     |████████████████▏               | 112kB 4.3MB/s eta 0:00:01\r\u001b[K     |█████████████████▋              | 122kB 4.3MB/s eta 0:00:01\r\u001b[K     |███████████████████             | 133kB 4.3MB/s eta 0:00:01\r\u001b[K     |████████████████████▋           | 143kB 4.3MB/s eta 0:00:01\r\u001b[K     |██████████████████████          | 153kB 4.3MB/s eta 0:00:01\r\u001b[K     |███████████████████████▌        | 163kB 4.3MB/s eta 0:00:01\r\u001b[K     |█████████████████████████       | 174kB 4.3MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▍     | 184kB 4.3MB/s eta 0:00:01\r\u001b[K     |████████████████████████████    | 194kB 4.3MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▍  | 204kB 4.3MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▉ | 215kB 4.3MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 225kB 4.3MB/s \n",
            "\u001b[?25hInstalling collected packages: cmocean\n",
            "Successfully installed cmocean-2.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BkaijrX98151"
      },
      "source": [
        "import pdb\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import scipy.linalg as la\n",
        "\n",
        "import os\n",
        "import os.path as osp\n",
        "\n",
        "import torchvision\n",
        "from torch import nn\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision import transforms\n",
        "from torchvision.utils import save_image\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data.dataset import Dataset\n",
        "from torch.autograd import Variable\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import cmocean\n",
        "import time\n",
        "import random\n",
        "from itertools import count\n",
        "from itertools import chain\n",
        "from skimage.transform import resize"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e5JnLKnIVuii"
      },
      "source": [
        "def get_path(method, string, **kwargs):\n",
        "  \"\"\" \n",
        "  determine path to directory based upon string\n",
        "  e.g. W=1 in kwargs return path of dir. to save weights\n",
        "\n",
        "  :returns: path to directory\n",
        "  \"\"\"\n",
        "\n",
        "  dir1 = args.data_type\n",
        "  dir2 = args.exp\n",
        "  dir3 = osp.join(method, args.operation_mode, args.RNN) if method == 'are' else method\n",
        "  if args.exp == 'mthd__tr_samp':\n",
        "     if hasattr(args, 'tr_samp'): add = 'tr_samp{}'.format(args.tr_samp)  \n",
        "     else: raise Exception('number of training samples are required')\n",
        "\n",
        "  if string == 'W':\n",
        "      path = osp.join(args.prjct_dir, 'weights_results', dir1, dir2, dir3)\n",
        "\n",
        "  elif string == 'W_AE':\n",
        "      path = osp.join(args.prjct_dir, 'weights_results', dir1, dir2, method, 'autoencoder')\n",
        "\n",
        "  elif string == 'I':\n",
        "      path = osp.join(args.prjct_dir, 'weights_results', dir1, dir2, dir3)\n",
        "\n",
        "  elif string == 'P':\n",
        "      path = osp.join(args.prjct_dir, 'weights_results', dir1, dir2)\n",
        "\n",
        "  else:\n",
        "    raise ValueError('type of path not recongnized')\n",
        "\n",
        "  path = osp.join(path, add) if args.exp == 'mthd__tr_samp' else path\n",
        "  path = osp.join(path, kwargs.get('add_dir')) if kwargs.get('add_dir') else path\n",
        "  if not os.path.exists(path):\n",
        "      os.makedirs(path)\n",
        "\n",
        "  return path\n",
        "\n",
        "def get_name(method, string, **kwargs):\n",
        "  \"\"\" \n",
        "  determine name of file \n",
        "  e.g. W=1 in kwargs return name of file of autoencoder weights\n",
        "\n",
        "  :returns: path to directory\n",
        "  \"\"\"\n",
        "\n",
        "  if string == 'W':\n",
        "\n",
        "    sensor_number = kwargs.get('sensor_number')\n",
        "    seq_len = kwargs.get('seq_len')\n",
        "    bottle_neck = kwargs.get('bottle_neck')\n",
        "\n",
        "    if method == 'are':\n",
        "      return 'weights_s{}_sq{}_bn{}'.format(sensor_number, seq_len, bottle_neck) \n",
        "\n",
        "    elif method == 'pds':\n",
        "      return 'weights_s{}_bn{}'.format(sensor_number, bottle_neck)\n",
        "\n",
        "    elif method == 'sd':\n",
        "      if kwargs.get('drop'):\n",
        "        return 'weights_s{}_drop'.format(sensor_number)\n",
        "      else:\n",
        "        return 'weights_s{}'.format(sensor_number)\n",
        "\n",
        "  elif string == 'W_AE':\n",
        "\n",
        "    bn = kwargs.get('bottle_neck')\n",
        "    dropout = kwargs.get('drop')\n",
        "    \n",
        "    if dropout:\n",
        "      return 'weights_bn{}_drop{}'.format(bn, dropout)\n",
        "    else:\n",
        "      return 'weights_bn{}'.format(bn)"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "msy3wuAAHVLn"
      },
      "source": [
        "def get_data():\n",
        "  \"\"\" \n",
        "  load data from data_dir\n",
        "  data is strored in args.raw_data\n",
        "  \"\"\"\n",
        "  path_data = args.data_dir\n",
        "\n",
        "  if hasattr(args, 'raw_data') and hasattr(args, 'last_data_type') and args.data_type == args.last_data_type:\n",
        "\n",
        "    print('using previously loaded data')\n",
        "\n",
        "  else:\n",
        "\n",
        "    start_time = time.time()\n",
        "    if args.data_type == 'transient':\n",
        "      print('loading transient flow past cylinder data...')\n",
        "      td180 = np.load(osp.join(path_data, 'data180.npy'))\n",
        "      td190 = np.load(osp.join(path_data, 'data190.npy'))\n",
        "      td200 = np.load(osp.join(path_data, 'data200.npy'))\n",
        "      td185 = np.load(osp.join(path_data, 'data185.npy'))\n",
        "      td195 = np.load(osp.join(path_data, 'data195.npy'))\n",
        "      args.raw_data = (td180, td190, td200, td185, td195)\n",
        "      print('data loaded')\n",
        "\n",
        "    elif args.data_type == 'periodic':\n",
        "      args.raw_data = np.load(osp.join(path_data, 'data190+.npy'))\n",
        "      print('data loaded')\n",
        "\n",
        "    elif args.data_type == 'turbulence':\n",
        "      args.raw_data = np.load(osp.join(path_data, 'tb_128_5000.npy'))\n",
        "      print('data loaded')\n",
        "    \n",
        "    elif args.data_type == 'sea_temp':\n",
        "      land_sea = np.load(osp.join(path_data, 'SSTdata1990.npz'))['arr_0']\n",
        "\n",
        "      t = land_sea[0]\n",
        "      n_pixel = len(t)\n",
        "      len_data = len(land_sea[:, 0])\n",
        "      land_pos = np.where(t > 1e+30)[0] \n",
        "\n",
        "      full_idx = np.arange(n_pixel)\n",
        "      args.sea_idx = np.delete(full_idx, land_pos)\n",
        "      args.raw_data = np.delete(land_sea, [land_pos], 1)\n",
        "      print('data loaded')\n",
        "\n",
        "    else:\n",
        "      print('data_type is not recognised')\n",
        "\n",
        "    print(\"--- %s seconds --\" % (time.time() - start_time))  \n",
        "    args.last_data_type = args.data_type\n"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J4F_GOvGvXbO"
      },
      "source": [
        "def mn(**kwargs):\n",
        "  \"\"\" returns data image dimentions \"\"\"\n",
        "\n",
        "  if args.data_type == 'periodic':\n",
        "    m, n = 251, 168\n",
        "  if args.data_type == 'transient':\n",
        "    m, n = 502, 252\n",
        "  if args.data_type == 'turbulence':\n",
        "    m, n = 128, 128\n",
        "  if args.data_type == 'sea_temp':\n",
        "    m, n = 360, 180\n",
        "  \n",
        "  return m,n\n",
        "\n",
        "\n",
        "def sensor_cord_data(sensor_num):\n",
        "  \"\"\" sensor pixel co-ordinates for all data_types\n",
        "  :returns: numpy array of shape (no_sensors, 2) with integer values\n",
        "  \"\"\"\n",
        "  m, n = mn()\n",
        "\n",
        "  if args.data_type == 'periodic':\n",
        "\n",
        "    theata = np.linspace(0, 2*np.pi, 300)\n",
        "    x_cord = np.round(25 * np.cos(theata)) + 0\n",
        "    y_cord = np.round(25 * np.sin(theata)) + n/2\n",
        "    cords_ = np.vstack((x_cord,y_cord)).T\n",
        "    cords_ = np.unique(cords_, axis=0)\n",
        "    idx = cords_[:,0] > 0\n",
        "    cords_ = cords_[idx,:]\n",
        "    np.random.seed(3265)  # changing seed will change the postion of sensors and networks will have to be retrained\n",
        "    idx = np.random.choice(range(cords_.shape[0]), sensor_num, False)\n",
        "    cords = np.int64(cords_[idx,:])\n",
        "\n",
        "    mask = np.zeros((m,n))\n",
        "    mask[cords[:,0], cords[:,1]] = 1\n",
        "    s_idx_in_flatten = np.where(mask.reshape(-1) == 1)\n",
        "    s_idx_in_flatten = np.asarray(s_idx_in_flatten).ravel()\n",
        "\n",
        "  elif args.data_type == 'transient':\n",
        "\n",
        "    theata = np.linspace(0, 2*np.pi, 300)\n",
        "    x_cord = np.round(24 * np.cos(theata)) + 0\n",
        "    y_cord = np.round(24 * np.sin(theata)) + n/2\n",
        "    cords_ = np.vstack((x_cord,y_cord)).T\n",
        "    cords_ = np.unique(cords_, axis=0)\n",
        "    idx = cords_[:,0] > 0\n",
        "    cords_ = cords_[idx,:]\n",
        "    np.random.seed(323)  # changing seed will change the postion of sensors and networks will have to be retrained\n",
        "    idx = np.random.choice(range(cords_.shape[0]), sensor_num, False)\n",
        "    cords = np.int64(cords_[idx,:])\n",
        "\n",
        "    mask = np.zeros((m,n))\n",
        "    mask[cords[:,0], cords[:,1]] = 1\n",
        "    s_idx_in_flatten = np.where(mask.reshape(-1) == 1)\n",
        "    s_idx_in_flatten = np.asarray(s_idx_in_flatten).ravel()\n",
        "\n",
        "  elif args.data_type == 'turbulence':\n",
        "\n",
        "    # x = np.tile(np.arange(0, 32), 32)[:, np.newaxis]\n",
        "    # y = np.repeat(np.arange(0, 32), 32, axis=0)[:, np.newaxis]\n",
        "    # cords = np.concatenate((x, y), axis=1)*4 + 2\n",
        "\n",
        "    # mask = np.zeros((m,n))\n",
        "    # mask[cords[:,0], cords[:,1]] = 1\n",
        "    # s_idx_in_flatten = np.where(mask.reshape(-1) == 1)\n",
        "    # s_idx_in_flatten = np.asarray(s_idx_in_flatten).ravel()\n",
        "    # plt.scatter(cord[:,0], cord[:, 1])\n",
        "    s_idx_in_flatten = cords = None\n",
        "\n",
        "  elif args.data_type == 'sea_temp': \n",
        "\n",
        "    idices = np.arange(0, 44219)\n",
        "    np.random.seed(73)  #323 21 23 changing seed will change the postion of sensors and networks will have to be retrained\n",
        "    s_idx_in_flatten = np.random.choice(idices, sensor_num, False)\n",
        "\n",
        "    flat_sea_mask = np.zeros((44219))\n",
        "    flat_sea_mask[s_idx_in_flatten] = 1\n",
        "    flat_mask = np.zeros((m*n))\n",
        "    for i, j in zip(args.sea_idx, count(0, 1)):\n",
        "      flat_mask[i] = flat_sea_mask[j]\n",
        "\n",
        "    mask = flat_mask.reshape(n, m)\n",
        "    cords = np.where(mask == 1)\n",
        "    cords = np.asarray(cords)\n",
        "\n",
        "    # plt.figure()\n",
        "    # plt.scatter(cords[1], cords[0])\n",
        "    # plt.show()\n",
        "  \n",
        "  return s_idx_in_flatten, cords"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uhHXW5YBfvnn"
      },
      "source": [
        "def image_select(method, i, images, s, plot, save, **kwargs):\n",
        "\n",
        "  seq_len = kwargs.get('seq_len')\n",
        "  if method == 'are':\n",
        "    seq_impct = (kwargs.get('seq_len')-1)//2 \n",
        "    add1 = '_sq{}_i{}'.format(seq_len, i+seq_impct, args.RNN)  \n",
        "  else:\n",
        "    seq_impct = 0\n",
        "    add1 =  ''\n",
        "  add2 = '_SNR{}'.format(kwargs.get('SNR', '')) if args.exp == 'mthd__snr' else ''\n",
        "  name = ('im_{}_{}_{}_s{}'.format(args.data_type, args.exp, method, s) + add1 + add2, \n",
        "          'im_true_{}_{}_{}_s{}'.format(args.data_type, args.exp, method, s) + add1 + add2)\n",
        "        \n",
        "  SNR = kwargs.get('SNR')\n",
        "  if s in kwargs.get('plot_s_list', [None]):\n",
        "    if seq_len in kwargs.get('plot_seq_len_list', [None]):\n",
        "      if SNR in kwargs.get('plot_SNR', [None]):\n",
        "        if i in kwargs.get('plot_image_idx', [None])-seq_impct:\n",
        "          show_save_image(method,\n",
        "                          images,\n",
        "                          plot,\n",
        "                          save, \n",
        "                          name=name,\n",
        "                          i=i,\n",
        "                          **kwargs)\n",
        "\n",
        "\n",
        "def show_save_image(method, images, plot, save, **kwargs):\n",
        "\n",
        "  istupl = True if isinstance(images, tuple) else False\n",
        "\n",
        "  len_tuple = len(images) if istupl else 1\n",
        "  get_value = lambda index, tuple_ : tuple_[index] if istupl else tuple_\n",
        "  i = kwargs.get('i', 0)\n",
        "  stats = kwargs.get('stats', np.array([0]))\n",
        "\n",
        "  if istupl:\n",
        "    assert len_tuple == len(save) == len(plot)\n",
        "    if any(save):\n",
        "      assert len(kwargs.get('name')) == len_tuple, 'mention all names of images for saving'\n",
        "  else:\n",
        "    if save: assert kwargs.get('name') \n",
        "\n",
        "  for idx in range(len_tuple):\n",
        "      \n",
        "    save_ = get_value(idx, save)\n",
        "    plot_ = get_value(idx, plot)\n",
        "    name = get_value(idx, kwargs.get('name', ['im']*len_tuple))\n",
        "\n",
        "    cords = kwargs.get('cords', np.array([0]))\n",
        "\n",
        "    t = get_value(idx, images)\n",
        "    t = t.cpu().data.numpy() if torch.is_tensor(t) else t\n",
        "    if t.ndim == 1:\n",
        "      t = t\n",
        "    elif t.ndim == 2:\n",
        "      t = t[-1]\n",
        "    elif t.ndim == 3:\n",
        "      t = t[-1, -1]\n",
        "    t = t * stats[1, :] + stats[0, :] if stats.any() else t\n",
        "\n",
        "    ny, nx = m, n = mn()\n",
        "    x2 = np.arange(0, ny, 1)\n",
        "    y2 = np.arange(0, nx, 1)\n",
        "    mX, mY = np.meshgrid(x2, y2)\n",
        "\n",
        "    if args.data_type == 'periodic' or args.data_type == 'transient':\n",
        "\n",
        "      t_ = t.reshape((ny, nx))\n",
        "      print('reconstruction',i)\n",
        "      minmax = np.max(np.abs(t_)) * 0.65\n",
        "      plt.figure(facecolor=\"white\",  edgecolor='k', figsize=(7.9,4.7))\n",
        "      im = plt.imshow(t_.T, cmap=cmocean.cm.balance, interpolation='none', vmin=-minmax, vmax=minmax)\n",
        "      if kwargs.get('contour', 1): \n",
        "        try:\n",
        "          plt.contourf(mX, mY, t_.T, 80, cmap=cmocean.cm.balance, alpha=1, vmin=-minmax+3, vmax=minmax-3) \n",
        "        except:\n",
        "          plt.contourf(mX, mY, t_.T, 80, cmap=cmocean.cm.balance, alpha=1, vmin=-minmax, vmax=minmax) \n",
        "      plt.scatter(cords[:,0], cords[:,1], marker='.', color='#ff7f00', s=500, zorder=5) if cords.any() else 0\n",
        "\n",
        "    if args.data_type == 'turbulence':\n",
        "\n",
        "      t_ = t.reshape((ny, nx))\n",
        "      print('reconstruction',i)\n",
        "      plt.figure(facecolor=\"white\",  edgecolor='k')\n",
        "      im = plt.imshow(t_)\n",
        "      \n",
        "    if args.data_type == 'sea_temp':\n",
        "\n",
        "      print('reconstruction',i)\n",
        "      recon = np.ones((m*n))*35\n",
        "      for i, j in zip(args.sea_idx, count(0, 1)):\n",
        "        recon[i] = t[j]\n",
        "\n",
        "      recon_im = recon.reshape(n, m)\n",
        "\n",
        "      plt.figure()\n",
        "      plt.imshow(recon_im, cmap='terrain')\n",
        "      plt.gca().invert_yaxis()\n",
        "      plt.scatter(cords[1], cords[0], marker='.', color='b', s=20, zorder=5) if cords.any() else 0\n",
        "      plt.axis('off')\n",
        "\n",
        "    if save_:\n",
        "      path = kwargs.get('path') if kwargs.get('path') else get_path(method, 'I') \n",
        "      plt.savefig(osp.join(path, '{}.pdf'.format(name)), format='pdf')\n",
        "      print('SAVED ==', osp.join(path, '{}.pdf'.format(name)))\n",
        "    \n",
        "    plt.show() if plot_ else 0\n",
        "        "
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CJIJ5VhCA1D5"
      },
      "source": [
        "def plot_graph(rerror_train, rerror_test, save, name):\n",
        "    fig = plt.figure()\n",
        "    plt.plot(rerror_train, lw=2, label='Trainings error', color='#377eb8',)  \n",
        "    plt.plot(rerror_test, lw=2, label='Valid error', color='#e41a1c',)            \n",
        "    plt.tick_params(axis='x', labelsize=14) \n",
        "    plt.tick_params(axis='y', labelsize=14) \n",
        "    plt.locator_params(axis='y', nbins=10)\n",
        "    plt.locator_params(axis='x', nbins=10)\n",
        "    \n",
        "    plt.ylabel('Error', fontsize=14)\n",
        "    plt.xlabel('Epochs', fontsize=14)\n",
        "    plt.grid(False)\n",
        "    plt.yscale(\"log\")\n",
        "    plt.legend(fontsize=14)\n",
        "    fig.tight_layout()\n",
        "\n",
        "    if save:\n",
        "      plt.savefig(name, format='png')\n"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xPUL6ZhqGIN0"
      },
      "source": [
        "from numpy import sum,isrealobj,sqrt\n",
        "from numpy.random import standard_normal\n",
        "\n",
        "def awgn(s,SNRdB,L=1):\n",
        "    \"\"\"\n",
        "    AWGN channel\n",
        "    Add AWGN noise to input signal. The function adds AWGN noise vector to signal\n",
        "    's' to generate a resulting signal vector 'r' of specified SNR in dB. It also\n",
        "    returns the noise vector 'n' that is added to the signal 's' and the power \n",
        "    spectral density N0 of noise added\n",
        "    Parameters:\n",
        "        s : input/transmitted signal vector\n",
        "        SNRdB : desired signal to noise ratio (expressed in dB) for the received signal\n",
        "        L : oversampling factor (applicable for waveform simulation) default L = 1.\n",
        "    Returns:\n",
        "        r : received signal vector (r=s+n)\n",
        "\"\"\"\n",
        "    gamma = 10**(SNRdB/10) #SNR to linear scale\n",
        "    if s.ndim==1:# if s is single dimensional vector\n",
        "        P=L*sum(abs(s)**2)/len(s) #Actual power in the vector\n",
        "    else: # multi-dimensional signals like MFSK\n",
        "        P=L*sum(sum(abs(s)**2))/len(s) # if s is a matrix [MxN]\n",
        "    N0=P/gamma # Find the noise spectral density\n",
        "    if isrealobj(s):# check if input is real/complex object type\n",
        "        n = sqrt(N0/2)*standard_normal(s.shape) # computed noise\n",
        "    else:\n",
        "        n = sqrt(N0/2)*(standard_normal(s.shape)+1j*standard_normal(s.shape))\n",
        "    r = s + n # received signal\n",
        "    return r\n",
        "\n",
        "# td_n2 = np.zeros((300,42168))\n",
        "# # td_n2 = awgn(td,10)\n",
        "# for i in range(len(td)):\n",
        "#   td_n2[i,:] = awgn(td[i],1)"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rUHtHdRLA8lq"
      },
      "source": [
        "#**NETWORKS**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aHl37tiKeYau"
      },
      "source": [
        "## Autoencoder Net"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Hsb3jPPkfnEq"
      },
      "source": [
        "class autoencoder(nn.Module):\n",
        "    def __init__(self,bottle_neck, **kwargs):\n",
        "        super(autoencoder, self).__init__()\n",
        "        m, n = mn()\n",
        "        k = m*n\n",
        "        drop = kwargs.get('drop')\n",
        "\n",
        "        if args.data_type == 'transient':\n",
        "\n",
        "            if drop:\n",
        "              print('autoencoder oprating with dropout:', drop)\n",
        "              self.encoder = nn.Sequential(\n",
        "                  # nn.Dropout(p =0.5),\n",
        "                  nn.Linear(k, 2000),\n",
        "                  nn.BatchNorm1d(num_features = 2000),\n",
        "                  nn.ReLU(True),\n",
        "                  # nn.Dropout(p =0.5),\n",
        "                  nn.Linear(2000, 300),\n",
        "                  nn.BatchNorm1d(num_features =300),\n",
        "                  nn.ReLU(True),\n",
        "                  nn.Dropout(p=drop),\n",
        "                  nn.Linear(300, bottle_neck))\n",
        "              self.decoder = nn.Sequential(\n",
        "                  nn.Linear(bottle_neck, 300),\n",
        "                  nn.ReLU(True),\n",
        "                  nn.Linear(300, 2000),       \n",
        "                  nn.ReLU(True),\n",
        "                  nn.Linear(2000, k)) \n",
        "\n",
        "            else:\n",
        "              print('autoencoder oprating without dropout layer')\n",
        "              self.encoder = nn.Sequential(\n",
        "                  nn.Linear(k, 2000),\n",
        "                  nn.ReLU(True),\n",
        "                  nn.Linear(2000, 300), \n",
        "                  nn.ReLU(True),\n",
        "                  nn.Linear(300, bottle_neck)) \n",
        "              self.decoder = nn.Sequential(\n",
        "                  nn.Linear(bottle_neck, 300),\n",
        "                  nn.ReLU(True),\n",
        "                  nn.Linear(300, 2000),\n",
        "                  nn.ReLU(True),\n",
        "                  nn.Linear(2000, k)) \n",
        "\n",
        "        elif args.data_type == 'periodic':\n",
        "\n",
        "            if drop:\n",
        "              print('autoencoder oprating with dropout:', drop)\n",
        "              self.encoder = nn.Sequential(\n",
        "                  nn.Linear(k, 1024),\n",
        "                  nn.BatchNorm1d(num_features = 1024),\n",
        "                  nn.ReLU(True),\n",
        "                  nn.Linear(1024, 256),\n",
        "                  nn.BatchNorm1d(num_features =256), \n",
        "                  nn.ReLU(True),\n",
        "                  nn.Dropout(p=drop),\n",
        "                  nn.Linear(256, bottle_neck)) \n",
        "              self.decoder = nn.Sequential(\n",
        "                  nn.Linear(bottle_neck, 256),\n",
        "                  nn.ReLU(True),\n",
        "                  nn.Linear(256, 1024),          \n",
        "                  nn.ReLU(True),\n",
        "                  nn.Linear(1024, k)) \n",
        "            else:\n",
        "              print('autoencoder oprating without dropout layer')\n",
        "              self.encoder = nn.Sequential(\n",
        "                  nn.Linear(k, 1024),\n",
        "                  nn.ReLU(True),\n",
        "                  nn.Linear(1024, 256),\n",
        "                  nn.ReLU(True),\n",
        "                  nn.Linear(256, bottle_neck)) \n",
        "              self.decoder = nn.Sequential(\n",
        "                  nn.Linear(bottle_neck, 256),\n",
        "                  nn.ReLU(True),\n",
        "                  nn.Linear(256, 1024),\n",
        "                  nn.ReLU(True),\n",
        "                  nn.Linear(1024, k)) \n",
        "              \n",
        "        elif args.data_type == 'turbulence':\n",
        "\n",
        "            if drop:\n",
        "              print('autoencoder oprating with dropout:', drop)\n",
        "              self.encoder = nn.Sequential(\n",
        "                  nn.Linear(k, 512),\n",
        "                  nn.BatchNorm1d(num_features = 512),\n",
        "                  nn.ReLU(True),\n",
        "                  nn.Linear(512, 256),\n",
        "                  nn.BatchNorm1d(num_features =256), \n",
        "                  nn.ReLU(True),\n",
        "                  nn.Dropout(p=drop),\n",
        "                  nn.Linear(256, bottle_neck)) \n",
        "              self.decoder = nn.Sequential(\n",
        "                  nn.Linear(bottle_neck, 256),\n",
        "                  nn.ReLU(True),\n",
        "                  nn.Linear(256, 512),          \n",
        "                  nn.ReLU(True),\n",
        "                  nn.Linear(512, k)) \n",
        "            else:\n",
        "              print('autoencoder oprating without dropout layer')\n",
        "              self.encoder = nn.Sequential(\n",
        "                  nn.Linear(k, 512),\n",
        "                  nn.BatchNorm1d(num_features=512),\n",
        "                  nn.ReLU(True),\n",
        "                  nn.Linear(512, 256),\n",
        "                  nn.BatchNorm1d(num_features=256),\n",
        "                  nn.ReLU(True),\n",
        "                  nn.Linear(256, bottle_neck)) \n",
        "              self.decoder = nn.Sequential(\n",
        "                  nn.Linear(bottle_neck, 256),\n",
        "                  nn.BatchNorm1d(num_features=256),\n",
        "                  nn.ReLU(True),\n",
        "                  nn.Linear(256, 512),\n",
        "                  nn.BatchNorm1d(num_features=512),\n",
        "                  nn.ReLU(True),\n",
        "                  nn.Linear(512, k)) \n",
        "              \n",
        "        elif args.data_type == 'sea_temp':\n",
        "            k = 44219\n",
        "\n",
        "            if drop:\n",
        "              print('autoencoder oprating with dropout:', drop)\n",
        "              self.encoder = nn.Sequential(\n",
        "                  nn.Linear(k, 512),\n",
        "                  nn.BatchNorm1d(num_features=512),\n",
        "                  nn.ReLU(True),\n",
        "                  nn.Linear(512, 256),\n",
        "                  nn.BatchNorm1d(num_features=256), \n",
        "                  nn.ReLU(True),\n",
        "                  nn.Dropout(p=drop),\n",
        "                  nn.Linear(256, bottle_neck)) \n",
        "              self.decoder = nn.Sequential(\n",
        "                  nn.Linear(bottle_neck, 256),\n",
        "                  nn.ReLU(True),\n",
        "                  nn.Linear(256, 512),          \n",
        "                  nn.ReLU(True),\n",
        "                  nn.Linear(512, k)) \n",
        "            else:\n",
        "              print('autoencoder oprating without dropout layer')\n",
        "              self.encoder = nn.Sequential(\n",
        "                  nn.Linear(k, 512),\n",
        "                  # nn.BatchNorm1d(num_features = 512),\n",
        "                  nn.ReLU(True),\n",
        "                  nn.Linear(512, 256),\n",
        "                  # nn.BatchNorm1d(num_features =256), \n",
        "                  nn.ReLU(True),\n",
        "                  nn.Linear(256, bottle_neck)) \n",
        "              self.decoder = nn.Sequential(\n",
        "                  nn.Linear(bottle_neck, 256),\n",
        "                  # nn.BatchNorm1d(num_features =256),\n",
        "                  nn.ReLU(True),\n",
        "                  nn.Linear(256, 512),\n",
        "                  # nn.BatchNorm1d(num_features =512),\n",
        "                  nn.ReLU(True),\n",
        "                  nn.Linear(512, k)) \n",
        "\n",
        "    def forward(self, x):\n",
        "      # pdb.set_trace()\n",
        "      try :\n",
        "        w = self.decoder(x)\n",
        "        return w\n",
        "      except :\n",
        "        y = self.encoder(x)\n",
        "        z = self.decoder(y)\n",
        "        return z,y\n",
        "        "
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2rx07OS-faK9"
      },
      "source": [
        "## PDS net"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vzvoU2sZf3aL"
      },
      "source": [
        "class PDSnetwork(nn.Module):\n",
        "\n",
        "    def __init__(self, sensor_number, bottle_neck):\n",
        "        super(PDSnetwork, self).__init__()\n",
        "\n",
        "        if args.data_type == 'transient':\n",
        "          H = 200\n",
        "        if args.data_type == 'periodic':\n",
        "          H = 50\n",
        "        if args.data_type == 'turbulence':\n",
        "          H = 150\n",
        "        if args.data_type == 'sea_temp':\n",
        "          H = 100\n",
        "\n",
        "        self.l1 = nn.Linear(sensor_number, H)\n",
        "        self.l2 = nn.Linear(H, H)\n",
        "        self.output = nn.Linear(H, bottle_neck)\n",
        "\n",
        "    def forward(self, leading_dna):\n",
        "        l1_out = F.relu(self.l1(leading_dna))\n",
        "        l2_out = F.relu(self.l2(l1_out))\n",
        "        output = self.output(l2_out)        \n",
        "        return output"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AcHm5KadgWzG"
      },
      "source": [
        "## RNN Net"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Zj7-LTsIjNxO"
      },
      "source": [
        "if args.data_type == 'transient':   \n",
        "  class are_rnn_net(nn.Module):\n",
        "\n",
        "      def __init__(self, hidden_size,sensor_number, bottle_neck, layers, seq_len,**kwargs):\n",
        "          super(are_rnn_net, self).__init__()\n",
        "          self.hidden_size = hidden_size\n",
        "          # self.AREdrop = kwargs.get('AREdrop',None)\n",
        "          print('args.data_type == transient', '\\n', 'using rnn cell')\n",
        "\n",
        "          if args.operation_mode == 'online':      \n",
        "            self.rl = nn.LSTM(sensor_number , self.hidden_size, num_layers=layers)#, nonlinearity = 'relu')#)\n",
        "            self.l1 = nn.Linear(hidden_size, 200, bias=True)\n",
        "\n",
        "          elif args.operation_mode == 'offline':\n",
        "            self.rl = nn.LSTM(sensor_number , self.hidden_size, num_layers=layers)#, nonlinearity = 'relu')#)\n",
        "            self.rlr = nn.LSTM(sensor_number, self.hidden_size, num_layers=layers)#, nonlinearity = 'relu')#)\n",
        "            self.l1 = nn.Linear(hidden_size*2, 200, bias=True)\n",
        "\n",
        "          \n",
        "          self.l2 = nn.Linear(200, 200, bias=True)\n",
        "          self.l3 = nn.Linear(200, bottle_neck, bias=True)\n",
        "          \n",
        "          # self.bn_25 = nn.BatchNorm1d(num_features=25)\n",
        "          #self.reset_hidden_states()\n",
        "\n",
        "\n",
        "      def reset_hidden_states(self, for_batch=None):\n",
        "          if for_batch is not None:\n",
        "              batch_size = for_batch.shape[1]\n",
        "\n",
        "          # Initialize recurrent hidden states\n",
        "          if args.operation_mode == 'online':    \n",
        "            self.rl_h = self.init_hidden(batch_size=batch_size)\n",
        "            self.rl_c = self.init_hidden(batch_size=batch_size)\n",
        "\n",
        "          if args.operation_mode == 'offline':\n",
        "            self.rl_h = self.init_hidden(batch_size=batch_size)\n",
        "            self.rlr_h = self.init_hidden(batch_size=batch_size)\n",
        "            self.rl_c = self.init_hidden(batch_size=batch_size)\n",
        "            self.rlr_c = self.init_hidden(batch_size=batch_size)\n",
        "\n",
        "          if for_batch is not None:\n",
        "              device = for_batch.device\n",
        "              if args.operation_mode == 'online':    \n",
        "                self.rl_h = self.rl_h.to(device)\n",
        "                self.rl_c = self.rl_c.to(device)\n",
        "                \n",
        "              if args.operation_mode == 'offline':\n",
        "                self.rl_h = self.rl_h.to(device)\n",
        "                self.rlr_h = self.rlr_h.to(device)\n",
        "                self.rl_c = self.rl_c.to(device)\n",
        "                self.rlr_c = self.rlr_c.to(device)\n",
        "\n",
        "\n",
        "      def init_hidden(self, batch_size):\n",
        "          layers =1\n",
        "          return Variable(torch.zeros(layers, batch_size, self.hidden_size))\n",
        "\n",
        "\n",
        "      def forward(self, leading_dna):\n",
        "          self.reset_hidden_states(for_batch=leading_dna)\n",
        "          \n",
        "          batch = len(leading_dna[0,:,0])\n",
        "          #print(batch)\n",
        "          sensor_number = len(leading_dna[0,0,:])\n",
        "          #print(sensor_number)\n",
        "          seq_len = len(leading_dna[:,0,0])\n",
        "          #print(seq_len)\n",
        "          seq_impct = int((seq_len-1)/2)\n",
        "\n",
        "          if args.operation_mode == 'online':    \n",
        "            split_data1 = torch.zeros(seq_impct+1,batch,sensor_number)\n",
        "            split_data1 = leading_dna[0:seq_impct+1,:,:] \n",
        "\n",
        "            rl_out, self.rl_h= self.rl(split_data1, (self.rl_h, self.rl_c))\n",
        "            r_out = rl_out[seq_impct]  \n",
        "\n",
        "\n",
        "          if args.operation_mode == 'offline':\n",
        "            split_data1 = torch.zeros(seq_impct+1,batch,sensor_number)\n",
        "            split_data2 = torch.zeros(seq_impct+1,batch,sensor_number)\n",
        "            split_data1 = leading_dna[0:seq_impct+1,:,:] #(seq_impct+1,batch,sensor_number)\n",
        "            split_data2 = leading_dna[seq_impct:seq_len,:,:]\n",
        "\n",
        "            rl_out, self.rl_h= self.rl(split_data1, (self.rl_h, self.rl_c))\n",
        "            rlr_out, self.rlr_h = self.rlr(self.flip_input(split_data2), (self.rlr_h, self.rlr_c))\n",
        "            r_out = torch.cat((rl_out[seq_impct], rlr_out[seq_impct]), 1)\n",
        "            \n",
        "          l1_out = F.relu(self.l1(r_out))\n",
        "          l2_out = F.relu(self.l2(l1_out))\n",
        "          l3_out = self.l3(l2_out)\n",
        "          return l3_out\n",
        "\n",
        "\n",
        "      def flip_input(self, input):\n",
        "          device = input.device\n",
        "          flipped_array = np.flip(input.data.cpu().numpy(), 0).copy()\n",
        "          return Variable(torch.from_numpy(flipped_array).to(device))\n",
        "\n",
        "if args.data_type == 'periodic':     \n",
        "  class RNNv3network(nn.Module):\n",
        "\n",
        "      def __init__(self, hidden_size,sensor_number, bottle_neck, layers, seq_len,**kwargs):\n",
        "          super(RNNv3network, self).__init__()\n",
        "          self.hidden_size = hidden_size\n",
        "          # self.AREdrop = kwargs.get('AREdrop',None)\n",
        "          print('args.data_type == periodic', '\\n', 'using rnn cell')\n",
        "\n",
        "          if args.operation_mode == 'online':      \n",
        "            self.rl = nn.LSTM(sensor_number , self.hidden_size, num_layers=layers)#, nonlinearity = 'relu')#)\n",
        "            self.l1 = nn.Linear(hidden_size, 50, bias=True)\n",
        "\n",
        "\n",
        "          if args.operation_mode == 'offline':\n",
        "            self.rl = nn.LSTM(sensor_number , self.hidden_size, num_layers=layers)#, nonlinearity = 'relu')#)\n",
        "            self.rlr = nn.LSTM(sensor_number, self.hidden_size, num_layers=layers)#, nonlinearity = 'relu')#)\n",
        "            self.l1 = nn.Linear(hidden_size*2, 50, bias=True)\n",
        "\n",
        "          \n",
        "          self.l2 = nn.Linear(50, 50, bias=True)\n",
        "          self.l3 = nn.Linear(50, bottle_neck, bias=True)\n",
        "          \n",
        "          # self.bn_25 = nn.BatchNorm1d(num_features=25)\n",
        "          #self.reset_hidden_states()\n",
        "\n",
        "\n",
        "      def reset_hidden_states(self, for_batch=None):\n",
        "          if for_batch is not None:\n",
        "              batch_size = for_batch.shape[1]\n",
        "\n",
        "          # Initialize recurrent hidden states\n",
        "          if args.operation_mode == 'online':    \n",
        "            self.rl_h = self.init_hidden(batch_size=batch_size)\n",
        "            self.rl_c = self.init_hidden(batch_size=batch_size)\n",
        "\n",
        "          if args.operation_mode == 'offline':\n",
        "            self.rl_h = self.init_hidden(batch_size=batch_size)\n",
        "            self.rlr_h = self.init_hidden(batch_size=batch_size)\n",
        "            self.rl_c = self.init_hidden(batch_size=batch_size)\n",
        "            self.rlr_c = self.init_hidden(batch_size=batch_size)\n",
        "\n",
        "          if for_batch is not None:\n",
        "              device = for_batch.device\n",
        "              if args.operation_mode == 'online':    \n",
        "                self.rl_h = self.rl_h.to(device)\n",
        "                self.rl_c = self.rl_c.to(device)\n",
        "                \n",
        "              if args.operation_mode == 'offline':\n",
        "                self.rl_h = self.rl_h.to(device)\n",
        "                self.rlr_h = self.rlr_h.to(device)\n",
        "                self.rl_c = self.rl_c.to(device)\n",
        "                self.rlr_c = self.rlr_c.to(device)\n",
        "\n",
        "\n",
        "      def init_hidden(self, batch_size):\n",
        "          layers =1\n",
        "          return Variable(torch.zeros(layers, batch_size, self.hidden_size))\n",
        "\n",
        "\n",
        "      def forward(self, leading_dna):\n",
        "          self.reset_hidden_states(for_batch=leading_dna)\n",
        "          \n",
        "          batch = len(leading_dna[0,:,0])\n",
        "          #print(batch)\n",
        "          sensor_number = len(leading_dna[0,0,:])\n",
        "          #print(sensor_number)\n",
        "          seq_len = len(leading_dna[:,0,0])\n",
        "          #print(seq_len)\n",
        "          seq_impct = int((seq_len-1)/2)\n",
        "\n",
        "          if args.operation_mode == 'online':    \n",
        "            split_data1 = torch.zeros(seq_impct+1,batch,sensor_number)\n",
        "            split_data1 = leading_dna[0:seq_impct+1,:,:] \n",
        "\n",
        "            rl_out, self.rl_h= self.rl(split_data1, (self.rl_h, self.rl_c))\n",
        "            r_out = rl_out[seq_impct]  \n",
        "\n",
        "\n",
        "          if args.operation_mode == 'offline':\n",
        "            split_data1 = torch.zeros(seq_impct+1,batch,sensor_number)\n",
        "            split_data2 = torch.zeros(seq_impct+1,batch,sensor_number)\n",
        "            split_data1 = leading_dna[0:seq_impct+1,:,:] #(seq_impct+1,batch,sensor_number)\n",
        "            split_data2 = leading_dna[seq_impct:seq_len,:,:]\n",
        "\n",
        "            rl_out, self.rl_h= self.rl(split_data1, (self.rl_h, self.rl_c))\n",
        "            rlr_out, self.rlr_h = self.rlr(self.flip_input(split_data2), (self.rlr_h, self.rlr_c))\n",
        "            r_out = torch.cat((rl_out[seq_impct], rlr_out[seq_impct]), 1)\n",
        "            \n",
        "          l1_out = F.relu(self.l1(r_out))\n",
        "          l2_out = F.relu(self.l2(l1_out))\n",
        "          l3_out = self.l3(l2_out)\n",
        "          return l3_out\n",
        "\n",
        "\n",
        "      def flip_input(self, input):\n",
        "          device = input.device\n",
        "          flipped_array = np.flip(input.data.cpu().numpy(), 0).copy()\n",
        "          return Variable(torch.from_numpy(flipped_array).to(device))\n",
        "                "
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i01G721Agfi0"
      },
      "source": [
        "## LSTM Net "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8ub64Yw2hPA8"
      },
      "source": [
        " \n",
        "class are_lstm_net(nn.Module):\n",
        "\n",
        "    def __init__(self, hidden_size,sensor_number, bottle_neck, layers, seq_len):\n",
        "        super(are_lstm_net, self).__init__()\n",
        "        self.hidden_size = hidden_size\n",
        "        print('data_type:', args.data_type, '\\n', 'using lstm cell')\n",
        "\n",
        "        if args.data_type == 'transient': \n",
        "\n",
        "          self.rl = nn.LSTM(sensor_number , self.hidden_size, num_layers=layers)\n",
        "          H = 200\n",
        "\n",
        "          if args.operation_mode == 'online':      \n",
        "            self.l1 = nn.Linear(hidden_size, H)\n",
        "            self.l2 = nn.Linear(H, H)\n",
        "            self.l3 = nn.Linear(H, bottle_neck)\n",
        "\n",
        "          if args.operation_mode == 'offline':\n",
        "            self.rlr = nn.LSTM(sensor_number, self.hidden_size, num_layers=layers)\n",
        "            self.l1 = nn.Linear(hidden_size*2, H)\n",
        "            self.l2 = nn.Linear(H, H)\n",
        "            self.l3 = nn.Linear(H, bottle_neck)\n",
        "\n",
        "        if args.data_type == 'periodic': \n",
        "\n",
        "          self.rl = nn.LSTM(sensor_number , self.hidden_size, num_layers=layers)\n",
        "          H = 50\n",
        "          \n",
        "          if args.operation_mode == 'online':      \n",
        "            self.l1 = nn.Linear(hidden_size, H)            \n",
        "            self.l2 = nn.Linear(H, H)\n",
        "            self.l3 = nn.Linear(H, bottle_neck)\n",
        "\n",
        "          if args.operation_mode == 'offline':\n",
        "            self.rlr = nn.LSTM(sensor_number, self.hidden_size, num_layers=layers)\n",
        "            self.l1 = nn.Linear(hidden_size*2, H)\n",
        "            self.l2 = nn.Linear(H, H)\n",
        "            self.l3 = nn.Linear(H, bottle_neck)\n",
        "\n",
        "        if args.data_type == 'turbulence': \n",
        "\n",
        "          self.rl = nn.LSTM(sensor_number , self.hidden_size, num_layers=layers)\n",
        "          H = 150\n",
        "\n",
        "          if args.operation_mode == 'online':      \n",
        "            self.l1 = nn.Linear(hidden_size, H)            \n",
        "            self.l2 = nn.Linear(H, H)\n",
        "            self.l3 = nn.Linear(H, bottle_neck)\n",
        "\n",
        "          if args.operation_mode == 'offline':\n",
        "            self.rlr = nn.LSTM(sensor_number, self.hidden_size, num_layers=layers)\n",
        "            self.l1 = nn.Linear(hidden_size*2, H)\n",
        "            self.l2 = nn.Linear(H, H)\n",
        "            self.l3 = nn.Linear(H, bottle_neck)\n",
        "\n",
        "        if args.data_type == 'sea_temp': \n",
        "\n",
        "          self.rl = nn.LSTM(sensor_number , self.hidden_size, num_layers=layers)\n",
        "          H = 100\n",
        "\n",
        "          if args.operation_mode == 'online':      \n",
        "            self.l1 = nn.Linear(hidden_size, H)\n",
        "            self.l2 = nn.Linear(H, H)\n",
        "            self.l3 = nn.Linear(H, bottle_neck)\n",
        "\n",
        "          if args.operation_mode == 'offline':\n",
        "            self.rlr = nn.LSTM(sensor_number, self.hidden_size, num_layers=layers)\n",
        "            self.l1 = nn.Linear(hidden_size*2, H)\n",
        "            self.l2 = nn.Linear(H, H)\n",
        "            self.l3 = nn.Linear(H, bottle_neck)\n",
        "\n",
        "    def reset_hidden_states(self, for_batch=None):\n",
        "        if for_batch is not None:\n",
        "            batch_size = for_batch.shape[1]\n",
        "\n",
        "        # Initialize recurrent hidden states\n",
        "        if args.operation_mode == 'online':      \n",
        "          self.rl_h = self.init_hidden(batch_size=batch_size)\n",
        "          self.rl_c = self.init_hidden(batch_size=batch_size)\n",
        "\n",
        "        if args.operation_mode == 'offline':\n",
        "          self.rl_h = self.init_hidden(batch_size=batch_size)\n",
        "          self.rlr_h = self.init_hidden(batch_size=batch_size)\n",
        "          self.rl_c = self.init_hidden(batch_size=batch_size)\n",
        "          self.rlr_c = self.init_hidden(batch_size=batch_size)\n",
        "\n",
        "        if for_batch is not None:\n",
        "            device = for_batch.device\n",
        "\n",
        "            if args.operation_mode == 'online':      \n",
        "              self.rl_h = self.rl_h.to(device)\n",
        "              self.rl_c = self.rl_c.to(device)\n",
        "\n",
        "            if args.operation_mode == 'offline':\n",
        "                self.rl_h = self.rl_h.to(device)\n",
        "                self.rlr_h = self.rlr_h.to(device)\n",
        "                self.rl_c = self.rl_c.to(device)\n",
        "                self.rlr_c = self.rlr_c.to(device)\n",
        "\n",
        "\n",
        "    def init_hidden(self, batch_size):\n",
        "        layers =1\n",
        "        return Variable(torch.zeros(layers, batch_size, self.hidden_size))\n",
        "\n",
        "    def forward(self, leading_dna):\n",
        "        self.reset_hidden_states(for_batch=leading_dna)\n",
        "        \n",
        "        batch = len(leading_dna[0,:,0])\n",
        "        sensor_number = len(leading_dna[0,0,:])\n",
        "        seq_len = len(leading_dna[:,0,0])\n",
        "        seq_impct = int((seq_len-1)/2)\n",
        "\n",
        "        if args.operation_mode == 'online':      \n",
        "          split_data1 = torch.zeros(seq_impct+1,batch,sensor_number)\n",
        "          split_data1 = leading_dna[0:seq_impct+1,:,:] \n",
        "\n",
        "          rl_out, self.rl_h= self.rl(split_data1, (self.rl_h, self.rl_c))\n",
        "          r_out = rl_out[seq_impct]\n",
        "\n",
        "        if args.operation_mode == 'offline':\n",
        "          split_data1 = torch.zeros(seq_impct+1,batch,sensor_number)\n",
        "          split_data2 = torch.zeros(seq_impct+1,batch,sensor_number)\n",
        "          split_data1 = leading_dna[0:seq_impct+1,:,:] #(seq_impct+1,batch,sensor_number)\n",
        "          split_data2 = leading_dna[seq_impct:seq_len,:,:]\n",
        "\n",
        "          rl_out, self.rl_h= self.rl(split_data1, (self.rl_h, self.rl_c))\n",
        "          rlr_out, self.rlr_h = self.rlr(self.flip_input(split_data2), (self.rlr_h, self.rlr_c))\n",
        "          r_out = torch.cat((rl_out[seq_impct], rlr_out[seq_impct]), 1)\n",
        "\n",
        "        l1_out = F.relu(self.l1(r_out))\n",
        "        l2_out = F.relu(self.l2(l1_out))\n",
        "        out = l3_out = self.l3(l2_out)\n",
        "\n",
        "        # if args.data_type == 'transient': \n",
        "        #   out = l3_out = self.l3(l2_out)\n",
        "\n",
        "        # if args.data_type == 'periodic': \n",
        "        #   out = l3_out = self.l3(l2_out)\n",
        "\n",
        "        return out\n",
        "\n",
        "\n",
        "    def flip_input(self, input):\n",
        "        device = input.device\n",
        "        flipped_array = np.flip(input.data.cpu().numpy(), 0).copy()\n",
        "        return Variable(torch.from_numpy(flipped_array).to(device))\n"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3JRIlWyFjSXl"
      },
      "source": [
        "## Shallow Decoder Net"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CCmAB_Xws9Mz"
      },
      "source": [
        "class shallow_decoder_net(nn.Module):\n",
        "    def __init__(self, n_sensors, **kwargs):\n",
        "        super(shallow_decoder_net, self).__init__()\n",
        "        \n",
        "        m, n = mn()\n",
        "        self.n_sensors = n_sensors\n",
        "        self.outputlayer_size = m*n\n",
        "        self.drop = kwargs.get('drop')\n",
        "\n",
        "        if self.drop:\n",
        "            print('shallow_decoder oprating with drop')\n",
        "        else:\n",
        "            print('shallow_decoder oprating without drop')\n",
        "\n",
        "        if args.data_type == 'periodic':\n",
        "          N1, N2 = 35, 40\n",
        "        if args.data_type == 'transient':\n",
        "          N1, N2 = 350, 400\n",
        "        if args.data_type == 'turbulence':\n",
        "          N1, N2 = 350, 400\n",
        "        if args.data_type == 'sea_temp':\n",
        "          N1, N2 = 350, 400\n",
        "          self.outputlayer_size = 44219\n",
        "        \n",
        "        self.learn_features = nn.Sequential(         \n",
        "            nn.Linear(n_sensors, N1),\n",
        "            nn.ReLU(True), \n",
        "            nn.BatchNorm1d(1),  \n",
        "            )        \n",
        "        \n",
        "        self.learn_coef = nn.Sequential(            \n",
        "            nn.Linear(N1, N2),\n",
        "            nn.ReLU(True),  \n",
        "            nn.BatchNorm1d(1),  \n",
        "            )\n",
        "\n",
        "        self.learn_dictionary = nn.Sequential(\n",
        "            nn.Linear(N2, self.outputlayer_size),\n",
        "            )\n",
        "        \n",
        "\n",
        "        for m in self.modules():\n",
        "            # torch.manual_seed(args.seed)\n",
        "            if isinstance(m, nn.Conv2d):\n",
        "                nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')\n",
        "            \n",
        "            elif isinstance(m, nn.BatchNorm2d):\n",
        "                nn.init.constant_(m.weight, 1)\n",
        "                nn.init.constant_(m.bias, 0)        \n",
        "\n",
        "            elif isinstance(m, nn.Linear):\n",
        "                nn.init.xavier_normal(m.weight)\n",
        "                if m.bias is not None:\n",
        "                    nn.init.constant(m.bias, 0.0)    \n",
        "\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.learn_features(x)\n",
        "        if self.drop:\n",
        "            # torch.manual_seed(args.seed)\n",
        "            x = nn.functional.dropout(x, p=0.1, training=self.training)   \n",
        "        x = self.learn_coef(x)\n",
        "        x = self.learn_dictionary(x) \n",
        "        return x"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qcx78Uurf7Li"
      },
      "source": [
        "## Other RNN Net variants \"Not used\""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PIiAnjPTA8ls"
      },
      "source": [
        "\"\"\" Not used \"\"\"\n",
        "\n",
        "class RNNvnetwork(nn.Module):\n",
        "\n",
        "    DROPOUT = 0.5\n",
        "\n",
        "    def __init__(self, hidden_size,sensor_number, bottle_neck, layers, seq_len):\n",
        "        super(RNNv3network, self).__init__()\n",
        "        self.hidden_size = hidden_size\n",
        "\n",
        "        # Recurrent layers\n",
        "        self.rl = nn.RNN(sensor_number , self.hidden_size, num_layers=layers, nonlinearity = 'relu')\n",
        "        self.rlr = nn.RNN(sensor_number, self.hidden_size, num_layers=layers, nonlinearity = 'relu')\n",
        "\n",
        "        # Final feed-forward layer\n",
        "        self.l1 = nn.Linear(hidden_size*2, 500, bias=True)\n",
        "        self.l2 = nn.Linear(500, 500, bias=True)\n",
        "        self.l3 = nn.Linear(500, bottle_neck, bias=True)\n",
        "\n",
        "    def reset_hidden_states(self, for_batch=None):\n",
        "        if for_batch is not None:\n",
        "            batch_size = for_batch.shape[1]\n",
        "\n",
        "        # Initialize recurrent hidden states\n",
        "        self.rl_h = self.init_hidden(batch_size=batch_size)\n",
        "        self.rlr_h = self.init_hidden(batch_size=batch_size)\n",
        "\n",
        "        if for_batch is not None:\n",
        "            device = for_batch.device\n",
        "            self.rl_h = self.rl_h.to(device)\n",
        "            self.rlr_h = self.rlr_h.to(device)\n",
        "\n",
        "\n",
        "    def init_hidden(self, batch_size):\n",
        "        layers = 1\n",
        "        return Variable(torch.zeros(layers, batch_size, self.hidden_size))\n",
        "\n",
        "\n",
        "    def forward(self, leading_dna):\n",
        "        self.reset_hidden_states(for_batch=leading_dna)\n",
        "        \n",
        "        batch = len(leading_dna[0,:,0])\n",
        "        sensor_number = len(leading_dna[0, 0, :])\n",
        "        seq_len = len(leading_dna[:,0,0])\n",
        "        seq_impct = int((seq_len-1)/2)\n",
        "\n",
        "        split_data1 = torch.zeros(seq_impct+1,batch,sensor_number)\n",
        "        split_data2 = torch.zeros(seq_impct+1,batch,sensor_number)\n",
        "        split_data1 = leading_dna[0:seq_impct+1,:,:]  # (seq_impct+1,batch,sensor_number)\n",
        "        split_data2 = leading_dna[seq_impct:seq_len,:,:]\n",
        "\n",
        "        rl_out, self.rl_h = self.rl(split_data1, self.rl_h)\n",
        "        rlr_out, self.rlr_h = self.rlr(self.flip_input(split_data2), self.rlr_h)\n",
        "        r_out = torch.cat((rl_out[seq_impct], rlr_out[seq_impct]), 1)\n",
        "        l1_out = F.relu(self.l1(r_out))\n",
        "        l2_out = F.relu(self.l2(l1_out))\n",
        "        l3_out = self.l3(l2_out)\n",
        "        return l3_out\n",
        "\n",
        "\n",
        "    def flip_input(self, input):\n",
        "        device = input.device\n",
        "        flipped_array = np.flip(input.data.cpu().numpy(), 0).copy()\n",
        "        return Variable(torch.from_numpy(flipped_array).to(device))\n",
        "\n",
        "\n",
        "class RNNnetwork(nn.Module):\n",
        "\n",
        "    DROPOUT = 0.5\n",
        "\n",
        "    def __init__(self, hidden_size,sensor_number, bottle_neck, layers):\n",
        "        super(RNNnetwork, self).__init__()\n",
        "        self.hidden_size = hidden_size\n",
        "\n",
        "        # Recurrent layers\n",
        "        self.rl = nn.RNN(sensor_number , self.hidden_size, num_layers=layers, nonlinearity = 'relu', dropout=self.DROPOUT)\n",
        "        self.rlr = nn.RNN(sensor_number, self.hidden_size, num_layers=layers, nonlinearity = 'relu', dropout=self.DROPOUT)\n",
        "\n",
        "        # Final feed-forward layer\n",
        "        self.l1 = nn.Linear(hidden_size*2, 200, bias=True)\n",
        "        self.l2 = nn.Linear(200, bottle_neck, bias=True)\n",
        "\n",
        "    def reset_hidden_states(self, for_batch=None):\n",
        "        if for_batch is not None:\n",
        "            batch_size = for_batch.shape[1]\n",
        "\n",
        "        # Initialize recurrent hidden states\n",
        "        self.rl_h = self.init_hidden(batch_size=batch_size)\n",
        "        self.rlr_h = self.init_hidden(batch_size=batch_size)\n",
        "\n",
        "        if for_batch is not None:\n",
        "            device = for_batch.device\n",
        "            self.rl_h = self.rl_h.to(device)\n",
        "            self.rlr_h = self.rlr_h.to(device)\n",
        "\n",
        "\n",
        "    def init_hidden(self, batch_size):\n",
        "        return Variable(torch.zeros(layers, batch_size, self.hidden_size))\n",
        "\n",
        "\n",
        "    def forward(self, leading_dna):\n",
        "        self.reset_hidden_states(for_batch=leading_dna)\n",
        "\n",
        "        rl_out, self.rl_h = self.rl(leading_dna, self.rl_h)\n",
        "        rlr_out, self.rlr_h = self.rlr(self.flip_input(leading_dna), self.rlr_h)\n",
        "        r_out = torch.cat((rl_out[1], rlr_out[1]), 1)\n",
        "        l1_out = F.relu(self.l1(r_out))\n",
        "        l2_out = self.l2(l1_out)\n",
        "        return l2_out\n",
        "\n",
        "\n",
        "    def flip_input(self, input):\n",
        "        device = input.device\n",
        "        flipped_array = np.flip(input.data.cpu().numpy(), 0).copy()\n",
        "        return Variable(torch.from_numpy(flipped_array).to(device))\n",
        "\n",
        "class RNNv2network(nn.Module):\n",
        "    DROPOUT = 0.5\n",
        "\n",
        "    def __init__(self, hidden_size,sensor_number, bottle_neck, layers, seq_len):\n",
        "        super(RNNv2network, self).__init__()\n",
        "        self.hidden_size = hidden_size\n",
        "        \n",
        "        # Recurrent layers\n",
        "        self.rl = nn.RNN(sensor_number, self.hidden_size, num_layers= layers, nonlinearity = 'relu', dropout=self.DROPOUT, bidirectional =True)\n",
        "        \n",
        "        # Final feed-forward layer\n",
        "        self.l1 = nn.Linear(hidden_size*2*seq_len, 200, bias=True)\n",
        "        self.l2 = nn.Linear(200, bottle_neck, bias=True)\n",
        "\n",
        "    def reset_hidden_states(self, for_batch=None):\n",
        "        if for_batch is not None:\n",
        "            batch_size = for_batch.shape[1]\n",
        "\n",
        "        # Initialize recurrent hidden states\n",
        "        self.rl_h = self.init_hidden(batch_size=batch_size)\n",
        "        \n",
        "        if for_batch is not None:\n",
        "            device = for_batch.device\n",
        "            self.rl_h = self.rl_h.to(device)\n",
        "\n",
        "    def init_hidden(self, batch_size):\n",
        "        return Variable(torch.zeros(layers*2, batch_size, self.hidden_size))\n",
        "\n",
        "\n",
        "    def forward(self, leading_dna):\n",
        "        self.reset_hidden_states(for_batch=leading_dna)\n",
        "\n",
        "        rl_out, self.rl_h = self.rl(leading_dna, self.rl_h)\n",
        "        if seq_len == 3:\n",
        "          r_out = torch.cat((rl_out[0], rl_out[1], rl_out[2]), 1)\n",
        "        elif seq_len == 5:\n",
        "          r_out = torch.cat((rl_out[0], rl_out[1], rl_out[2], rl_out[3], rl_out[4]), 1)\n",
        "        elif seq_len == 7:\n",
        "          r_out = torch.cat((rl_out[0], rl_out[1], rl_out[2], rl_out[3], rl_out[4], rl_out[5], rl_out[6]), 1)\n",
        "        l1_out = F.relu(self.l1(r_out))\n",
        "        l2_out = self.l2(l1_out)\n",
        "        return l2_out\n"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w0gojo-TBmgh"
      },
      "source": [
        "#**Pytorch custom DATASET class**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Yhm5b78kBmgt"
      },
      "source": [
        "class sensorgcdatasetAE(Dataset):\n",
        "\n",
        "    def __init__(self, in_file, stats, transform=None):\n",
        "        self.sensor_frame = in_file\n",
        "        self.stats = stats\n",
        "        self.transform = transform\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.sensor_frame)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        sensor = self.sensor_frame[idx, :]\n",
        "        sensor = (sensor - self.stats[0, :]) / self.stats[1, :]\n",
        "        sensor = torch.from_numpy(sensor).float()\n",
        "        return sensor\n",
        "\n",
        "class sensorgcdatasetAA(Dataset):\n",
        "    def __init__(self, in_file, out_file, stats, transform=None):\n",
        "        self.sensor_frame = in_file\n",
        "        self.gc_frame = out_file\n",
        "        self.stats = stats\n",
        "        self.transform = transform\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.sensor_frame)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        sensor = self.sensor_frame[idx, :]\n",
        "        gc = self.gc_frame[idx, :]\n",
        "        gc = (gc - self.stats[0, :])/self.stats[1, :]\n",
        "        sensor = torch.from_numpy(sensor).float()\n",
        "        gc = torch.from_numpy(gc).float()\n",
        "        return sensor, gc\n",
        "\n",
        "class sensorgcdatasetRNN(Dataset):\n",
        "\n",
        "    def __init__(self, in_file, out_file, transform=None):\n",
        "        self.sensor_frame = in_file\n",
        "        self.gc_frame = out_file\n",
        "        self.transform = transform\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.sensor_frame)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        sensor = self.sensor_frame[idx, :]\n",
        "        gc = self.gc_frame[idx, :]\n",
        "        sensor = torch.from_numpy(sensor).float()\n",
        "        gc = torch.from_numpy(gc).float()\n",
        "        return sensor, gc\n",
        "\n",
        "class sensorgcdatasetDS(Dataset):\n",
        "\n",
        "    def __init__(self, in_file, out_file, transform=None):\n",
        "        self.in_frame = in_file\n",
        "        self.out_frame = out_file\n",
        "        self.transform = transform\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.in_frame)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        inn = self.in_frame[idx, :]\n",
        "        out = self.out_frame[idx, :]\n",
        "        inn = torch.from_numpy(inn).float()\n",
        "        out = torch.from_numpy(out).float()\n",
        "        return inn, out"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2zzdQRBFBxO4"
      },
      "source": [
        "# **TRAIN/TEST DATA**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PShg7PEiLssv"
      },
      "source": [
        "def sensor_data(trainAE_stan, validAE_stan, testAE_stan, sensor_num):\n",
        "\n",
        "  n_train, n_valid, n_test = trainAE_stan.shape[0], validAE_stan.shape[0], testAE_stan.shape[0]\n",
        "\n",
        "  if args.data_type != 'turbulence':\n",
        "\n",
        "    s_idx_in_flatten, cords = sensor_cord_data(sensor_num)\n",
        "\n",
        "    strain_in = trainAE_stan[:, s_idx_in_flatten]\n",
        "    svalid_in = validAE_stan[:, s_idx_in_flatten]\n",
        "    stest_in = testAE_stan[:, s_idx_in_flatten]\n",
        "\n",
        "  else:\n",
        "\n",
        "    sqrt_s = int(sensor_num**0.5)\n",
        "    strain_in = (resize(trainAE_stan, (n_train, sqrt_s, sqrt_s))).reshape(n_train, sensor_num)\n",
        "    svalid_in = (resize(validAE_stan, (n_valid, sqrt_s, sqrt_s))).reshape(n_valid, sensor_num)\n",
        "    stest_in = (resize(testAE_stan, (n_test, sqrt_s, sqrt_s))).reshape(n_test, sensor_num)\n",
        "\n",
        "  return strain_in, svalid_in, stest_in\n",
        "\n",
        "\n",
        "def standardize(a, b, c, stats):\n",
        "\n",
        "  a_stan = (a - stats[0, :])/stats[1, :]\n",
        "  b_stan = (b - stats[0, :])/stats[1, :]\n",
        "  c_stan = (c - stats[0, :])/stats[1, :]\n",
        "  \n",
        "  return a_stan, b_stan, c_stan "
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G6ei1SQuALhP"
      },
      "source": [
        "## Autoencoder_train_data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yGgtqqwkAUTi"
      },
      "source": [
        "def autoencoder_train_data(**kwargs):\n",
        "  \n",
        "  noisy_data = SNR = kwargs.get('SNR')\n",
        "  plot, save = kwargs.get('plot'), kwargs.get('save')\n",
        "\n",
        "  if args.data_type == 'periodic': \n",
        "\n",
        "    get_data()\n",
        "    td = args.raw_data  \n",
        "    m, n = mn()\n",
        "    tr_samp = args.tr_samp if hasattr(args, 'tr_samp') else 180\n",
        "\n",
        "    if noisy_data:\n",
        "      print('in autoencoder_train_data-- noise data', 'SNR: ', SNR)\n",
        "      \n",
        "      td_ = np.zeros_like(td)\n",
        "      for i in range(len(td)):\n",
        "        td_[i,:] = awgn(td[i],kwargs.get('SNR',10))\n",
        "\n",
        "      image, name = td_[tr_samp+60:tr_samp+120,:][39,:], 'im_true_39_SNR{}'.format(SNR)\n",
        "      if SNR in kwargs.get('plot_SNR', [0]): show_save_image('', image, plot, save, path=get_path('', 'P'), name=name, contour=0) \n",
        "\n",
        "    else :\n",
        "      print('in autoencoder_train_data-- no noise data')\n",
        "      td_ = td\n",
        "\n",
        "    trainAE = td_[0:tr_samp,:]\n",
        "    validAE = td_[tr_samp:tr_samp+60,:]\n",
        "    testAE = td_[tr_samp+60:tr_samp+120,:]  \n",
        " \n",
        "\n",
        "  elif args.data_type == 'transient':\n",
        "\n",
        "    get_data()\n",
        "    td180, td190, td200, td185, td195 = args.raw_data\n",
        "    tr_samp = len(td180)*3\n",
        "\n",
        "    if noisy_data:\n",
        "      print('in autoencoder_train_data-- noise data', 'SNR =',SNR)\n",
        "      \n",
        "      td1_ = td2_ = td3_ = td4_ = td5_ = np.zeros_like(td180)\n",
        "      for i in range(len(td1_)):\n",
        "        td1_[i,:] = awgn(td180[i],SNR)\n",
        "        td2_[i,:] = awgn(td190[i],SNR)\n",
        "        td3_[i,:] = awgn(td200[i],SNR)\n",
        "        td4_[i,:] = awgn(td185[i],SNR)\n",
        "        td5_[i,:] = awgn(td195[i],SNR)\n",
        "      image, name = td5_[335,:], 'im_true_335_SNR{}'.format(SNR)\n",
        "      if SNR in kwargs.get('plot_SNR', [0]): show_save_image('', image, plot, save, path=get_path('', 'P'), name=name, contour=0) \n",
        "\n",
        "    else :\n",
        "      print('in autoencoder_train_data-- no noise data')\n",
        "      td1_, td2_, td3_, td4_, td5_ = td180, td190, td200, td185, td195\n",
        "    \n",
        "    trainAE = np.concatenate((td1_, td2_, td3_), axis=0)\n",
        "    validAE = td4_\n",
        "    testAE = td5_ \n",
        "\n",
        "\n",
        "  elif args.data_type == 'turbulence':\n",
        "\n",
        "    get_data()\n",
        "    td = args.raw_data\n",
        "\n",
        "    tr_samp = args.tr_samp if hasattr(args, 'tr_samp') else 3000\n",
        "    trainAE = td[0:tr_samp,:]\n",
        "    validAE = td[tr_samp:tr_samp+400,:]\n",
        "    testAE = td[tr_samp+400:tr_samp+800,:]\n",
        "\n",
        "\n",
        "  elif args.data_type == 'sea_temp':\n",
        "\n",
        "    get_data()\n",
        "    td = args.raw_data\n",
        "    tr_samp = args.tr_samp if hasattr(args, 'tr_samp') else 400\n",
        "\n",
        "    if noisy_data:\n",
        "      print('in autoencoder_train_data-- noise data', 'SNR: ', SNR)\n",
        "    \n",
        "      td_ = np.zeros_like(td)\n",
        "      for i in range(len(td)):\n",
        "        td_[i,:] = awgn(td[i], SNR)\n",
        "        \n",
        "      image, name = td_[tr_samp+100:tr_samp+200,:][0], 'im_true_0_SNR{}'.format(SNR)\n",
        "      if SNR in kwargs.get('plot_SNR', [0]): show_save_image('', image, plot, save, path=get_path('', 'P'), name=name)\n",
        "\n",
        "    else :\n",
        "      print('in autoencoder_train_data-- no noise data')\n",
        "      td_ = td\n",
        "      \n",
        "    trainAE = td_[0:tr_samp+0,:]\n",
        "    validAE = td_[tr_samp+0:tr_samp+100,:]\n",
        "    testAE = td_[tr_samp+100:tr_samp+300,:]\n",
        "\n",
        "  \n",
        "  AE_mean = np.mean(trainAE, axis = 0)  \n",
        "  AE_std = np.std(trainAE, axis = 0)  \n",
        "\n",
        "  # if args.data_type == 'sea_temp': \n",
        "  #   AE_std = np.ones_like(AE_std)\n",
        "\n",
        "  if args.data_type == 'sea_temp' or args.data_type == 'transient': \n",
        "    AE_std = np.ones_like(AE_std)\n",
        "\n",
        "  stats = np.array([AE_mean, AE_std])\n",
        "  stats[stats==0] = 0.0000001\n",
        "  print('number of training samples:', tr_samp)\n",
        "\n",
        "  return trainAE, validAE, testAE, stats"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Kz7i2VSsAgnL"
      },
      "source": [
        "## ARE_train_data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MUh4DNH_AnSS"
      },
      "source": [
        "def ARE_train_data(sensor_num, bottle_neck, seq_len, **kwargs):\n",
        "\n",
        "  seq_impct = int((seq_len-1)/2)\n",
        "\n",
        "  trainAE, validAE, testAE, stats = autoencoder_train_data(SNR=kwargs.get('SNR'), plot=kwargs.get('plot'), save=kwargs.get('save'),\n",
        "                                                           tr_samp=kwargs.get('tr_samp'), plot_SNR=kwargs.get('plot_SNR'))\n",
        "  \n",
        "  len_train_and_valid = len(trainAE[:,0])+ len(validAE[:,0])\n",
        "\n",
        "  trainAE_stan, validAE_stan, testAE_stan = standardize(trainAE, validAE, testAE, stats)\n",
        "  strain_in, svalid_in, stest_in = sensor_data(trainAE_stan, validAE_stan, testAE_stan, sensor_num)\n",
        "  n_train, n_valid, n_test = trainAE_stan.shape[0], validAE_stan.shape[0], testAE_stan.shape[0]\n",
        "  assert n_train == len(trainAE[:,0])\n",
        "  \n",
        "  # ----------------------------------------------------------------------------\n",
        "  total = np.concatenate((trainAE, validAE, testAE), axis=0)\n",
        "  train = sensorgcdatasetAE(total, stats)\n",
        "\n",
        "  path_to_weights = get_path('are', 'W_AE')\n",
        "  weights_name = get_name('are', 'W_AE', drop=kwargs.get('drop'), bottle_neck=bottle_neck)\n",
        "  pretrained_weightsAE = osp.join(path_to_weights, weights_name)\n",
        "\n",
        "  train_data_gen = torch.utils.data.DataLoader(train, shuffle=False, batch_size=len(total[:,0]))\n",
        "  model = autoencoder(bottle_neck, drop=kwargs.get('drop')).to(device)\n",
        "  model.load_state_dict(torch.load(pretrained_weightsAE, map_location=torch.device(device)))\n",
        "  model.eval()\n",
        "\n",
        "  for data in enumerate(train_data_gen):\n",
        "    img = data[1]\n",
        "    img = Variable(img).to(device)\n",
        "    waste,gg = model(img)\n",
        "    del(waste,img)\n",
        "    gtotal_out = gg.cpu().data.numpy()\n",
        "    del(gg)\n",
        "\n",
        "  gstats = np.array([np.mean(gtotal_out[0:n_train,:],axis = 0), np.std(gtotal_out[0:n_train,:], axis = 0)])\n",
        "  del(trainAE,validAE,testAE)\n",
        "  \n",
        "  gtotal = (gtotal_out - gstats[0]) / gstats[1] \n",
        "\n",
        "  if args.operation_mode == 'online':\n",
        "    gtrain_out = gtotal[seq_impct:n_train, :] \n",
        "    gvalid_out = gtotal[n_train+seq_impct: len_train_and_valid,:]\n",
        "    gtest = gtotal[len_train_and_valid+seq_impct : len(total[:,0]),:]\n",
        "\n",
        "  elif args.operation_mode == 'offline':\n",
        "    gtrain_out = gtotal[seq_impct:n_train - seq_impct, :] \n",
        "    gvalid_out = gtotal[n_train+seq_impct: len_train_and_valid-seq_impct,:]\n",
        "    gtest = gtotal[len_train_and_valid+seq_impct : len(total[:,0])-seq_impct,:]\n",
        "\n",
        "  # ----------------------------------------------------------------------------\n",
        "\n",
        "  if args.data_type == 'transient':\n",
        "    \"\"\" Deleting rows in concatenated data at junction of Reynolds's number 180, 190 and 200 \n",
        "    Because, data is not sequential at junction. Only in training data. \"\"\"\n",
        "\n",
        "    if args.operation_mode == 'online':\n",
        "\n",
        "      start1, end1 = int(n_train/3), int(n_train/3)+seq_impct\n",
        "      start2, end2 = int(n_train*2/3), int(n_train*2/3)+seq_impct\n",
        "\n",
        "    elif args.operation_mode == 'offline':\n",
        "\n",
        "      start1, end1 = int(n_train/3)- seq_impct, int(n_train/3)+ seq_impct\n",
        "      start2, end2 = int(n_train*2/3)- seq_impct, int(n_train*2/3)+ seq_impct\n",
        "\n",
        "    del_row = np.array([])\n",
        "\n",
        "    # if seq_impct=2 e = (397. 398. 399. 400.) (796. 797. 798. 799.)\n",
        "    for e in chain(range(start1, end1), range(start2, end2)): \n",
        "      del_row = np.append(del_row, [e], axis=0)\n",
        "\n",
        "    del_row = del_row.astype(int)\n",
        "    print('del_row =',del_row)\n",
        "    gtrain_out = np.delete(gtrain_out, del_row, 0)\n",
        "\n",
        "  del(total,train,gtotal)\n",
        "\n",
        "  # ----------------------------------------------------------------------------\n",
        "  if args.operation_mode == 'online':\n",
        "\n",
        "    strain_in_stacked = np.zeros((n_train-seq_impct,(seq_impct+1)*sensor_num))\n",
        "    svalid_in_stacked = np.zeros((n_valid-seq_impct,(seq_impct+1)*sensor_num))\n",
        "    stest_in_stacked = np.zeros((n_test-seq_impct,(seq_impct+1)*sensor_num))\n",
        "    \n",
        "    for i in range(0,n_train-seq_impct):\n",
        "      strain_in_stacked[i] = np.concatenate([(strain_in[i+sl,:]) for sl in range(seq_impct+1)], axis=None)\n",
        "    for i in range(0,n_valid-seq_impct):\n",
        "      svalid_in_stacked[i] = np.concatenate([(svalid_in[i+sl,:]) for sl in range(seq_impct+1)], axis=None)\n",
        "    for i in range(0,n_test-seq_impct):\n",
        "      stest_in_stacked[i] = np.concatenate([(stest_in[i+sl,:]) for sl in range(seq_impct+1)], axis=None)\n",
        "    \n",
        "    strain_in_stacked = np.delete(strain_in_stacked, del_row-seq_impct, 0) if args.data_type == 'transient' else strain_in_stacked\n",
        "\n",
        "\n",
        "  elif args.operation_mode == 'offline':\n",
        "\n",
        "    strain_in_stacked = np.zeros((n_train-2*seq_impct,seq_len*sensor_num))\n",
        "    svalid_in_stacked = np.zeros((n_valid-2*seq_impct,seq_len*sensor_num))\n",
        "    stest_in_stacked = np.zeros((n_test-2*seq_impct,seq_len*sensor_num))\n",
        "\n",
        "    for i in range(0,n_train-2*seq_impct) :\n",
        "      strain_in_stacked[i] = np.concatenate([(strain_in[i+sl,:]) for sl in range(seq_len)], axis=None)\n",
        "    for i in range(0,n_valid-2*seq_impct) :\n",
        "      svalid_in_stacked[i] = np.concatenate([(svalid_in[i+sl,:]) for sl in range(seq_len)], axis=None)\n",
        "    for i in range(0,n_test-2*seq_impct) :\n",
        "      stest_in_stacked[i] = np.concatenate([(stest_in[i+sl,:]) for sl in range(seq_len)], axis=None)\n",
        "    \n",
        "    strain_in_stacked = np.delete(strain_in_stacked, del_row-seq_impct, 0) if args.data_type == 'transient' else strain_in_stacked\n",
        "  # ----------------------------------------------------------------------------\n",
        "  \n",
        "  return gtrain_out, gvalid_out, strain_in_stacked, svalid_in_stacked, gtest, stest_in_stacked, gstats, stats\n"
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Pe3n_7wJV2Xm",
        "outputId": "793275d7-fd76-413e-ea44-09e1522e610d"
      },
      "source": [
        "n_train = 399*3\n",
        "seq_impct = 2\n",
        "del_row = np.array([])\n",
        "for e in range(int(n_train/3), int(n_train/3)+seq_impct): # if seq_impct=2 (397. 398. 399. 400.)\n",
        "  del_row = np.append(del_row, [e], axis=0)    #n_train/3 = 399\n",
        "for e in range(int(n_train*2/3), int(n_train*2/3)+seq_impct):  #if seq_impct=2 (796. 797. 798. 799.)\n",
        "  del_row = np.append(del_row, [e], axis=0)\n",
        "del_row = del_row.astype(int)\n",
        "print('del_row =',del_row)\n",
        "del_row-seq_impct"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "del_row = [399 400 798 799]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([397, 398, 796, 797])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TVs3XVBXA22T"
      },
      "source": [
        "## PDS_train_data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pksOAQGhBxPG"
      },
      "source": [
        "\n",
        "def PDS_train_data(sensor_num, bottle_neck, **kwargs):\n",
        "  \n",
        "  trainAE, validAE, testAE, stats = autoencoder_train_data(SNR=kwargs.get('SNR'), plot=kwargs.get('plot'), save=kwargs.get('save'),\n",
        "                                                           tr_samp=kwargs.get('tr_samp'), plot_SNR=kwargs.get('plot_SNR'))\n",
        "  mean, std = stats[0], stats[1]\n",
        "\n",
        "  # ----------------------------------------------------------------------------\n",
        "  X = (trainAE - mean).transpose()\n",
        "  phi, s, V = np.linalg.svd(X, full_matrices=False)\n",
        "  del(s,V)\n",
        "  phi = phi[:,0:bottle_neck]\n",
        "\n",
        "  gtrain_out = (np.dot(phi.transpose(), X)).transpose()\n",
        "  del(X)\n",
        "\n",
        "  XX = (validAE - mean).transpose()\n",
        "  gvalid_out = (np.dot(phi.transpose(), XX)).transpose()\n",
        "\n",
        "  XXX = (testAE - mean).transpose()\n",
        "  gtest_out = (np.dot(phi.transpose(), XXX)).transpose()\n",
        "  del(XX, XXX)\n",
        "  # ----------------------------------------------------------------------------\n",
        "\n",
        "  trainAE_stan, validAE_stan, testAE_stan = standardize(trainAE, validAE, testAE, stats)\n",
        "  del(trainAE, validAE, testAE)\n",
        "\n",
        "  strain_in, svalid_in, stest_in = sensor_data(trainAE_stan, validAE_stan, testAE_stan, sensor_num)\n",
        "\n",
        "  gstats = np.array([np.mean(gtrain_out, axis = 0), np.std(gtrain_out, axis = 0)])\n",
        "  gtrain_out, gvalid_out, gtest_out = standardize(gtrain_out, gvalid_out, gtest_out, gstats)\n",
        "\n",
        "  # gc_pod = gtest_out * gstats[1, :] + gstats[0, :]\n",
        "  # t_pod = gc_pod.dot(phi.T) + mean\n",
        "  # show_save_image('pds', t_pod[0], 1, 0)\n",
        "  # show_save_image('pds', testAE[0], 1, 0)\n",
        "  del(trainAE_stan,validAE_stan,testAE_stan)\n",
        "\n",
        "  return gtrain_out, gvalid_out, strain_in, svalid_in, gtest_out, stest_in, gstats, phi, stats   "
      ],
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gGf32xK4qIXQ"
      },
      "source": [
        "## SD_train_data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fO6gSkfPqOn0"
      },
      "source": [
        "def SD_train_data(sensor_num, **kwargs):\n",
        "\n",
        "  trainAE, validAE, testAE, stats = autoencoder_train_data(SNR=kwargs.get('SNR'), plot=kwargs.get('plot'), save=kwargs.get('save'),\n",
        "                                                           tr_samp=kwargs.get('tr_samp'), plot_SNR=kwargs.get('plot_SNR'))\n",
        "  stats[1] = np.ones_like(stats[1])\n",
        "\n",
        "  trainAE_stan, validAE_stan, testAE_stan = standardize(trainAE, validAE, testAE, stats)\n",
        "\n",
        "  strain_in, svalid_in, stest_in = sensor_data(trainAE_stan, validAE_stan, testAE_stan, sensor_num)\n",
        "\n",
        "  return trainAE_stan, validAE_stan, strain_in, svalid_in, testAE_stan, stest_in, stats"
      ],
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mO-omVf397wc"
      },
      "source": [
        "\n",
        "# **ARE**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pxFT2535ui8y"
      },
      "source": [
        "## AUTOENCODER Train Loop"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XNOOTjJ7uwZI"
      },
      "source": [
        "def AutoEcoder(bn, lr, num_epochs, bs, bsv, early_stop, **kwargs):\n",
        "\n",
        "  bottle_neck = bn\n",
        "  \n",
        "  trainAE, validAE, waste, stats = autoencoder_train_data(drop=0, tr_samp=kwargs.get('tr_samp'))\n",
        "  del(waste)\n",
        "  train = sensorgcdatasetAE(trainAE, stats)\n",
        "  valid = sensorgcdatasetAE(validAE, stats)\n",
        "\n",
        "  train_data_gen = torch.utils.data.DataLoader(train, shuffle=True, batch_size=bs)#)\n",
        "  valid_data_gen = torch.utils.data.DataLoader(valid, batch_size=bsv)#)\n",
        "  dataloaders = {'train': train_data_gen, 'valid':valid_data_gen}\n",
        "  dataset_sizes = {'train': len(train_data_gen.dataset), 'valid': len(valid_data_gen.dataset)}\n",
        "\n",
        "  model = autoencoder(bottle_neck, drop=kwargs.get('drop',None)).to(device)\n",
        "  criterion = nn.MSELoss().to(device)\n",
        "  optimizer = torch.optim.Adam(model.parameters(), lr=lr,  weight_decay=1e-5)\n",
        "\n",
        "  prev_valid_loss = prev_train_loss = 100  # anylarge number\n",
        "  stop=0\n",
        "  rerror_train = []\n",
        "  rerror_valid = []\n",
        "  ii = 0\n",
        "  k=0\n",
        "  for epochs in range(num_epochs):\n",
        "    for phase in ['train','valid']:\n",
        "      running_loss=0\n",
        "\n",
        "      if phase == 'train':\n",
        "        model.train()\n",
        "      else:\n",
        "        model.eval()\n",
        "\n",
        "      for data in enumerate(dataloaders[phase]):\n",
        "          img = data[1]\n",
        "          # print(img.shape)\n",
        "          img = Variable(img).to(device)\n",
        "          # ===================forward=====================\n",
        "          output, waste = model(img)\n",
        "          #print(output.size(),'oo')\n",
        "          loss = criterion(output, img)\n",
        "          # ===================backward====================\n",
        "          optimizer.zero_grad()\n",
        "\n",
        "          if phase=='train':\n",
        "              #backward\n",
        "              loss.backward()\n",
        "              #update weights\n",
        "              optimizer.step()\n",
        "\n",
        "          running_loss += output.shape[0]*loss.data\n",
        "\n",
        "      if phase == 'train':\n",
        "        train_epoch_loss = running_loss/dataset_sizes[phase]\n",
        "        train_out_last = output \n",
        "        train_in_last = img \n",
        "      elif phase == 'valid':\n",
        "        valid_epoch_loss = running_loss/dataset_sizes[phase]\n",
        "    stop = stop+1\n",
        "\n",
        "    # if (valid_epoch_loss < prev_valid_loss):\n",
        "    if (train_epoch_loss < prev_train_loss):\n",
        "\n",
        "      model_wts = model.state_dict()\n",
        "      prev_valid_loss = valid_epoch_loss\n",
        "      prev_train_loss = train_epoch_loss\n",
        "      print('({}) Training Loss: {:.8f} NEW Valid Loss: {:.8f} *'.format(epochs, train_epoch_loss, valid_epoch_loss))\n",
        "      stop = 0\n",
        "      best_train_out, best_train_in = train_out_last, train_in_last\n",
        "      best_valid_out, best_valid_in = output, img\n",
        "      \n",
        "    else:\n",
        "      print('({}) Training Loss: {:.8f} Valid Loss: {:.8f} '.format(epochs, train_epoch_loss, valid_epoch_loss))\n",
        "      pass\n",
        "\n",
        "    if stop == early_stop:\n",
        "      print('Early stopping criteria fulfilled')\n",
        "      break\n",
        "\n",
        "    rerror_train.append(train_epoch_loss)\n",
        "    rerror_valid.append(valid_epoch_loss)\n",
        "\n",
        "  images = (best_train_out, best_train_in, best_valid_out, best_valid_in)\n",
        "  show, save, path = (1, 1, 1, 1), (1, 1, 1, 1), get_path('are', 'W_AE') \n",
        "  name = ('best_train_out', 'best_train_in', 'best_valid_out', 'best_valid_in')\n",
        "  show_save_image('are', images, show, save, stats=stats, path=path, name=name)\n",
        "\n",
        "  del(trainAE, validAE, stats, train)\n",
        "\n",
        "  path_to_weights = get_path('are', 'W_AE')\n",
        "  weights_name = get_name('', 'W_AE', drop=kwargs.get('drop'), bottle_neck=bottle_neck)\n",
        "  torch.save(model_wts, osp.join(path_to_weights, weights_name))\n",
        "  print('==========weights saved==========')\n",
        "\n",
        "  plot_graph(rerror_train,rerror_valid, 1, osp.join(path_to_weights, weights_name + '.png'))\n",
        "  print('================================ over ================================')"
      ],
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0l797mzuBa8t"
      },
      "source": [
        "## ARE TrainLoop"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vk5J8fv797wh"
      },
      "source": [
        "def ARE_train_LSTM(sensor_number, seq_len, bottle_neck, hidden_size, num_epochs, lr, **kwargs):    \n",
        "\n",
        "  print('sensor_number =',sensor_number,'seq_len = ',seq_len)\n",
        "  layers = 1\n",
        "  seq_impct = int((seq_len-1)/2)\n",
        "\n",
        "  gtrain_out, gvalid_out, strain_in, svalid_in, gtest_stan, stest_in_stacked, gstats, stats  = ARE_train_data(sensor_number, \n",
        "                                                                                                       bottle_neck,\n",
        "                                                                                                       seq_len, \n",
        "                                                                                                       drop=kwargs.get('drop'), \n",
        "                                                                                                       data_noise=None)\n",
        "  # if args.data_type == 'sea_temp':\n",
        "  #   strain_in, svalid_in  = np.expand_dims(strain_in, axis=1), np.expand_dims(svalid_in, axis=1)\n",
        "  #   gtrain_out, gvalid_out = np.expand_dims(gtrain_out, axis=1), np.expand_dims(gvalid_out, axis=1)\n",
        "\n",
        "  \n",
        "  del(gtest_stan, stest_in_stacked)\n",
        "  train = sensorgcdatasetRNN(strain_in, gtrain_out)\n",
        "  valid = sensorgcdatasetRNN(svalid_in, gvalid_out)\n",
        "  print(strain_in.shape, gtrain_out.shape, svalid_in.shape, gvalid_out.shape)\n",
        "\n",
        "  bs = len(train)  # batch size\n",
        "  bsv = len(valid)\n",
        "  print(bs, bsv)\n",
        "  train_data_gen = torch.utils.data.DataLoader(train, shuffle=False, batch_size=bs)\n",
        "  valid_data_gen = torch.utils.data.DataLoader(valid, shuffle=False, batch_size=bsv)\n",
        "  dataloaders = {'train': train_data_gen, 'valid':valid_data_gen}\n",
        "  dataset_sizes = {'train': len(train_data_gen.dataset), 'valid': len(valid_data_gen.dataset)}\n",
        "  batch ={'train':bs, 'valid':bsv}\n",
        "\n",
        "  if args.RNN == 'lstm':\n",
        "    model = are_lstm_net(hidden_size, sensor_number, bottle_neck, layers, seq_len).to(device)\n",
        "  elif args.RNN == 'rnn':\n",
        "    model = are_rnn_net(hidden_size, sensor_number, bottle_neck, layers, seq_len).to(device)\n",
        "\n",
        "  criterion = nn.MSELoss().to(device)\n",
        "  optimizer = torch.optim.Adam(model.parameters(), lr=lr,  weight_decay=1e-5)\n",
        "\n",
        "  prev_valid_loss = prev_train_loss = 100  # any large number\n",
        "  stop = 0\n",
        "  rerror_train = []\n",
        "  rerror_valid = []\n",
        "  for epochs in range(num_epochs):\n",
        "\n",
        "    for phase in ['train','valid']:\n",
        "      running_loss=0\n",
        "      if phase == 'train':\n",
        "        model.train()\n",
        "      else:\n",
        "        model.eval()\n",
        "\n",
        "      for i, (sensor, gc) in enumerate(dataloaders[phase]):\n",
        "\n",
        "          if args.operation_mode == 'online': \n",
        "            y = torch.zeros((seq_impct+1), batch[phase], sensor_number)\n",
        "            for i in range(0, batch[phase]):\n",
        "                for j in range(0, (seq_impct+1)):\n",
        "                    y[j, i,:] = sensor[i, sensor_number*j: sensor_number*j + sensor_number]\n",
        "                    \n",
        "          elif args.operation_mode == 'offline':\n",
        "            y = torch.zeros(seq_len, batch[phase], sensor_number)\n",
        "            for i in range(0, batch[phase]):\n",
        "                for j in range(0, seq_len):\n",
        "                    y[j, i,:] = sensor[i, sensor_number*j: sensor_number*j + sensor_number]\n",
        "                      \n",
        "          sensor = Variable(y).to(device)\n",
        "          gc = Variable(gc).to(device)\n",
        "\n",
        "          optimizer.zero_grad()\n",
        "          gc_out = model(sensor)\n",
        "          loss = criterion(gc_out, gc)\n",
        "          if phase=='train':\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "          running_loss = gc_out.shape[0] * loss.data + running_loss\n",
        "\n",
        "      if phase == 'train':\n",
        "        train_epoch_loss = running_loss/dataset_sizes[phase]\n",
        "      elif phase == 'valid':\n",
        "        valid_epoch_loss = running_loss/dataset_sizes[phase]\n",
        "    stop = stop + 1\n",
        "\n",
        "    # if (valid_epoch_loss < prev_valid_loss):\n",
        "    if (train_epoch_loss < prev_train_loss):\n",
        "      model_wts = model.state_dict()\n",
        "      prev_valid_loss = valid_epoch_loss\n",
        "      prev_train_loss = train_epoch_loss\n",
        "      # print('({}) Training Loss: {:.8f} NEW Valid Loss: {:.8f} *'.format(epochs, train_epoch_loss, valid_epoch_loss))\n",
        "      stop = 0\n",
        "      output_last = gc_out\n",
        "      gc_last = gc\n",
        "\n",
        "    else:\n",
        "      nothing = 1\n",
        "      #print('({}) Training Loss: {:.8f} Valid Loss: {:.8f} '.format(epochs, train_epoch_loss, valid_epoch_loss))\n",
        "    \n",
        "    if stop == 230:\n",
        "        print('Early stopping criteria fulfilled')\n",
        "        break\n",
        "\n",
        "    rerror_train.append(train_epoch_loss)\n",
        "    rerror_valid.append(valid_epoch_loss)\n",
        "\n",
        "  #plot_image(output_last, img_last, gstats)\n",
        "  print('({}) BEST_Training Loss: {:.8f} BEST_Valid Loss: {:.8f} '.format(epochs, prev_train_loss, prev_valid_loss))\n",
        "\n",
        "  path_to_weights = get_path('are', 'W')\n",
        "  weights_name = get_name('are', 'W', drop=kwargs.get('drop'), sensor_number=sensor_number, seq_len=seq_len, bottle_neck=bottle_neck)\n",
        "  torch.save(model_wts, osp.join(path_to_weights, weights_name))\n",
        "  print('=========saved weights=========')\n",
        "\n",
        "  plot_graph(rerror_train,rerror_valid, 1, osp.join(path_to_weights, weights_name + '.png'))\n",
        "  print('================================ over ================================')"
      ],
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "58B1Cz0wBkjo"
      },
      "source": [
        "## ARE TestLoop"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VA9j2Dn_AQ1g"
      },
      "source": [
        "def ARE_test(sensor_number, seq_len, bottle_neck, plot, save, **kwargs):\n",
        "\n",
        "  hidden_size = 50\n",
        "  layers = 1\n",
        "  seq_impct = int((seq_len-1)/2)\n",
        "\n",
        "  if args.data_type == 'periodic':\n",
        "    start, end = 0, 60\n",
        "  if args.data_type == 'transient':\n",
        "    start, end = 200, 399\n",
        "  if args.data_type == 'turbulence':\n",
        "    start, end = 0, 400\n",
        "  if args.data_type == 'sea_temp':\n",
        "    start, end = 0, 200  # 200\n",
        "\n",
        "  # ----------------------------------------------------------------------------\n",
        "\n",
        "  path_to_weights = get_path('are', 'W')\n",
        "  weights_name = get_name('are', 'W', drop=kwargs.get('drop'), sensor_number=sensor_number, seq_len=seq_len, bottle_neck=bottle_neck)\n",
        "  pretrained_weightsRNN = osp.join(path_to_weights, weights_name)\n",
        "\n",
        "  gtrain_out, gvalid_out, strain_in, svalid_in, gtest_out, stest_in, gstats, stats = ARE_train_data(sensor_number, \n",
        "                                                                                                      bottle_neck, \n",
        "                                                                                                      seq_len, \n",
        "                                                                                                      drop=kwargs.get('drop',None), \n",
        "                                                                                                      SNR=kwargs.get('SNR',None), \n",
        "                                                                                                      data_noise=kwargs.get('drop',None),\n",
        "                                                                                                      plot_SNR=kwargs.get('plot_SNR'),\n",
        "                                                                                                      plot=plot[2], save=save[2])\n",
        "\n",
        "  del(gtrain_out, gvalid_out, strain_in, svalid_in)\n",
        "\n",
        "  if args.RNN == 'lstm':\n",
        "    model = are_lstm_net(hidden_size, sensor_number, bottle_neck, layers, seq_len).to(device)\n",
        "  elif args.RNN == 'rnn':\n",
        "    model = are_rnn_net(hidden_size, sensor_number, bottle_neck, layers, seq_len).to(device)\n",
        "\n",
        "  model.load_state_dict(torch.load(pretrained_weightsRNN, map_location=torch.device(device)))\n",
        "  model.eval()\n",
        "\n",
        "  test = sensorgcdatasetRNN(stest_in, gtest_out)\n",
        "  test_data_genn = torch.utils.data.DataLoader(test, shuffle=False, batch_size=1)\n",
        "\n",
        "  gc_pred = np.zeros([len(test), bottle_neck])\n",
        "\n",
        "  for i, (sensor, gc) in enumerate(test):\n",
        "\n",
        "      if args.operation_mode == 'online': \n",
        "\n",
        "        y = torch.zeros((seq_impct+1),1, sensor_number)\n",
        "        for j in range(0, (seq_impct+1)):\n",
        "            y[j,0, :] = sensor[sensor_number*j : (sensor_number*j + sensor_number)]\n",
        "                  \n",
        "      elif args.operation_mode == 'offline':\n",
        "\n",
        "        y = torch.zeros(seq_len,1, sensor_number)\n",
        "        for j in range(0, seq_len):\n",
        "            y[j,0, :] = sensor[sensor_number*j : (sensor_number*j + sensor_number)]\n",
        "            \n",
        "      gc_out = model(y.to(device)).cpu().data.numpy()\n",
        "      gc_pred[i, :] = gc_out\n",
        "      \n",
        "  del(stest_in, gtest_out)\n",
        "  gc_pred = gc_pred * gstats[1, :] + gstats[0, :]\n",
        "\n",
        "  # ----------------------------------------------------------------------------\n",
        "\n",
        "  path_to_weights = get_path('are', 'W_AE')\n",
        "  weights_name = get_name('are', 'W_AE', drop=kwargs.get('drop'), bottle_neck=bottle_neck)\n",
        "  pretrained_weightsAE = osp.join(path_to_weights, weights_name)\n",
        "\n",
        "  model = autoencoder(bottle_neck, drop=kwargs.get('drop')).to(device)\n",
        "\n",
        "  model.load_state_dict(torch.load(pretrained_weightsAE, map_location=torch.device(device)))\n",
        "  model.eval()\n",
        "\n",
        "  # if args.data_type == 'sea_temp':\n",
        "  #   gc_pred  = np.expand_dims(gc_pred, axis=1)\n",
        "\n",
        "  _, _, testAE, statsAE = autoencoder_train_data()\n",
        "  del(_)\n",
        "\n",
        "  if args.operation_mode == 'online': \n",
        "    testAA = sensorgcdatasetAA(gc_pred, testAE[seq_impct:end,:], statsAE)\n",
        "                  \n",
        "  elif args.operation_mode == 'offline':\n",
        "    testAA = sensorgcdatasetAA(gc_pred, testAE[seq_impct:end-seq_impct,:], statsAE)\n",
        "  \n",
        "  train_data_genn = torch.utils.data.DataLoader(testAA, shuffle=False, batch_size=1)\n",
        "  _, cords = sensor_cord_data(sensor_number)\n",
        "\n",
        "  t_pred = np.zeros([len(testAE), len(testAE[0,:]) ])\n",
        "  t_true = np.zeros([len(testAE), len(testAE[0,:]) ])\n",
        "  for i, (gc_pre, t) in enumerate(train_data_genn):\n",
        "      # t_out_tensor = model(gc_pre[None, :].to(device))\n",
        "      t_out_tensor = model(gc_pre.to(device))\n",
        "      t_out = t_out_tensor.cpu().data.numpy()\n",
        "      t_pred[i, :] = t_out\n",
        "      t_true[i, :] = t\n",
        "      image_select('are', i, (t_out_tensor[0], t), sensor_number, plot[:2], save[:2], stats=statsAE, seq_len=seq_len, cords=cords, **kwargs)\n",
        "\n",
        "  if kwargs.get('use_stats', 1):\n",
        "    # t_pred = t_pred * statsAE[1, :] + statsAE[0, :]\n",
        "    t_pred = t_pred * stats[1, :] + stats[0, :]\n",
        "    t_true = t_true * statsAE[1, :] + statsAE[0, :]\n",
        "\n",
        "  # ----------------------------------------------------------------------------\n",
        "\n",
        "  Ferr = []\n",
        "\n",
        "  if args.operation_mode == 'online':      \n",
        "      for i in range(start, end-seq_impct):\n",
        "        Ferr.append(np.linalg.norm(t_true[i,:] - t_pred[i,:]) / np.linalg.norm(t_true[i,:]))\n",
        "\n",
        "  elif args.operation_mode == 'offline':\n",
        "    for i in range(start, end-2*seq_impct):\n",
        "        Ferr.append(np.linalg.norm(t_true[i,:] - t_pred[i,:]) / np.linalg.norm(t_true[i,:]))\n",
        "\n",
        "  Ferr_avg = np.mean(Ferr)\n",
        "  print(\"Error: \", Ferr_avg, '------------------------------------------------')\n",
        "\n",
        "  return Ferr_avg"
      ],
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mZjvskHx-lYk"
      },
      "source": [
        "# **POD DEEP STATE**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9kJS4vOK99ja"
      },
      "source": [
        "## PDS TrainLoop"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PiShL93hrBi7"
      },
      "source": [
        "def PDS_train(sensor_number, bottle_neck, num_epochs, lr, **kwargs):\n",
        "  \n",
        "  gtrain_out, gvalid_out, strain_in, svalid_in, gtest_stan, stest_DS, gstats, phi, stats = PDS_train_data(sensor_number, bottle_neck)\n",
        "\n",
        "\n",
        "  del(gtest_stan, stest_DS,phi, stats)\n",
        "\n",
        "  train = sensorgcdatasetDS(strain_in, gtrain_out)\n",
        "  valid = sensorgcdatasetDS(svalid_in, gvalid_out)\n",
        "\n",
        "  bs = len(train)  # batch size\n",
        "  bsv = len(valid)\n",
        "  train_data_gen = torch.utils.data.DataLoader(train, shuffle=True, batch_size=bs)\n",
        "  valid_data_gen = torch.utils.data.DataLoader(valid, shuffle=True, batch_size=bsv)\n",
        "\n",
        "  dataloaders = {'train': train_data_gen, 'valid':valid_data_gen}\n",
        "  dataset_sizes = {'train': len(train_data_gen.dataset), 'valid': len(valid_data_gen.dataset)}\n",
        "  batch ={'train':bs, 'valid':bsv}\n",
        "\n",
        "  model = PDSnetwork(sensor_number, bottle_neck).to(device)\n",
        "\n",
        "  # Loss and Optimizer\n",
        "  criterion = nn.MSELoss()\n",
        "  optimizer = torch.optim.Adam(model.parameters(), lr=lr, weight_decay=1e-5)\n",
        "  #exp_lr_scheduler = torch.optim.lr_scheduler.MultiStepLR(optimizer, milestones=[500], gamma=0.1)\n",
        "\n",
        "  prev_valid_loss = prev_train_loss = 100  # any large number\n",
        "  stop = 0\n",
        "  rerror_train = []\n",
        "  rerror_valid = []\n",
        "  for epochs in range(num_epochs):\n",
        "    for phase in ['train','valid']:\n",
        "      running_loss=0\n",
        "      if phase == 'train':\n",
        "        model.train()\n",
        "      else:\n",
        "        model.eval()\n",
        "\n",
        "      for i, (sensor, gc) in enumerate(dataloaders[phase]):\n",
        "          sensor = Variable(sensor).to(device)\n",
        "          gc = Variable(gc).to(device)\n",
        "          optimizer.zero_grad()\n",
        "          gc_out = model(sensor)\n",
        "\n",
        "          # calculate loss\n",
        "          loss = criterion(gc_out, gc)\n",
        "\n",
        "          if phase=='train':\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            #exp_lr_scheduler.step()\n",
        "\n",
        "          running_loss = gc_out.shape[0] * loss.data + running_loss\n",
        "\n",
        "      if phase == 'train':\n",
        "        train_epoch_loss = running_loss/dataset_sizes[phase]\n",
        "      elif phase == 'valid':\n",
        "        valid_epoch_loss = running_loss/dataset_sizes[phase]\n",
        "    stop = stop + 1\n",
        "\n",
        "    # if (valid_epoch_loss < prev_valid_loss):\n",
        "    if (train_epoch_loss < prev_train_loss):\n",
        "      model_wts = model.state_dict()\n",
        "      prev_valid_loss = valid_epoch_loss\n",
        "      prev_train_loss = train_epoch_loss\n",
        "      #print('({}) Training Loss: {:.8f} NEW Valid Loss: {:.8f} *'.format(epochs, train_epoch_loss, valid_epoch_loss))\n",
        "      stop = 0\n",
        "\n",
        "    else:\n",
        "      ignore=0\n",
        "      #print('({}) Training Loss: {:.8f} Valid Loss: {:.8f} '.format(epochs, train_epoch_loss, valid_epoch_loss))\n",
        "\n",
        "    if stop == 230:\n",
        "        print('Early stopping criteria fulfilled')\n",
        "        break\n",
        "    rerror_train.append(train_epoch_loss)\n",
        "    rerror_valid.append(valid_epoch_loss)\n",
        "\n",
        "  path_to_weights = get_path('pds', 'W')\n",
        "  weights_name = get_name('pds', 'W', drop=kwargs.get('drop'), sensor_number=sensor_number, bottle_neck=bottle_neck)\n",
        "  torch.save(model_wts, osp.join(path_to_weights, weights_name))\n",
        "  print('=========saved weights=========')\n",
        "\n",
        "  plot_graph(rerror_train,rerror_valid, 1, osp.join(path_to_weights, weights_name + '.png'))\n",
        "  print('================================ over ================================')"
      ],
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UCZoIA0e-D4z"
      },
      "source": [
        "## PDS TestLoop"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XDhfdO0b-lY3"
      },
      "source": [
        "\n",
        "def PDS_test(sensor_number, bottle_neck, plot, save, **kwargs): \n",
        "\n",
        "  path_to_weights = get_path('pds', 'W')\n",
        "  weights_name = get_name('pds', 'W', sensor_number=sensor_number, bottle_neck=bottle_neck)\n",
        "  pretrained_weightsRNN = osp.join(path_to_weights, weights_name)\n",
        "  \n",
        "  gtrain_out, gvalid_out, strain_in, svalid_in, gtest, stest_in, gstats, phi, stats = PDS_train_data(sensor_number,bottle_neck,\n",
        "                                                                                                                  SNR=kwargs.get('SNR'),\n",
        "                                                                                                                  plot_SNR=kwargs.get('plot_SNR'),\n",
        "                                                                                                                  plot=plot[2], save=save[2])\n",
        "\n",
        "  del(gtrain_out, gvalid_out, strain_in, svalid_in)\n",
        "\n",
        "  modell = PDSnetwork(sensor_number, bottle_neck).to(device)\n",
        "  modell.load_state_dict(torch.load(pretrained_weightsRNN, map_location=torch.device(device)))\n",
        "  modell.eval()\n",
        "\n",
        "  test = sensorgcdatasetDS(stest_in, gtest)\n",
        "\n",
        "  gc_pred = np.zeros([len(test), bottle_neck])\n",
        "  gc_pod = np.zeros([len(test), bottle_neck])\n",
        "  for i, (sensor, gc) in enumerate(test):\n",
        "      y = modell(sensor.to(device))\n",
        "      gc_pred[i, :] = y.cpu().data.numpy()\n",
        "      gc_pod[i, :] = gc.cpu().data.numpy()\n",
        "\n",
        "  gc_pred = gc_pred * gstats[1, :] + gstats[0, :]\n",
        "  gc_pod = gc_pod * gstats[1, :] + gstats[0, :]\n",
        "\n",
        "  t_pred = gc_pred.dot(phi.T) \n",
        "  t_pod = gc_pod.dot(phi.T) \n",
        "  \n",
        "  _, _, testAE, statsAE = autoencoder_train_data()\n",
        "  del(_)\n",
        "  t_true = testAE \n",
        "\n",
        "  if kwargs.get('use_stats', 1):\n",
        "    # t_pred = t_pred + statsAE[0]\n",
        "    t_pred = t_pred + stats[0]\n",
        "  else:\n",
        "    t_true = t_true - statsAE[0]\n",
        "\n",
        "  _, cords = sensor_cord_data(sensor_number)\n",
        "  for i in range(len(t_true)):\n",
        "    image_select('pds', i, (t_pred[i], t_true[i]), sensor_number, plot[:2], save[:2], cords=cords, **kwargs)\n",
        "    # if i in  kwargs.get('plot_image_idx', [None]):   show_save_image('pds', t_pod[i], 1, 0)\n",
        "\n",
        "  # ----------------------------------------------------------------------------\n",
        "  \n",
        "  Ferr = []\n",
        "  if args.data_type == 'periodic':\n",
        "    start, end = 0, 60\n",
        "  if args.data_type == 'transient':\n",
        "    start, end = 200, 399\n",
        "  if args.data_type == 'turbulence':\n",
        "    start, end = 0, 400\n",
        "  if args.data_type == 'sea_temp':\n",
        "    start, end = 0, 200#200\n",
        "\n",
        "  for i in range(start,end):\n",
        "      Ferr.append(np.linalg.norm(t_true[i,:] - t_pred[i,:]) / np.linalg.norm(t_true[i,:]))\n",
        "\n",
        "  Ferr_avg = np.mean(Ferr)\n",
        "  print(\"Error: \", Ferr_avg, '------------------------------------------------')\n",
        "\n",
        "  return Ferr_avg"
      ],
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-5nYRiX1f-ya"
      },
      "source": [
        "# SHALLOW DECODER"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "It5emxOFNrBB"
      },
      "source": [
        "## SD Train Loop"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sKVNtfLAN5re"
      },
      "source": [
        "def SD_train(sensor_number, num_epochs, lr, wd, learning_rate_change=0.9, weight_decay_change=0.8, epoch_update=100, **kwargs):    \n",
        "\n",
        "  print('sensor_number =',sensor_number)\n",
        "  train_out, valid_out, strain_in, svalid_in, test_out, stest_in, stats = SD_train_data(sensor_number,\n",
        "                                                                                        data_noise=None)\n",
        "  del(test_out, stest_in)\n",
        "  strain_in, svalid_in  = torch.tensor(np.expand_dims(strain_in, axis=1)).float().to(device), torch.tensor(np.expand_dims(svalid_in, axis=1)).float().to(device)\n",
        "  train_out, valid_out = torch.tensor(np.expand_dims(train_out, axis=1)).float().to(device), torch.tensor(np.expand_dims(valid_out, axis=1)).float().to(device)\n",
        "\n",
        "  # pdb.set_trace()\n",
        "  train_data = torch.utils.data.TensorDataset(strain_in, train_out)\n",
        "  valid_data = torch.utils.data.TensorDataset(svalid_in, valid_out)\n",
        "\n",
        "  bs = 50  # len(strain_in)  # batch size\n",
        "  bsv = len(svalid_in)\n",
        "\n",
        "  train_loader = DataLoader(dataset = train_data, batch_size = bs, shuffle = True)\n",
        "  valid_loader = DataLoader(dataset = valid_data, batch_size = bsv, shuffle = True)\n",
        "\n",
        "  dataloaders = {'train': train_loader, 'valid':valid_loader}\n",
        "  dataset_sizes = {'train': len(train_loader.dataset), 'valid': len(valid_loader.dataset)}\n",
        "  batch ={'train':bs, 'valid':bsv}\n",
        "\n",
        "  model = shallow_decoder_net(sensor_number, drop=kwargs.get('drop',None)).to(device)\n",
        "\n",
        "  criterion = nn.MSELoss().to(device)\n",
        "  optimizer = torch.optim.Adam(model.parameters(), lr=lr, weight_decay=wd)\n",
        "\n",
        "  def exp_lr_scheduler(optimizer, epoch, lr_decay_rate=0.8, weight_decay_rate=0.8, lr_decay_epoch=100):\n",
        "        \"\"\"Decay learning rate by a factor of lr_decay_rate every lr_decay_epoch epochs\"\"\"\n",
        "        if epoch % lr_decay_epoch:\n",
        "            return \n",
        "        \n",
        "        # if args.optimizer == 'sgd':\n",
        "        for param_group in optimizer.param_groups:\n",
        "            param_group['lr'] *= lr_decay_rate\n",
        "            param_group['weight_decay'] *= weight_decay_rate\n",
        "        return \n",
        "\n",
        "  prev_valid_loss = prev_train_loss = 100  # any large number\n",
        "  stop = 0\n",
        "  rerror_train = []\n",
        "  rerror_valid = []\n",
        "\n",
        "  for epochs in range(num_epochs):\n",
        "    for phase in ['train','valid']:\n",
        "      running_loss=0\n",
        "      if phase == 'train':\n",
        "        model.train()\n",
        "      else:\n",
        "        model.eval()\n",
        "\n",
        "      for i, (sensor, true) in enumerate(dataloaders[phase]):\n",
        "\n",
        "          optimizer.zero_grad()\n",
        "          pred = model(sensor).to(device)\n",
        "          loss = criterion(pred, true)\n",
        "\n",
        "          if phase=='train':\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            # ===================adjusted lr========================\n",
        "            exp_lr_scheduler(optimizer, epochs, lr_decay_rate = learning_rate_change, \n",
        "                             weight_decay_rate = weight_decay_change, \n",
        "                             lr_decay_epoch = epoch_update)\n",
        "\n",
        "          running_loss = pred.shape[0] * loss.data + running_loss\n",
        "\n",
        "      if phase == 'train':\n",
        "        train_epoch_loss = running_loss/dataset_sizes[phase]\n",
        "        train_out_last = pred \n",
        "        train_in_last = true \n",
        "      elif phase == 'valid':\n",
        "        valid_epoch_loss = running_loss/dataset_sizes[phase]\n",
        "    stop = stop + 1\n",
        "\n",
        "    # if (valid_epoch_loss < prev_valid_loss):\n",
        "    if (train_epoch_loss < prev_train_loss):\n",
        "      model_wts = model.state_dict()\n",
        "      prev_valid_loss = valid_epoch_loss\n",
        "      prev_train_loss = train_epoch_loss\n",
        "\n",
        "      # print('({}) Training Loss: {:.8f} NEW Valid Loss: {:.8f} *'.format(epochs, train_epoch_loss, valid_epoch_loss))\n",
        "      stop = 0\n",
        "      best_train_out, best_train_in = train_out_last, train_in_last\n",
        "      best_valid_out, best_valid_in = pred, true\n",
        "\n",
        "    else:\n",
        "      nothing = 1\n",
        "      print('({}) Training Loss: {:.8f} Valid Loss: {:.8f} '.format(epochs, train_epoch_loss, valid_epoch_loss))\n",
        "    \n",
        "    if stop == 230:\n",
        "        print('Early stopping criteria fulfilled')\n",
        "        break\n",
        "\n",
        "    rerror_train.append(train_epoch_loss)\n",
        "    rerror_valid.append(valid_epoch_loss)\n",
        "\n",
        "  # =================== plot and save best reconstruciton ===================\n",
        "  # images = (best_train_out[:, 0], best_train_in[:, 0], best_valid_out[:, 0], best_valid_in[:, 0])\n",
        "  # show, save, path = (1, 1, 1, 1), (1, 1, 1, 1), get_path('sd', 'W') \n",
        "  # name = ('best_train_out', 'best_train_in', 'best_valid_out', 'best_valid_in')\n",
        "  # show_save_image('sd', images, show, save, stats=stats, path=path, name=name)\n",
        "\n",
        "  print('({}) BEST_Training Loss: {:.8f} BEST_Valid Loss: {:.8f} '.format(epochs, prev_train_loss, prev_valid_loss))\n",
        "\n",
        "  path_to_weights = get_path('sd', 'W')\n",
        "  weights_name = get_name('sd', 'W', drop=kwargs.get('drop'), sensor_number=sensor_number)\n",
        "  torch.save(model_wts, osp.join(path_to_weights, weights_name))\n",
        "  print('=========saved weights=========')\n",
        "\n",
        "  plot_graph(rerror_train,rerror_valid, 1, osp.join(path_to_weights, weights_name + '.png'))\n",
        "  print('================================ over ================================')"
      ],
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MWq73EDK8JMj"
      },
      "source": [
        "## SD TestLoop"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YTFX_FQf8auV"
      },
      "source": [
        "def SD_test(sensor_number, plot, save, **kwargs):\n",
        "\n",
        "  path_to_weights = get_path('sd', 'W')\n",
        "  weights_name = get_name('sd', 'W', drop=kwargs.get('drop'), sensor_number=sensor_number)\n",
        "  pretrained_weightsRNN = osp.join(path_to_weights, weights_name)\n",
        "\n",
        "  train_out, valid_out, strain_in, svalid_in, test_out, stest_in, stats = SD_train_data(sensor_number,\n",
        "                                                                                        SNR=kwargs.get('SNR',None), \n",
        "                                                                                        data_noise=kwargs.get('drop',None),\n",
        "                                                                                        plot_SNR=kwargs.get('plot_SNR'),\n",
        "                                                                                        plot=plot[2], save=save[2])\n",
        "  del(train_out, valid_out, strain_in, svalid_in)\n",
        "  test_out, stest_in  = torch.tensor(np.expand_dims(test_out, axis=1)).float().to(device), torch.tensor(np.expand_dims(stest_in, axis=1)).float().to(device)\n",
        "  \n",
        "  model = shallow_decoder_net(sensor_number, drop=kwargs.get('drop')).to(device)\n",
        "\n",
        "  # Load saved neural network weights\n",
        "  model.load_state_dict(torch.load(pretrained_weightsRNN, map_location=torch.device(device)))\n",
        "  model.eval()\n",
        "\n",
        "  test = torch.utils.data.TensorDataset(stest_in, test_out)\n",
        "  test_data_genn = torch.utils.data.DataLoader(test, shuffle=False, batch_size=1)\n",
        "\n",
        "  _, cords = sensor_cord_data(sensor_number)      \n",
        "\n",
        "  states_pred = np.zeros([len(test), len(test_out[0, 0, :]) ])\n",
        "  states_true = np.zeros([len(test), len(test_out[0, 0, :]) ])\n",
        "\n",
        "  for i, (sensor, state) in enumerate(test):\n",
        "      pred = (model(sensor[:, None].to(device)))[:, 0]\n",
        "      states_pred[i, :] = pred.cpu().data.numpy()\n",
        "      states_true[i, :] = state.cpu().data.numpy()\n",
        "      image_select('sd', i, (pred[0], state), sensor_number, plot[:2], save[:2], stats=stats, cords=cords, **kwargs)\n",
        "\n",
        "  assert (states_true == test_out[:, 0].cpu().data.numpy()).any()\n",
        "\n",
        "  trainAE, validAE, testAE, statsAE = autoencoder_train_data()\n",
        "  del(trainAE,validAE)\n",
        "  statsAE[1] = np.ones_like(statsAE[1])\n",
        "\n",
        "  states_true = testAE\n",
        "\n",
        "  if kwargs.get('use_stats', 1):\n",
        "    # states_pred = states_pred + statsAE[0, :]\n",
        "    states_pred = states_pred + stats[0, :]\n",
        "  else:\n",
        "    states_true = (testAE - statsAE[0, :])  # / statsAE[1, :]\n",
        "\n",
        "  Ferr = []\n",
        "  if args.data_type == 'periodic':\n",
        "    start, end = 0, 60\n",
        "  if args.data_type == 'transient':\n",
        "    start, end = 200, 399\n",
        "  if args.data_type == 'turbulence':\n",
        "    start, end = 0, 400\n",
        "  if args.data_type == 'sea_temp':\n",
        "    start, end = 0, 200 #200\n",
        "\n",
        "  if args.operation_mode == 'online':      \n",
        "      for i in range(start, end-seq_impct):\n",
        "        Ferr.append(np.linalg.norm(states_true[i,:] - states_pred[i,:]) / np.linalg.norm(states_true[i,:]))\n",
        "  \n",
        "  elif args.operation_mode == 'offline':\n",
        "    for i in range(start, end-2*seq_impct):\n",
        "        Ferr.append(np.linalg.norm(states_true[i,:] - states_pred[i,:]) / np.linalg.norm(states_true[i,:]))\n",
        "\n",
        "  del(states_true, states_pred)\n",
        "  Ferr_avg = np.mean(Ferr)\n",
        "  print(\"Error: \", Ferr_avg, '------------------------------------------------')\n",
        "\n",
        "  return Ferr_avg"
      ],
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j37RCU8qvIHE"
      },
      "source": [
        "# Final results"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Jy03DcJbX6LD"
      },
      "source": [
        "## Periodic"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qL_yyo2lnJIw"
      },
      "source": [
        "method and sensors"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k6WeKByMm2ox"
      },
      "source": [
        "def periodic_mthd__snsr():\n",
        "  args.data_type = 'periodic' \n",
        "  args.exp = 'mthd__snsr'\n",
        "\n",
        "  num_epochs, lr, bs, bsv, bn, early_stop  = 1200, 0.0007, 399, 133, 25, 200\n",
        "  AutoEcoder(bn, lr, num_epochs, bs, bsv, early_stop, drop=0)\n",
        "\n",
        "\n",
        "  for s in [1, 2, 5, 10]:\n",
        "\n",
        "      num_epochs, lr, hidden_size, seq_len, bn  = 2700, 0.0006, 50, 7, 25\n",
        "      ARE_train_LSTM(s, seq_len, bn, hidden_size, num_epochs, lr, drop=0)\n",
        "\n",
        "      num_epochs, lr, bn  = 1200, 0.0009, 25\n",
        "      PDS_train(s, bn, num_epochs, lr)\n",
        "\n",
        "      num_epochs, lr, wd  = 1500, 0.001, 1e-4\n",
        "      SD_train(s, num_epochs, lr, wd)\n",
        "\n",
        "  error = np.zeros((3, 4))\n",
        "  plot_s_list = np.array([1, 2])\n",
        "  plot_seq_len_list = np.array([7])\n",
        "  plot_image_idx = np.array([39])  \n",
        "\n",
        "  for s, i in zip([1, 2, 5, 10], count(0, 1)):  \n",
        "    seq_len, bn, plot, save = 7, 25, (1, 1, 0), (1, 1, 0)\n",
        "\n",
        "    er1 = ARE_test(s, seq_len, bn,\n",
        "                  plot, save,\n",
        "                  plot_s_list=plot_s_list, \n",
        "                  plot_seq_len_list=plot_seq_len_list,\n",
        "                  plot_image_idx=plot_image_idx)\n",
        "    \n",
        "    er2 = PDS_test(s, bn, plot, save,\n",
        "                   plot_s_list=plot_s_list,\n",
        "                   plot_image_idx=plot_image_idx)\n",
        "\n",
        "    er3 = SD_test(s, plot, save,\n",
        "                  plot_s_list=plot_s_list,\n",
        "                  plot_image_idx=plot_image_idx)\n",
        "\n",
        "    error[:, i] = [er1, er2, er3]\n",
        "\n",
        "  error_path = get_path('', 'P')\n",
        "  np.savetxt(osp.join(error_path, \"error_{}.csv\".format(args.RNN)), error, delimiter=\",\")\n",
        "\n",
        "periodic_mthd__snsr()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zO1zFSwon2r8"
      },
      "source": [
        "method and SNR(signal to noise ratio)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "32jIfLvWnkvv"
      },
      "source": [
        "def periodic_mthd__snr():\n",
        "  args.data_type = 'periodic' \n",
        "  args.exp = 'mthd__snr'\n",
        "\n",
        "  num_epochs, lr, bs, bsv, bn, early_stop = 1200, 0.0007, 399, 133, 25, 200\n",
        "  AutoEcoder(bn, lr, num_epochs, bs, bsv, early_stop, drop=0.35)\n",
        "\n",
        "  for s in [1, 2]:\n",
        "\n",
        "      num_epochs, lr, hidden_size, seq_len, bn  = 2700, 0.0006, 50, 7, 25\n",
        "      ARE_train_LSTM(s, seq_len, bn, hidden_size, num_epochs, lr, drop=0.35)\n",
        "\n",
        "      num_epochs, lr, bn  = 1200, 0.0009, 25\n",
        "      PDS_train(s, bn, num_epochs, lr)\n",
        "\n",
        "      num_epochs, lr, wd  = 1500, 0.001, 1e-4\n",
        "      SD_train(s, num_epochs, lr, wd, drop=0.1)\n",
        "\n",
        "  SNR_range = range(16, 84, 4)\n",
        "  error = np.zeros((3, len(SNR_range)))\n",
        "  plot_s_list = np.array([1, 2])\n",
        "  plot_seq_len_list = np.array([7])\n",
        "  plot_image_idx = np.array([39])\n",
        "  plot_SNR = np.array([20, 72])\n",
        "  \n",
        "  for s in [1, 2]:\n",
        "    for SNR, i in zip(SNR_range, count(0, 1)):\n",
        "      seq_len, bn, plot, save, use_stats = 7, 25, (0, 0), (0, 0), 1\n",
        "      print('\\n no. of sensors:',s,' SNR:',SNR)\n",
        "\n",
        "      er1 = ARE_test(s, seq_len, bn,\n",
        "                    plot, save,\n",
        "                    drop=0.35, SNR=SNR,\n",
        "                    plot_s_list=plot_s_list, \n",
        "                    plot_seq_len_list=plot_seq_len_list,\n",
        "                    plot_image_idx=plot_image_idx,\n",
        "                    plot_SNR=plot_SNR,\n",
        "                    use_stats=use_stats)\n",
        "      \n",
        "      er2 = PDS_test(s, bn, plot, save,\n",
        "                    drop=1, SNR=SNR,\n",
        "                    plot_s_list=plot_s_list,\n",
        "                    plot_image_idx=plot_image_idx,\n",
        "                    plot_SNR=plot_SNR,\n",
        "                    use_stats=use_stats)\n",
        "\n",
        "      er3 = SD_test(s, plot, save,\n",
        "                    drop=0.1, SNR=SNR,\n",
        "                    plot_s_list=plot_s_list,\n",
        "                    plot_image_idx=plot_image_idx,\n",
        "                    plot_SNR=plot_SNR,\n",
        "                    use_stats=use_stats)\n",
        "\n",
        "      error[:, i] = [er1, er2, er3]\n",
        "\n",
        "    error_path = get_path('', 'P')\n",
        "    np.savetxt(osp.join(error_path, \"error_{}_{}.csv\".format(args.RNN, s)), error, delimiter=\",\")\n",
        "\n",
        "periodic_mthd__snr()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L25U3kgEd9WX"
      },
      "source": [
        "sensor and sequence len."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "38i3UgNad-Ez"
      },
      "source": [
        "def periodic_snsr__seq_len():\n",
        "  args.data_type = 'periodic' \n",
        "  args.exp = 'snsr__seq_len'\n",
        "\n",
        "  num_epochs, lr, bs, bsv, bn, early_stop = 1200, 0.0007, 399, 133, 25, 200\n",
        "  AutoEcoder(bn, lr, num_epochs, bs, bsv, early_stop, drop=0)\n",
        "\n",
        "  for s in [1, 2, 5, 10]:\n",
        "    for seq_len in [3, 5, 7, 9]:\n",
        "\n",
        "        num_epochs, lr, hidden_size, s, bn  = 2700, 0.0006, 50, s, 25\n",
        "        ARE_train_LSTM(s, seq_len, bn, hidden_size, num_epochs, lr, drop=0.35)\n",
        "\n",
        "  error = np.zeros((4, 4))\n",
        "  plot_s_list = np.array([1, 2])\n",
        "  plot_seq_len_list = np.array([7])\n",
        "  plot_image_idx = np.array([39])  \n",
        "  plot_SNR = np.array([20, 72])\n",
        "\n",
        "  for SNR in [None]:  \n",
        "    for s, j in zip([1, 2, 5, 10], count(0, 1)):\n",
        "      for seq_len, i in zip([3, 5, 7, 9], count(0, 1)):  \n",
        "        bn, plot, save = (25, (1, 1, 0), (1, 1, 0))\n",
        "        print('\\n no. of sensors:',s,' seq_len:',seq_len)\n",
        "        \n",
        "        er1 = ARE_test(s, seq_len, bn,\n",
        "                       plot, save,\n",
        "                       plot_s_list=plot_s_list, \n",
        "                       plot_seq_len_list=plot_seq_len_list,\n",
        "                       plot_image_idx=plot_image_idx)\n",
        "        \n",
        "        # er1 = ARE_test(s, seq_len, bn,\n",
        "        #                plot, save,\n",
        "        #                drop=0.35, SNR=SNR,\n",
        "        #                plot_s_list=plot_s_list, \n",
        "        #                plot_seq_len_list=plot_seq_len_list,\n",
        "        #                plot_image_idx=plot_image_idx,\n",
        "        #                plot_SNR=plot_SNR)\n",
        "\n",
        "        error[j, i] = er1\n",
        "\n",
        "    error_path = get_path('', 'P')\n",
        "    np.savetxt(osp.join(error_path, \"error_{}_SNR{}.csv\".format(args.RNN, SNR)), error, delimiter=\",\")\n",
        "\n",
        "periodic_snsr__seq_len()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s_gLbgljX-nS"
      },
      "source": [
        "## Transient"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tUNo5vl-oQIc"
      },
      "source": [
        "method and sensors"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nXEEN2K3YNbU"
      },
      "source": [
        "def transient_mthd__snsr():\n",
        "  args.data_type = 'transient' \n",
        "  args.exp = 'mthd__snsr'\n",
        "\n",
        "  # num_epochs, lr, bs, bsv, bn, early_stop  = 1200, 0.001, 399, 133, 25, 200\n",
        "  # AutoEcoder(bn, lr, num_epochs, bs, bsv, early_stop, drop=0)\n",
        "\n",
        "\n",
        "  # for s in [1, 2, 5, 10]:\n",
        "\n",
        "  #     # num_epochs, lr, hidden_size, seq_len, bn  = 2600, 0.0008, 50, 7, 25\n",
        "  #     # ARE_train_LSTM(s, seq_len, bn, hidden_size, num_epochs, lr, drop=0)\n",
        "\n",
        "  #     # num_epochs, lr, bn  = 1500, 0.0009, 25\n",
        "  #     # PDS_train(s, bn, num_epochs, lr)\n",
        "\n",
        "  #     num_epochs, lr, wd  = 1500, 0.001, 1e-4\n",
        "  #     SD_train(s, num_epochs, lr, wd)\n",
        "\n",
        "  error = np.zeros((3, 4))\n",
        "  plot_s_list = np.array([1, 2])\n",
        "  plot_seq_len_list = np.array([7])\n",
        "  plot_image_idx = np.array([335])    \n",
        "\n",
        "  for s, i in zip([1, 2, 5, 10], count(0, 1)):  \n",
        "    seq_len, bn, plot, save = 7, 25, (1, 1, 0), (1, 1, 0)\n",
        "\n",
        "    er1 = ARE_test(s, seq_len, bn,\n",
        "                  plot, save,\n",
        "                  plot_s_list=plot_s_list, \n",
        "                  plot_seq_len_list=plot_seq_len_list,\n",
        "                  plot_image_idx=plot_image_idx)\n",
        "    \n",
        "    er2 = PDS_test(s, bn, plot, save,\n",
        "                   plot_s_list=plot_s_list,\n",
        "                   plot_image_idx=plot_image_idx)\n",
        "\n",
        "    er3 = SD_test(s, plot, save,\n",
        "                  plot_s_list=plot_s_list,\n",
        "                  plot_image_idx=plot_image_idx)\n",
        "\n",
        "    error[:, i] = [er1, er2, er3]\n",
        "\n",
        "  error_path = get_path('', 'P')\n",
        "  np.savetxt(osp.join(error_path, \"error_{}.csv\".format(args.RNN)), error, delimiter=\",\")\n",
        "\n",
        "transient_mthd__snsr()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n1Yn6stOGiKR"
      },
      "source": [
        "method and SNR(signal to noise ratio)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w7QYo1ZsGvGJ"
      },
      "source": [
        "def transient_mthd__snr():\n",
        "  args.data_type = 'transient' \n",
        "  args.exp = 'mthd__snr'\n",
        "\n",
        "  num_epochs, lr, bs, bsv, bn, early_stop  = 1500, 0.0007, 399, 133, 25, 250\n",
        "  AutoEcoder(bn, lr, num_epochs, bs, bsv, early_stop, drop=0.35)\n",
        "\n",
        "\n",
        "  for s in [1, 2]:\n",
        "\n",
        "      num_epochs, lr, hidden_size, seq_len, bn  = 2600, 0.0008, 50, 7, 25\n",
        "      ARE_train_LSTM(s, seq_len, bn, hidden_size, num_epochs, lr, drop=0.35)\n",
        "\n",
        "      num_epochs, lr, bn  = 1500, 0.0009, 25\n",
        "      PDS_train(s, bn, num_epochs, lr)\n",
        "\n",
        "      num_epochs, lr, wd  = 1700, 0.0009, 1e-4\n",
        "      SD_train(s, num_epochs, lr, wd, drop=0.1)\n",
        "\n",
        "  SNR_range = range(4, 84, 4)\n",
        "  error = np.zeros((3, len(SNR_range)))\n",
        "  plot_s_list = np.array([1, 2])\n",
        "  plot_seq_len_list = np.array([7])\n",
        "  plot_image_idx = np.array([335])\n",
        "  plot_SNR = np.array([28, 76])\n",
        "  \n",
        "  for s in [1, 2]:\n",
        "    for SNR, i in zip(SNR_range, count(0, 1)):\n",
        "      seq_len, bn, plot, save = 7, 25, (1, 1, 1), (1, 1, 1)\n",
        "      print('no. of sensors:', s, ' SNR:', SNR)\n",
        "\n",
        "      er1 = ARE_test(s, seq_len, bn,\n",
        "                    plot, save,\n",
        "                    drop=0.35, SNR=SNR,\n",
        "                    plot_s_list=plot_s_list, \n",
        "                    plot_seq_len_list=plot_seq_len_list,\n",
        "                    plot_image_idx=plot_image_idx,\n",
        "                    plot_SNR=plot_SNR)\n",
        "      \n",
        "      er2 = PDS_test(s, bn, plot, save,\n",
        "                    SNR=SNR,\n",
        "                    plot_s_list=plot_s_list,\n",
        "                    plot_image_idx=plot_image_idx,\n",
        "                    plot_SNR=plot_SNR)\n",
        "\n",
        "      er3 = SD_test(s, plot, save,\n",
        "                    drop=0.1, SNR=SNR,\n",
        "                    plot_s_list=plot_s_list,\n",
        "                    plot_image_idx=plot_image_idx,\n",
        "                    plot_SNR=plot_SNR)\n",
        "\n",
        "      error[:, i] = [er1, er2, er3]\n",
        "\n",
        "    error_path = get_path('', 'P')\n",
        "    np.savetxt(osp.join(error_path, \"error_{}_{}.csv\".format(args.RNN, s)), error, delimiter=\",\")\n",
        "\n",
        "transient_mthd__snr()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RlB5k2FKMDdM"
      },
      "source": [
        "sensor and sequence len."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bJYUq6StL339"
      },
      "source": [
        "def transient_snsr__seq_len():\n",
        "  args.data_type = 'transient' \n",
        "  args.exp = 'snsr__seq_len'\n",
        "\n",
        "  num_epochs, lr, bs, bsv, bn, early_stop = 1300, 0.0007, 399, 133, 25, 200\n",
        "  AutoEcoder(bn, lr, num_epochs, bs, bsv, early_stop, drop=0)\n",
        "\n",
        "  for s in [1, 2, 5, 10]:\n",
        "    for seq_len in [3, 5, 7, 9]:\n",
        "\n",
        "        num_epochs, lr, hidden_size, s, bn  = 2700, 0.0008, 50, s, 25\n",
        "        ARE_train_LSTM(s, seq_len, bn, hidden_size, num_epochs, lr, drop=0.35)\n",
        "\n",
        "  error = np.zeros((4, 4))\n",
        "  plot_s_list = np.array([1, 2])\n",
        "  plot_seq_len_list = np.array([7])\n",
        "  plot_image_idx = np.array([335])  \n",
        "  plot_SNR = np.array([28, 76])\n",
        "\n",
        "  for SNR in [None]:  \n",
        "    for s, j in zip([1, 2, 5, 10], count(0, 1)):\n",
        "      for seq_len, i in zip([3, 5, 7, 9], count(0, 1)):  \n",
        "        bn, plot, save = (25, (1, 1, 0), (1, 1, 0))\n",
        "        print('\\n no. of sensors:',s,' seq_len:',seq_len)\n",
        "        \n",
        "        er1 = ARE_test(s, seq_len, bn,\n",
        "                       plot, save,\n",
        "                       plot_s_list=plot_s_list, \n",
        "                       plot_seq_len_list=plot_seq_len_list,\n",
        "                       plot_image_idx=plot_image_idx)\n",
        "        \n",
        "        # er1 = ARE_test(s, seq_len, bn,\n",
        "        #                plot, save,\n",
        "        #                drop=0.35, SNR=SNR,\n",
        "        #                plot_s_list=plot_s_list, \n",
        "        #                plot_seq_len_list=plot_seq_len_list,\n",
        "        #                plot_image_idx=plot_image_idx,\n",
        "        #                plot_SNR=plot_SNR)\n",
        "\n",
        "        error[j, i] = er1\n",
        "\n",
        "    error_path = get_path('', 'P')\n",
        "    np.savetxt(osp.join(error_path, \"error_{}_SNR{}.csv\".format(args.RNN, SNR)), error, delimiter=\",\")\n",
        "\n",
        "transient_snsr__seq_len()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eK0jmMLvC1ui"
      },
      "source": [
        "## SST"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MvVV34ZGC6jO"
      },
      "source": [
        "method and sensors"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c9ZWWN8fuVPH"
      },
      "source": [
        "# del args.raw_data\n",
        "def sst_mthd__snsr():\n",
        "  args.data_type = 'sea_temp' \n",
        "  args.exp = 'mthd__snsr'\n",
        "\n",
        "  num_epochs, lr, bs, bsv, bn, early_stop  = 1300, 0.0007, 400, 100, 25, 100\n",
        "  AutoEcoder(bn, lr, num_epochs, bs, bsv, early_stop, drop=0)\n",
        "\n",
        "  # for s in [4]:\n",
        "  for s in [4, 8, 16, 32]:\n",
        "\n",
        "      num_epochs, lr, hidden_size, seq_len, bn  = 700, 0.0006, 50, 7, 25\n",
        "      ARE_train_LSTM(s, seq_len, bn, hidden_size, num_epochs, lr, drop=0)\n",
        "\n",
        "      num_epochs, lr, bn  = 700, 0.0007, 25\n",
        "      PDS_train(s, bn, num_epochs, lr)\n",
        "\n",
        "      num_epochs, lr, wd  = 600, 0.0009, 1e-4\n",
        "      SD_train(s, num_epochs, lr, wd)\n",
        "\n",
        "  error = np.zeros((3, 4))\n",
        "  plot_s_list = np.array([4])\n",
        "  plot_seq_len_list = np.array([7])\n",
        "  plot_image_idx = np.array([10]) \n",
        "\n",
        "  for s, i in zip([4, 8, 16, 32], count(0, 1)):  \n",
        "    seq_len, bn, plot, save, use_stats = 7, 25, (1, 1, 0), (0, 0), 1\n",
        "\n",
        "    er1 = ARE_test(s, seq_len, bn,\n",
        "                  plot, save,\n",
        "                  plot_s_list=plot_s_list, \n",
        "                  plot_seq_len_list=plot_seq_len_list,\n",
        "                  plot_image_idx=plot_image_idx,\n",
        "                  use_stats=use_stats)\n",
        "    \n",
        "    er2 = PDS_test(s, bn, plot, save,\n",
        "                   plot_s_list=plot_s_list,\n",
        "                   plot_image_idx=plot_image_idx,\n",
        "                   use_stats=use_stats)\n",
        "\n",
        "    er3 = SD_test(s, plot, save,\n",
        "                  plot_s_list=plot_s_list,\n",
        "                  plot_image_idx=plot_image_idx,\n",
        "                  use_stats=use_stats)\n",
        "\n",
        "    error[:, i] = [er1, er2, er3]\n",
        "\n",
        "  error_path = get_path('', 'P')\n",
        "  np.savetxt(osp.join(error_path, \"error_{}_seq_len{}.csv\".format(args.RNN, seq_len)), error, delimiter=\",\")\n",
        "\n",
        "sst_mthd__snsr()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HKWvynDgZWeL"
      },
      "source": [
        "method and sensor at certain noise level"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yeAk4f1lZf9G"
      },
      "source": [
        "def sst_mthd__snsr_with_noise():\n",
        "  args.data_type = 'sea_temp' \n",
        "  args.exp = 'mthd__snsr_with_noise'\n",
        "\n",
        "  # num_epochs, lr, bs, bsv, bn, early_stop  = 1300, 0.0007, 400, 100, 25, 100\n",
        "  # AutoEcoder(bn, lr, num_epochs, bs, bsv, early_stop, drop=0.35)\n",
        "\n",
        "  # for s in [4, 8, 16, 32]:\n",
        "\n",
        "  #     num_epochs, lr, hidden_size, seq_len, bn  = 700, 0.0006, 50, 7, 25\n",
        "  #     ARE_train_LSTM(s, seq_len, bn, hidden_size, num_epochs, lr, drop=0.35)\n",
        "\n",
        "  #     num_epochs, lr, bn  = 700, 0.0007, 25\n",
        "  #     PDS_train(s, bn, num_epochs, lr)\n",
        "\n",
        "  #     num_epochs, lr, wd  = 600, 0.0009, 1e-4\n",
        "  #     SD_train(s, num_epochs, lr, wd, drop=0.1)\n",
        "\n",
        "  error = np.zeros((3, 4))\n",
        "  plot_s_list = np.array([4, 8])\n",
        "  plot_seq_len_list = np.array([7])\n",
        "  plot_image_idx = np.array([20])  \n",
        "  plot_SNR = np.array([20])\n",
        "\n",
        "  for SNR in [10, 20, 30]:  \n",
        "    for s, i in zip([4, 8, 16, 32], count(0, 1)):  \n",
        "      seq_len, bn, plot, save, use_stats = 7, 25, (1, 1, 1), (1, 1, 1), 1\n",
        "\n",
        "      er1 = ARE_test(s, seq_len, bn,\n",
        "                      plot, save,\n",
        "                      drop=0.35, SNR=SNR,\n",
        "                      plot_s_list=plot_s_list, \n",
        "                      plot_seq_len_list=plot_seq_len_list,\n",
        "                      plot_image_idx=plot_image_idx,\n",
        "                      plot_SNR=plot_SNR,\n",
        "                      use_stats=use_stats)\n",
        "        \n",
        "      er2 = PDS_test(s, bn, plot, save,\n",
        "                    SNR=SNR,\n",
        "                    plot_s_list=plot_s_list,\n",
        "                    plot_image_idx=plot_image_idx,\n",
        "                    plot_SNR=plot_SNR,\n",
        "                    use_stats=use_stats)\n",
        "\n",
        "      er3 = SD_test(s, plot, save,\n",
        "                    drop=0.1, SNR=SNR,\n",
        "                    plot_s_list=plot_s_list,\n",
        "                    plot_image_idx=plot_image_idx,\n",
        "                    plot_SNR=plot_SNR,\n",
        "                    use_stats=use_stats)\n",
        "\n",
        "      error[:, i] = [er1, er2, er3]\n",
        "\n",
        "    error_path = get_path('', 'P')\n",
        "    np.savetxt(osp.join(error_path, \"error_{}_seq_len{}_SNR{}.csv\".format(args.RNN, seq_len, SNR)), error, delimiter=\",\")\n",
        "\n",
        "sst_mthd__snsr_with_noise()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XXrgNCixLhlF"
      },
      "source": [
        "method and bottle neck"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Cw93GMiZMISF"
      },
      "source": [
        "def sst_mthd__bn():\n",
        "  args.data_type = 'sea_temp' \n",
        "  args.exp = 'mthd__bn'\n",
        "\n",
        "  for bn in [5, 15, 25, 50]:\n",
        "\n",
        "    num_epochs, lr, bs, bsv, bn, early_stop  = 1300, 0.0007, 400, 100, bn, 100\n",
        "    AutoEcoder(bn, lr, num_epochs, bs, bsv, early_stop, drop=0.35)\n",
        "\n",
        "    for s in [2, 4, 8]:\n",
        "\n",
        "      num_epochs, lr, hidden_size, seq_len, bn  = 700, 0.0006, 50, 7, bn\n",
        "      ARE_train_LSTM(s, seq_len, bn, hidden_size, num_epochs, lr, drop=0.35)\n",
        "\n",
        "      num_epochs, lr, bn  = 700, 0.0009, bn\n",
        "      PDS_train(s, bn, num_epochs, lr)\n",
        "\n",
        "  error = np.zeros((2, 4))\n",
        "  plot_s_list = np.array([2])\n",
        "  plot_seq_len_list = np.array([7])\n",
        "  plot_image_idx = np.array([40])  # 5\n",
        "  plot_SNR = np.array([20])\n",
        "  \n",
        "  for s in [2, 4, 8]:\n",
        "    for bn, i in zip([5, 15, 25, 50], count(0, 1)): \n",
        "\n",
        "      seq_len, bn, plot, save, use_stats, SNR = 7, bn, (1, 1, 0), (1, 1, 0), 1, 20\n",
        "\n",
        "      er1 = ARE_test(s, seq_len, bn,\n",
        "                    plot, save,\n",
        "                    drop=0.35, SNR=SNR,\n",
        "                    plot_s_list=plot_s_list, \n",
        "                    plot_seq_len_list=plot_seq_len_list,\n",
        "                    plot_image_idx=plot_image_idx,\n",
        "                    plot_SNR=plot_SNR)\n",
        "      \n",
        "      er2 = PDS_test(s, bn, plot, save,\n",
        "                    SNR=SNR,\n",
        "                    plot_s_list=plot_s_list,\n",
        "                    plot_image_idx=plot_image_idx,\n",
        "                    plot_SNR=plot_SNR)\n",
        "      \n",
        "      error[:, i] = [er1, er2]\n",
        "\n",
        "    error_path = get_path('', 'P')\n",
        "    np.savetxt(osp.join(error_path, \"error_{}_{}.csv\".format(args.RNN, s)), error, delimiter=\",\")\n",
        "\n",
        "\n",
        "sst_mthd__bn()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vMi5QJ8UV9zN"
      },
      "source": [
        "method and SNR(signal to noise ratio)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zBL4WCWpLcKE"
      },
      "source": [
        "def sst_mthd__snr():\n",
        "  args.data_type = 'sea_temp' \n",
        "  args.exp = 'mthd__snr'\n",
        "\n",
        "  num_epochs, lr, bs, bsv, bn, early_stop  = 1300, 0.0007, 400, 100, 25, 100\n",
        "  AutoEcoder(bn, lr, num_epochs, bs, bsv, early_stop, drop=0.35)\n",
        "\n",
        "\n",
        "  for s in [2, 4, 8]:  \n",
        "\n",
        "      num_epochs, lr, hidden_size, seq_len, bn  = 700, 0.0006, 50, 7, 25\n",
        "      ARE_train_LSTM(s, seq_len, bn, hidden_size, num_epochs, lr, drop=0.35)\n",
        "\n",
        "      num_epochs, lr, bn  = 700, 0.0009, 25\n",
        "      PDS_train(s, bn, num_epochs, lr)\n",
        "\n",
        "      num_epochs, lr, wd  = 500, 0.0009, 1e-4\n",
        "      SD_train(s, num_epochs, lr, wd, drop=0.1)\n",
        "\n",
        "  SNR_range = range(5, 80, 5)\n",
        "  error = np.zeros((3, len(SNR_range)))\n",
        "  plot_s_list = np.array([4])\n",
        "  plot_seq_len_list = np.array([7])\n",
        "  plot_image_idx = np.array([20])\n",
        "  plot_SNR = np.array([10, 70])\n",
        "  \n",
        "  for s in [2, 4, 8]:\n",
        "    for SNR, i in zip(SNR_range, count(0, 1)):\n",
        "      seq_len, bn, plot, save, use_stats = 7, 25, (0, 0, 0), (0, 0, 0), 1\n",
        "\n",
        "      er1 = ARE_test(s, seq_len, bn,\n",
        "                    plot, save,\n",
        "                    drop=0.35, SNR=SNR,\n",
        "                    plot_s_list=plot_s_list, \n",
        "                    plot_seq_len_list=plot_seq_len_list,\n",
        "                    plot_image_idx=plot_image_idx,\n",
        "                    plot_SNR=plot_SNR)\n",
        "      \n",
        "      er2 = PDS_test(s, bn, plot, save,\n",
        "                    SNR=SNR,\n",
        "                    plot_s_list=plot_s_list,\n",
        "                    plot_image_idx=plot_image_idx,\n",
        "                    plot_SNR=plot_SNR)\n",
        "\n",
        "      er3 = SD_test(s, plot, save,\n",
        "                    drop=0.1, SNR=SNR,\n",
        "                    plot_s_list=plot_s_list,\n",
        "                    plot_image_idx=plot_image_idx,\n",
        "                    plot_SNR=plot_SNR)\n",
        "\n",
        "      error[:, i] = [er1, er2, er3]\n",
        "\n",
        "    error_path = get_path('', 'P')\n",
        "    np.savetxt(osp.join(error_path, \"error_{}_{}.csv\".format(args.RNN, s)), error, delimiter=\",\")\n",
        "\n",
        "sst_mthd__snr()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d4ENdp1oSPOW"
      },
      "source": [
        "sensor and sequence len."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SVjf_hOMSX1L"
      },
      "source": [
        "def sst_snsr__seq_len():\n",
        "  args.data_type = 'sea_temp' \n",
        "  args.exp = 'snsr__seq_len'\n",
        "\n",
        "  num_epochs, lr, bs, bsv, bn, early_stop  = 1300, 0.0007, 400, 100, 25, 100\n",
        "  AutoEcoder(bn, lr, num_epochs, bs, bsv, early_stop, drop=0.35)\n",
        "\n",
        "  for s in [2, 4, 8, 16]:\n",
        "    for seq_len in [5, 7, 9, 11, 13]:\n",
        "\n",
        "        num_epochs, lr, hidden_size, s, bn  = 700, 0.0006, 50, s, 25\n",
        "        ARE_train_LSTM(s, seq_len, bn, hidden_size, num_epochs, lr, drop=0.35)\n",
        "\n",
        "  error = np.zeros((5, 5))\n",
        "  plot_s_list = np.array([16])\n",
        "  plot_seq_len_list = np.array([0])\n",
        "  plot_image_idx = np.array([20])  # 5\n",
        "  plot_SNR = np.array([50])\n",
        "\n",
        "  for SNR in [10, 20, 30]:  \n",
        "    for s, j in zip([2, 4, 8, 16], count(0, 1)):\n",
        "      for seq_len, i in zip([5, 7, 9, 11, 13], count(0, 1)):  \n",
        "        bn, plot, save = (25, (1, 1, 0), (1, 1, 0)) if s in [4] else (25, (0, 0, 0), (0, 0, 0)) \n",
        "        print('\\n no. of sensors:',s,' seq_len:',seq_len)\n",
        "        \n",
        "        er1 = ARE_test(s, seq_len, bn,\n",
        "                       plot, save,\n",
        "                       drop=0.35, SNR=SNR,\n",
        "                       plot_s_list=plot_s_list, \n",
        "                       plot_seq_len_list=plot_seq_len_list,\n",
        "                       plot_image_idx=plot_image_idx,\n",
        "                       plot_SNR=plot_SNR)\n",
        "\n",
        "        error[j, i] = er1\n",
        "\n",
        "    error_path = get_path('', 'P')\n",
        "    np.savetxt(osp.join(error_path, \"error_{}_SNR{}.csv\".format(args.RNN, SNR)), error, delimiter=\",\")\n",
        "\n",
        "sst_snsr__seq_len()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3bqOcsCvA0Vl"
      },
      "source": [
        "# Recycle Bin"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tk1qu5xWfT2m"
      },
      "source": [
        "def periodic_rndm_trials():\n",
        "  args.data_type = 'periodic' \n",
        "  args.exp = 'rndm_trials'\n",
        "\n",
        "  num_epochs, lr, bs, bsv, bn, early_stop  = 1200, 0.0007, 399, 133, 25, 200 \n",
        "  AutoEcoder(bn, lr, num_epochs, bs, bsv, early_stop, drop=0.35)\n",
        "\n",
        "\n",
        "  for s in [1, 2]:  \n",
        "    \n",
        "      num_epochs, lr, hidden_size, seq_len, bn  = 2700, 0.0006, 50, 7, 25 \n",
        "      ARE_train_LSTM(s, seq_len, bn, hidden_size, num_epochs, lr, drop=0.35)\n",
        "\n",
        "      num_epochs, lr, bn  = 1200, 0.0009, 25 \n",
        "      PDS_train(s, bn, num_epochs, lr)\n",
        "\n",
        "      num_epochs, lr, wd  = 1500, 0.001, 1e-4 \n",
        "      SD_train(s, num_epochs, lr, wd, drop=0.1)\n",
        "\n",
        "  SNR_range = range(16, 84, 4)\n",
        "  error = np.zeros((3, len(SNR_range)))\n",
        "  plot_s_list = np.array([10, 20])\n",
        "  plot_seq_len_list = np.array([7])\n",
        "  plot_image_idx = np.array([39])\n",
        "  plot_SNR = np.array([20, 72])\n",
        "  \n",
        "  for s in [1, 2]:\n",
        "    for SNR, i in zip(SNR_range, count(0, 1)):\n",
        "      seq_len, bn, plot, save, use_stats = 7, 25, (0, 0, 0), (0, 0, 0), 1\n",
        "      print('\\n no. of sensors:',s,' SNR:',SNR)\n",
        "\n",
        "      er1 = ARE_test(s, seq_len, bn,\n",
        "                    plot, save,\n",
        "                    drop=0.35, SNR=SNR,\n",
        "                    plot_s_list=plot_s_list, \n",
        "                    plot_seq_len_list=plot_seq_len_list,\n",
        "                    plot_image_idx=plot_image_idx,\n",
        "                    plot_SNR=plot_SNR)\n",
        "      \n",
        "      er2 = PDS_test(s, bn, plot, save,\n",
        "                    SNR=SNR,\n",
        "                    plot_s_list=plot_s_list,\n",
        "                    plot_image_idx=plot_image_idx,\n",
        "                    plot_SNR=plot_SNR)\n",
        "\n",
        "      er3 = SD_test(s, plot, save,\n",
        "                    drop=0.1, SNR=SNR,\n",
        "                    plot_s_list=plot_s_list,\n",
        "                    plot_image_idx=plot_image_idx,\n",
        "                    plot_SNR=plot_SNR)\n",
        "\n",
        "      error[:, i] = [er1, er2, er3]\n",
        "\n",
        "    error_path = get_path('', 'P')\n",
        "    np.savetxt(osp.join(error_path, \"error_{}_{}.csv\".format(args.RNN, s)), error, delimiter=\",\")\n",
        "\n",
        "periodic_rndm_trials()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v1JLqOIUeQ5p"
      },
      "source": [
        "## Turbulence"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7WyjqIMMofpB"
      },
      "source": [
        "method and sensors"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XhgQDF6OXtbt"
      },
      "source": [
        "def turbulence_mthd__snsr():\n",
        "  args.data_type = 'turbulence' \n",
        "  args.exp = 'mthd__snsr'\n",
        "\n",
        "  num_epochs, lr, bs, bsv, bn, early_stop = 1200, 0.0006, 1000, 400, 50, 300 \n",
        "  AutoEcoder(bn, lr, num_epochs, bs, bsv, early_stop, drop=0)\n",
        "\n",
        "\n",
        "  for s in [32*32]:\n",
        "\n",
        "      num_epochs, lr, hidden_size, seq_len, bn  = 700, 0.0006, 50, 7, 25\n",
        "      ARE_train_LSTM(s, seq_len, bn, hidden_size, num_epochs, lr, drop=0)\n",
        "\n",
        "      num_epochs, lr, bn  = 700, 0.0009, 25\n",
        "      PDS_train(s, bn, num_epochs, lr)\n",
        "\n",
        "      num_epochs, lr, wd  = 600, 0.0009, 1e-4\n",
        "      SD_train(s, num_epochs, lr, wd)\n",
        "\n",
        "  error = np.zeros((3, 1))\n",
        "  plot_s_list = np.array([32*32])\n",
        "  plot_seq_len_list = np.array([7])\n",
        "  plot_image_idx = np.array([0])  # 5\n",
        "\n",
        "  for s, i in zip([32*32], count(0, 1)):  \n",
        "    seq_len, bn, plot, save = 7, 25, (1, 1, 0), (1, 1, 0)\n",
        "\n",
        "    er1 = ARE_test(s, seq_len, bn,\n",
        "                  plot, save,\n",
        "                  plot_s_list=plot_s_list, \n",
        "                  plot_seq_len_list=plot_seq_len_list,\n",
        "                  plot_image_idx=plot_image_idx)\n",
        "    \n",
        "    er2 = PDS_test(s, bn, plot, save,\n",
        "                   plot_s_list=plot_s_list,\n",
        "                   plot_image_idx=plot_image_idx)\n",
        "\n",
        "    er3 = SD_test(s, plot, save,\n",
        "                  plot_s_list=plot_s_list,\n",
        "                  plot_image_idx=plot_image_idx)\n",
        "\n",
        "    error[:, i] = [er1, er2, er3]\n",
        "\n",
        "  error_path = get_path('', 'P')\n",
        "  np.savetxt(osp.join(error_path, \"error_{}.csv\".format(args.RNN)), error, delimiter=\",\")\n",
        "\n",
        "turbulence_mthd__snsr()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TXF__yr5BAcA"
      },
      "source": [
        "method and training samples"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FL3zPdmJBGX0"
      },
      "source": [
        "args.data_type = 'turbulence'\n",
        "args.exp = 'mthd__tr_samp'\n",
        "\n",
        "for args.tr_samp in range(3500, 4001, 500):\n",
        "\n",
        "  num_epochs, lr, bs, bsv, bn, early_stop = 1500, 0.0006, 1000, 400, 50, 200\n",
        "  AutoEcoder(bn, lr, num_epochs, bs, bsv, early_stop, drop=0) \n",
        "\n",
        "  num_epochs, lr, hidden_size, seq_len, bn  = 2700, 0.0006, 50, 7, 25\n",
        "  ARE_train_LSTM(s, seq_len, bn, hidden_size, num_epochs, lr, drop=0)\n",
        "\n",
        "  num_epochs, lr, bn  = 1200, 0.0009, 25\n",
        "  PDS_train(s, bn, num_epochs, lr)\n",
        "\n",
        "  num_epochs, lr, wd  = 12, 0.0009, 1e-4\n",
        "  SD_train(s, num_epochs, lr, wd)\n",
        "\n",
        "del args.tr_samp"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tXqtvxhtkUpD"
      },
      "source": [
        "\"\"\" testing sensor locations in get_data() \"\"\"\n",
        "\n",
        "#####################################              \n",
        "\n",
        "      recon = np.ones((m*n))*35\n",
        "      for i, j in zip(args.sea_idx, count(0, 1)):\n",
        "        recon[i] = args.raw_data[0][j]\n",
        "\n",
        "      idices = np.arange(0, 44219)\n",
        "      np.random.seed(323)  # changing seed will change the postion of sensors and networks will have to be retrained\n",
        "      s_idx_in_flatten = np.random.choice(idices, 500, False)\n",
        "\n",
        "      flat_sea_mask = np.zeros((44219))\n",
        "      flat_sea_mask[s_idx_in_flatten] = 1\n",
        "      # flat_sea_mask = np.ones((44219))\n",
        "      flat_mask = np.zeros((m*n))\n",
        "      for i, j in zip(args.sea_idx, count(0, 1)):\n",
        "        flat_mask[i] = flat_sea_mask[j]\n",
        "\n",
        "      mask = flat_mask.reshape(n, m)\n",
        "      cords = np.where(mask == 1)\n",
        "      cords = np.asarray(cords)\n",
        "\n",
        "      recon = np.ones((m*n))*35\n",
        "      for i, j in zip(args.sea_idx, count(0, 1)):\n",
        "        recon[i] = args.raw_data[0][j]\n",
        "\n",
        "      recon_im = recon.reshape(n, m)\n",
        "      \n",
        "      plt.figure()\n",
        "      plt.imshow(recon_im, cmap='terrain')\n",
        "      plt.gca().invert_yaxis()\n",
        "      plt.scatter(cords[1], cords[0], marker='.', color='b', s=5, zorder=5) if cords.any() else 0\n",
        "      \n",
        "      \n",
        "      plt.show()\n",
        "      plt.close()\n",
        "\n",
        "      \n",
        "      \n",
        "      pdb.set_trace()\n",
        "      ######################################"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}